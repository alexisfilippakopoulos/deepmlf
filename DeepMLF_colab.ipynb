{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaELr2E1QvTi",
        "outputId": "58a914eb-2419-42f9-906a-58cd6eda8388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'deepmlf'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 127 (delta 37), reused 113 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (127/127), 373.98 KiB | 8.70 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/alexisfilippakopoulos/deepmlf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaXId7iEuYOQ",
        "outputId": "5aab8405-0bee-4ab5-c57c-5fc1f66862d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-08 14:23:47--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156323998 (149M) [application/octet-stream]\n",
            "Saving to: ‘miniconda3/miniconda.sh’\n",
            "\n",
            "miniconda3/minicond 100%[===================>] 149.08M   109MB/s    in 1.4s    \n",
            "\n",
            "2025-11-08 14:23:48 (109 MB/s) - ‘miniconda3/miniconda.sh’ saved [156323998/156323998]\n",
            "\n",
            "PREFIX=/content/miniconda3\n",
            "Unpacking bootstrapper...\n",
            "Unpacking payload...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /content/miniconda3\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p miniconda3\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda3/miniconda.sh\n",
        "!bash miniconda3/miniconda.sh -b -u -p miniconda3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fipq9tFuedw",
        "outputId": "e13211d4-54ff-40bc-a5a0-5b614d17859d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "modified      /root/.bashrc\n",
            "modified      /root/.zshrc\n",
            "modified      /root/.config/fish/config.fish\n",
            "modified      /root/.xonshrc\n",
            "modified      /root/.tcshrc\n",
            "\n",
            "==> For changes to take effect, close and re-open your current shell. <==\n",
            "\n",
            "accepted Terms of Service for \u001b[4;94mhttps://repo.anaconda.com/pkgs/main\u001b[0m\n",
            "accepted Terms of Service for \u001b[4;94mhttps://repo.anaconda.com/pkgs/r\u001b[0m\n",
            "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
            "Retrieving notices: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.10.19       | 24.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "icu-75.1             | 11.6 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libstdcxx-15.2.0     | 3.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.4        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pip-25.2             | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.51.0     | 923 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 254 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libstdcxx-15.2.0     | 3.7 MB    | :   0% 0.004202891078065675/1 [00:00<01:37, 97.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.4        | 3.0 MB    | :   1% 0.005251914974368706/1 [00:00<01:18, 78.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "python-3.10.19       | 24.1 MB   | :   0% 0.0006472898490776396/1 [00:00<10:53, 654.42s/it]\n",
            "\n",
            "libstdcxx-15.2.0     | 3.7 MB    | :  98% 0.9750707301112366/1 [00:00<00:00,  2.52it/s]  \u001b[A\u001b[A\n",
            "python-3.10.19       | 24.1 MB   | :  13% 0.12945796981552793/1 [00:00<00:02,  3.09s/it]   \n",
            "\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | :   0% 0.004987209317899284/1 [00:00<01:45, 106.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | :  57% 0.5727347460505031/1 [00:00<00:00,  1.26it/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.4        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.97it/s]                 \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.19       | 24.1 MB   | :  28% 0.27574547570707447/1 [00:00<00:01,  1.67s/it]\n",
            "\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | :  68% 0.683247676552202/1 [00:00<00:00,  1.40it/s]    \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libstdcxx-15.2.0     | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  2.52it/s]               \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pip-25.2             | 1.1 MB    | :   1% 0.013918149321082462/1 [00:00<00:49, 50.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "python-3.10.19       | 24.1 MB   | :  47% 0.470579720279444/1 [00:00<00:00,  1.04s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.51.0     | 923 KB    | :   2% 0.01732700491552239/1 [00:00<00:41, 42.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pip-25.2             | 1.1 MB    | : 100% 1.0/1 [00:00<00:00, 50.16s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.40it/s]              \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.19       | 24.1 MB   | :  64% 0.6401696607377856/1 [00:00<00:00,  1.14it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   2% 0.018375108367605347/1 [00:00<00:46, 47.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | :   2% 0.021880692532465797/1 [00:00<00:39, 40.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | :   2% 0.019918497553954036/1 [00:00<00:43, 44.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.19       | 24.1 MB   | :  83% 0.8265891372721458/1 [00:00<00:00,  1.34it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:00<00:00, 40.57s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:00<00:00, 44.74s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | :   2% 0.022065963547523843/1 [00:00<00:44, 45.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:01<00:00, 45.07s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | :   4% 0.03657804201206022/1 [00:01<00:28, 29.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | :   3% 0.02886651702497278/1 [00:01<00:36, 37.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:01<00:00, 29.07s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   6% 0.05800056641178136/1 [00:01<00:18, 19.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:01<00:00, 37.38s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:01<00:00, 19.31s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | : 100% 1.0/1 [00:01<00:00,  1.72it/s]               \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 254 KB    | :   6% 0.06293284576766625/1 [00:01<00:17, 18.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | :  11% 0.10508828981379925/1 [00:01<00:10, 11.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 254 KB    | : 100% 1.0/1 [00:01<00:00, 18.71s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:01<00:00, 11.28s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | :  13% 0.1332379155552664/1 [00:01<00:07,  9.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | : 100% 1.0/1 [00:01<00:00,  9.14s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libstdcxx-15.2.0     | 3.7 MB    | : 100% 1.0/1 [00:01<00:00,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :  15% 0.1451272875440679/1 [00:01<00:07,  8.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :  16% 0.163198629386511/1 [00:01<00:06,  7.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:01<00:00,  8.70s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:01<00:00,  7.83s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.19       | 24.1 MB   | : 100% 1.0/1 [00:01<00:00,  1.73s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pip-25.2             | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pip-25.2             | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.51.0     | 923 KB    | : 100% 1.0/1 [00:01<00:00,  1.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.51.0     | 923 KB    | : 100% 1.0/1 [00:01<00:00,  1.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:02<00:00,  2.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:02<00:00,  2.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:02<00:00,  2.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:02<00:00,  2.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:02<00:00,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:02<00:00,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:02<00:00,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:02<00:00,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:02<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:02<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:03<00:00,  2.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:03<00:00,  2.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "icu-75.1             | 11.6 MB   | : 100% 1.0/1 [00:04<00:00,  1.72it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 254 KB    | : 100% 1.0/1 [00:04<00:00,  3.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 254 KB    | : 100% 1.0/1 [00:04<00:00,  3.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:04<00:00,  3.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:04<00:00,  3.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:04<00:00,  3.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:04<00:00,  3.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:04<00:00,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:04<00:00,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:04<00:00,  4.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:04<00:00,  4.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | : 100% 1.0/1 [00:04<00:00,  4.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.19       | 24.1 MB   | : 100% 1.0/1 [00:05<00:00,  1.73s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Installing pip dependencies: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| Ran pip subprocess with arguments:\n",
            "['/content/miniconda3/envs/deepmlf/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/deepmlf/condaenv.9s9bc87i.requirements.txt', '--exists-action=b']\n",
            "Pip subprocess output:\n",
            "Collecting torch==1.13.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting transformers==4.26.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 2))\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
            "Collecting datasets==2.12.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting huggingface-hub==0.12.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 4))\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting librosa==0.9.2 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting audiofile==1.1.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 6))\n",
            "  Downloading audiofile-1.1.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting audioread==3.0.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 7))\n",
            "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting soundfile==0.11.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 8))\n",
            "  Downloading soundfile-0.11.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting opensmile==2.4.2 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 9))\n",
            "  Downloading opensmile-2.4.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting python-speech-features==0.6 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 10))\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting resampy==0.4.2 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 11))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting opencv-contrib-python==4.7.0.68 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 12))\n",
            "  Downloading opencv_contrib_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting mediapipe>=0.10.5 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting pillow==9.4.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 14))\n",
            "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting numpy==1.23.5 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 15))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==1.5.3 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 16))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit-learn==1.2.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 17))\n",
            "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 18))\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting matplotlib==3.6.3 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting tqdm==4.64.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 20))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "Collecting requests==2.28.2 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 21))\n",
            "  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyyaml==6.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 22))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting click==8.1.3 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 23))\n",
            "  Downloading click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting gdown==4.6.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 24))\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting ipython==8.9.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading ipython-8.9.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jupyter (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting audeer==1.19.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 27))\n",
            "  Downloading audeer-1.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audformat==0.16.0 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 28))\n",
            "  Downloading audformat-0.16.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting audinterface==0.10.2 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 29))\n",
            "  Downloading audinterface-0.10.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting audobject==0.7.7 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 30))\n",
            "  Downloading audobject-0.7.7-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting audresample==1.2.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 31))\n",
            "  Downloading audresample-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting mmsa-fet==0.4.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 32))\n",
            "  Downloading MMSA_FET-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ctc-segmentation==1.7.4 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 33))\n",
            "  Downloading ctc_segmentation-1.7.4.tar.gz (73 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting scenedetect==0.6.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 34))\n",
            "  Downloading scenedetect-0.6.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pylatex==1.4.1 (from -r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 35))\n",
            "  Downloading PyLaTeX-1.4.1.tar.gz (84 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting typing-extensions (from torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting filelock (from transformers==4.26.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 2))\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.26.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 2))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.26.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 2))\n",
            "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pyarrow>=8.0.0 (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting xxhash (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
            "Collecting fsspec>=2021.11.1 (from fsspec[http]>=2021.11.1->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiohttp (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting responses<0.19 (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting joblib>=0.14 (from librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting decorator>=4.0.10 (from librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting numba>=0.45.1 (from librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting pooch>=1.0 (from librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cffi>=1.0 (from soundfile==0.11.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 8))\n",
            "  Downloading cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting python-dateutil>=2.8.1 (from pandas==1.5.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 16))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==1.5.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 16))\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 17))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.6.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib==3.6.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib==3.6.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib==3.6.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pyparsing>=2.2.1 (from matplotlib==3.6.3->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 19))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests==2.28.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 21))\n",
            "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests==2.28.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 21))\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests==2.28.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 21))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests==2.28.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 21))\n",
            "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting six (from gdown==4.6.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 24))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting beautifulsoup4 (from gdown==4.6.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 24))\n",
            "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting backcall (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting jedi>=0.16 (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading matplotlib_inline-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pickleshare (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.30 (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting stack-data (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5 (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pexpect>4.3 (from ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting iso-639 (from audformat==0.16.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 28))\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting iso3166 (from audformat==0.16.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 28))\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting oyaml (from audformat==0.16.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 28))\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting importlib-metadata>=4.8.0 (from audobject==0.7.7->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 30))\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: setuptools in /content/miniconda3/envs/deepmlf/lib/python3.10/site-packages (from ctc-segmentation==1.7.4->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 33)) (80.9.0)\n",
            "Collecting Cython (from ctc-segmentation==1.7.4->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 33))\n",
            "  Using cached cython-3.2.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting appdirs (from scenedetect==0.6.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 34))\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting ordered-set (from pylatex==1.4.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 35))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: wheel in /content/miniconda3/envs/deepmlf/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 1)) (0.45.1)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.30->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting absl-py (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting attrs>=19.1.0 (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flatbuffers>=2.0 (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.6.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting sentencepiece (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting notebook (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading notebook-7.4.7-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jupyter-console (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nbconvert (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting ipykernel (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading ipykernel-7.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting ipywidgets (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting jupyterlab (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab-4.4.10-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile==0.11.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 8))\n",
            "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata>=4.8.0->audobject==0.7.7->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 30))\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.45.1->librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting platformdirs>=2.5.0 (from pooch>=1.0->librosa==0.9.2->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 5))\n",
            "  Downloading platformdirs-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->gdown==4.6.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 24))\n",
            "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading debugpy-1.8.17-cp310-cp310-manylinux_2_34_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting jupyter-client>=8.0.0 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_core-5.9.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting nest-asyncio>=1.4 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting psutil>=5.7 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting pyzmq>=25 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading pyzmq-27.1.0-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting tornado>=6.2 (from ipykernel->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.6.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.6.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.5.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.5.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.5.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.38-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.33-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting opt-einsum (from jax->mediapipe>=0.10.5->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 13))\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting httpx<1,>=0.25.0 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jinja2>=3.0.3 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting tomli>=1.2.2 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting anyio (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "INFO: pip is looking at multiple versions of jupyterlab-server to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.27.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.27.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jupyterlab (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab-4.4.9-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: pip is still looking at multiple versions of jupyterlab-server to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jupyterlab-4.4.8-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.7-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.5-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.4-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.8-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.7-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.6-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.4-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.3-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.2-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.7-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.6-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.5-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.4-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.3-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.2-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.8-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.7-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting jupyterlab-server<3,>=2.19.0 (from jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab_server-2.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.26.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.25.4-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.25.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.25.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.25.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.25.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading jupyterlab_server-2.24.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading rpds_py-0.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading webcolors-25.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting defusedxml (from nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting jupyterlab-pygments (from nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting nbclient>=0.5.0 (from nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading lark-1.3.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading arrow-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting tzdata (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.12.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting notebook (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading notebook-7.4.6-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.4.4-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.4.3-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.4.2-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jupyterlab (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading jupyterlab-4.4.0rc1-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: pip is still looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting notebook (from jupyter->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 26))\n",
            "  Downloading notebook-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading notebook-7.3.3-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading notebook-7.1.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown==4.6.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 24))\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.9.0->-r /content/deepmlf/condaenv.9s9bc87i.requirements.txt (line 25))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.5/887.5 MB 20.7 MB/s  0:00:18\n",
            "Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 132.6 MB/s  0:00:00\n",
            "Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "Downloading audiofile-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading opensmile-2.4.2-py3-none-any.whl (4.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 105.1 MB/s  0:00:00\n",
            "Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.0 MB/s  0:00:00\n",
            "Downloading opencv_contrib_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.9/67.9 MB 43.3 MB/s  0:00:01\n",
            "Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 116.3 MB/s  0:00:00\n",
            "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 155.4 MB/s  0:00:00\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 164.9 MB/s  0:00:00\n",
            "Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 144.1 MB/s  0:00:00\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 51.4 MB/s  0:00:00\n",
            "Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 157.7 MB/s  0:00:00\n",
            "Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 682.2/682.2 kB 29.6 MB/s  0:00:00\n",
            "Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Downloading ipython-8.9.0-py3-none-any.whl (783 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 783.1/783.1 kB 43.9 MB/s  0:00:00\n",
            "Downloading audeer-1.19.0-py3-none-any.whl (20 kB)\n",
            "Downloading audformat-0.16.0-py3-none-any.whl (63 kB)\n",
            "Downloading audinterface-0.10.2-py3-none-any.whl (30 kB)\n",
            "Downloading audresample-1.2.1-py3-none-any.whl (494 kB)\n",
            "Downloading audobject-0.7.7-py3-none-any.whl (24 kB)\n",
            "Downloading MMSA_FET-0.4.1-py3-none-any.whl (72 kB)\n",
            "Downloading scenedetect-0.6.1-py3-none-any.whl (115 kB)\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 60.2 MB/s  0:00:04\n",
            "Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 152.7 MB/s  0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 32.8 MB/s  0:00:00\n",
            "Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 22.0 MB/s  0:00:13\n",
            "Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
            "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 101.8 MB/s  0:00:00\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "Downloading mediapipe-0.10.21-cp310-cp310-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.6/35.6 MB 60.1 MB/s  0:00:00\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Downloading cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
            "Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 100.6 MB/s  0:00:00\n",
            "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
            "Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 52.5 MB/s  0:00:00\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
            "Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 63.9 MB/s  0:00:00\n",
            "Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 81.0 MB/s  0:00:00\n",
            "Downloading numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/3.7 MB 76.4 MB/s  0:00:00\n",
            "Downloading llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 54.4 MB/s  0:00:01\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "Downloading platformdirs-4.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
            "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.6/47.6 MB 53.9 MB/s  0:00:00\n",
            "Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 67.7 MB/s  0:00:00\n",
            "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 791.7/791.7 kB 45.3 MB/s  0:00:00\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
            "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
            "Using cached cython-3.2.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Downloading ipykernel-7.1.0-py3-none-any.whl (117 kB)\n",
            "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading debugpy-1.8.17-cp310-cp310-manylinux_2_34_x86_64.whl (3.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 98.8 MB/s  0:00:00\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "Downloading jupyter_core-5.9.1-py3-none-any.whl (29 kB)\n",
            "Downloading matplotlib_inline-0.2.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
            "Downloading pyzmq-27.1.0-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (854 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 854.1/854.1 kB 54.6 MB/s  0:00:00\n",
            "Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 914.9/914.9 kB 55.3 MB/s  0:00:00\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 102.2 MB/s  0:00:00\n",
            "Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 100.1 MB/s  0:00:00\n",
            "Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 MB 60.2 MB/s  0:00:01\n",
            "Downloading ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 119.8 MB/s  0:00:00\n",
            "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading jupyterlab-4.1.6-py3-none-any.whl (11.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.4/11.4 MB 143.6 MB/s  0:00:00\n",
            "Downloading jupyterlab_server-2.24.0-py3-none-any.whl (57 kB)\n",
            "Downloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
            "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
            "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 141.6 MB/s  0:00:00\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
            "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
            "Downloading mistune-3.1.4-py3-none-any.whl (53 kB)\n",
            "Downloading bleach-6.3.0-py3-none-any.whl (164 kB)\n",
            "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
            "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
            "Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
            "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
            "Downloading python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
            "Downloading rpds_py-0.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
            "Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
            "Downloading tomli-2.3.0-py3-none-any.whl (14 kB)\n",
            "Downloading webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
            "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
            "Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (87 kB)\n",
            "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.4.0-py3-none-any.whl (68 kB)\n",
            "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "Downloading notebook-7.1.3-py3-none-any.whl (5.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 118.5 MB/s  0:00:00\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 68.9 MB/s  0:00:00\n",
            "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
            "Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Building wheels for collected packages: audioread, python-speech-features, ctc-segmentation, pylatex, iso-639\n",
            "  Building wheel for audioread (setup.py): started\n",
            "  Building wheel for audioread (setup.py): finished with status 'done'\n",
            "  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23783 sha256=d0112e7061ebcbf3f883279f2a40033e0eebbe595b3a9d4c4b5cfbdd0cddfd6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/4b/39/c5f6c4ee93b43281dda4dab5ac5f2bdf9d11074d427493cd55\n",
            "  Building wheel for python-speech-features (setup.py): started\n",
            "  Building wheel for python-speech-features (setup.py): finished with status 'done'\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-py3-none-any.whl size=5897 sha256=1ff66fddd575af8ed087ecbcf615222542e3c8ba5e8e58fcf4cc4dab8fb39c5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "  Building wheel for ctc-segmentation (pyproject.toml): started\n",
            "  Building wheel for ctc-segmentation (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for ctc-segmentation: filename=ctc_segmentation-1.7.4-cp310-cp310-linux_x86_64.whl size=45647 sha256=b33ff1499b8b1a66cdcf35d9dc6ba4102e90febc0108fb8f2f6e98775d878773\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/8d/64/062be0990aac6687780c9f5998af6ba110daad94354a57cea0\n",
            "  Building wheel for pylatex (setup.py): started\n",
            "  Building wheel for pylatex (setup.py): finished with status 'done'\n",
            "  Created wheel for pylatex: filename=pylatex-1.4.1-py3-none-any.whl size=42855 sha256=833c2aecafa0adcd97c4c4f4243124b46da7374cdaf59ed605ec2d4056876135\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/6d/a1/716565cec073a7ccebc1eb9a7c061658a75d5559c16f0ee875\n",
            "  Building wheel for iso-639 (setup.py): started\n",
            "  Building wheel for iso-639 (setup.py): finished with status 'done'\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=168876 sha256=70c49fffd6eaa77171ce29c616882fbaa0d644e13b5d34449b8a32b059414ddf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/78/cc/5478ca3b1c3f602eae6f8cdbd78f909c0a0bfa0bbcb5c7771f\n",
            "Successfully built audioread python-speech-features ctc-segmentation pylatex iso-639\n",
            "Installing collected packages: webencodings, tokenizers, pytz, python-speech-features, pure-eval, ptyprocess, pickleshare, iso-639, flatbuffers, fastjsonschema, backcall, appdirs, zipp, xxhash, widgetsnbextension, websocket-client, webcolors, wcwidth, urllib3, uri-template, tzdata, typing-extensions, traitlets, tqdm, tornado, tomli, tinycss2, threadpoolctl, soupsieve, sniffio, six, sentencepiece, send2trash, rpds-py, rfc3986-validator, regex, pyzmq, pyyaml, python-json-logger, PySocks, pyparsing, pygments, pycparser, pyarrow, psutil, protobuf, propcache, prometheus-client, platformdirs, pillow, pexpect, parso, pandocfilters, packaging, overrides, ordered-set, opt-einsum, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, nest-asyncio, MarkupSafe, llvmlite, lark, kiwisolver, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, joblib, iso3166, idna, h11, fsspec, frozenlist, fqdn, fonttools, filelock, executing, dill, defusedxml, decorator, debugpy, Cython, cycler, comm, click, charset-normalizer, certifi, bleach, babel, audioread, attrs, async-timeout, asttokens, aiohappyeyeballs, absl-py, terminado, stack-data, scipy, scenedetect, rfc3987-syntax, rfc3339-validator, requests, referencing, python-dateutil, pylatex, prompt-toolkit, oyaml, opencv-contrib-python, nvidia-cudnn-cu11, numba, multiprocess, multidict, ml_dtypes, mistune, matplotlib-inline, jupyter-core, jinja2, jedi, importlib-metadata, httpcore, exceptiongroup, ctc-segmentation, contourpy, cffi, beautifulsoup4, audresample, audeer, async-lru, aiosignal, yarl, torch, soundfile, sounddevice, scikit-learn, responses, resampy, pooch, pandas, matplotlib, jupyter-server-terminals, jupyter-client, jsonschema-specifications, jaxlib, ipython, huggingface-hub, audobject, arrow, argon2-cffi-bindings, anyio, transformers, librosa, jsonschema, jax, isoduration, ipywidgets, ipykernel, httpx, gdown, audiofile, argon2-cffi, aiohttp, nbformat, mediapipe, jupyter-console, audformat, nbclient, jupyter-events, datasets, audinterface, opensmile, nbconvert, mmsa-fet, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
            "\n",
            "Successfully installed Cython-3.2.0 MarkupSafe-3.0.3 PySocks-1.7.1 absl-py-2.3.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 appdirs-1.4.4 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.4.0 asttokens-3.0.0 async-lru-2.0.5 async-timeout-5.0.1 attrs-25.4.0 audeer-1.19.0 audformat-0.16.0 audinterface-0.10.2 audiofile-1.1.1 audioread-3.0.0 audobject-0.7.7 audresample-1.2.1 babel-2.17.0 backcall-0.2.0 beautifulsoup4-4.14.2 bleach-6.3.0 certifi-2025.10.5 cffi-2.0.0 charset-normalizer-3.4.4 click-8.1.3 comm-0.2.3 contourpy-1.3.2 ctc-segmentation-1.7.4 cycler-0.12.1 datasets-2.12.0 debugpy-1.8.17 decorator-5.2.1 defusedxml-0.7.1 dill-0.3.6 exceptiongroup-1.3.0 executing-2.2.1 fastjsonschema-2.21.2 filelock-3.20.0 flatbuffers-25.9.23 fonttools-4.60.1 fqdn-1.5.1 frozenlist-1.8.0 fsspec-2025.10.0 gdown-4.6.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.12.0 idna-3.11 importlib-metadata-8.7.0 ipykernel-7.1.0 ipython-8.9.0 ipywidgets-8.1.8 iso-639-0.4.5 iso3166-2.1.1 isoduration-20.11.0 jax-0.4.30 jaxlib-0.4.30 jedi-0.19.2 jinja2-3.1.6 joblib-1.5.2 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-client-8.6.3 jupyter-console-6.6.3 jupyter-core-5.9.1 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.1.6 jupyterlab-pygments-0.3.0 jupyterlab-server-2.24.0 jupyterlab_widgets-3.0.16 kiwisolver-1.4.9 lark-1.3.1 librosa-0.9.2 llvmlite-0.45.1 matplotlib-3.6.3 matplotlib-inline-0.2.1 mediapipe-0.10.21 mistune-3.1.4 ml_dtypes-0.5.3 mmsa-fet-0.4.1 multidict-6.7.0 multiprocess-0.70.14 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nest-asyncio-1.6.0 notebook-7.1.3 notebook-shim-0.2.4 numba-0.62.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opencv-contrib-python-4.7.0.68 opensmile-2.4.2 opt-einsum-3.4.0 ordered-set-4.1.0 overrides-7.7.0 oyaml-1.0 packaging-25.0 pandas-1.5.3 pandocfilters-1.5.1 parso-0.8.5 pexpect-4.9.0 pickleshare-0.7.5 pillow-9.4.0 platformdirs-4.5.0 pooch-1.8.2 prometheus-client-0.23.1 prompt-toolkit-3.0.52 propcache-0.4.1 protobuf-4.25.8 psutil-7.1.3 ptyprocess-0.7.0 pure-eval-0.2.3 pyarrow-22.0.0 pycparser-2.23 pygments-2.19.2 pylatex-1.4.1 pyparsing-3.2.5 python-dateutil-2.9.0.post0 python-json-logger-4.0.0 python-speech-features-0.6 pytz-2025.2 pyyaml-6.0 pyzmq-27.1.0 referencing-0.37.0 regex-2025.11.3 requests-2.28.2 resampy-0.4.2 responses-0.18.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.28.0 scenedetect-0.6.1 scikit-learn-1.2.1 scipy-1.10.0 send2trash-1.8.3 sentencepiece-0.2.1 six-1.17.0 sniffio-1.3.1 sounddevice-0.5.3 soundfile-0.11.0 soupsieve-2.8 stack-data-0.6.3 terminado-0.18.1 threadpoolctl-3.6.0 tinycss2-1.4.0 tokenizers-0.13.3 tomli-2.3.0 torch-1.13.1 tornado-6.5.2 tqdm-4.64.1 traitlets-5.14.3 transformers-4.26.0 typing-extensions-4.15.0 tzdata-2025.2 uri-template-1.3.0 urllib3-1.26.20 wcwidth-0.2.14 webcolors-25.10.0 webencodings-0.5.1 websocket-client-1.9.0 widgetsnbextension-4.0.15 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate deepmlf\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main && \\\n",
        "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r && \\\n",
        "conda env create -f environment.yml -n deepmlf && \\\n",
        "conda activate deepmlf\n",
        "pip install pyarrow==14.0.2 --force-reinstall && \\\n",
        "pip install numpy==1.26.4 --force-reinstall && \\\n",
        "pip install tiktoken pynvml easydict tensorflow keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MOSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AV Encoder Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXJsVNHUu5RF",
        "outputId": "8e09c8df-23d4-41d8-c6a4-02cde0b8db93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "no change     /root/.zshrc\n",
            "no change     /root/.config/fish/config.fish\n",
            "no change     /root/.xonshrc\n",
            "no change     /root/.tcshrc\n",
            "No action taken.\n",
            "2025-11-08 10:49:05.244937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/functions.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/schedulers.py:108: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  params_decay = [p for n, p in model.named_parameters() if n is not 'bias']\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/schedulers.py:109: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  params_no_decay = [p for n, p in model.named_parameters() if n is 'bias']\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "MMSA - {'model_name': 'bienc', 'dataset_name': 'mosi', 'featurePath': '/content/drive/MyDrive/MSA-Datasets/CMU-MOSI/Processed/unaligned_50.pkl', 'seq_lens': [50, 375, 500], 'feature_dims': [768, 5, 20], 'train_samples': 1284, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 8, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'update_epochs': 4, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'fs2': False, 'max_token_len': 50, 'pad_token': '<|endoftext|>', 'lm': 'gpt2', 'gpt': {'mm_dropout': 0.1, 'mm_layer': [3, 5, 7, 9, 11], 'layer_dropout': 0.2, 'dense': True, 'tie_ffn': True}, 'av_enc': {'d_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 50, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_ln': False, 'use_bn': {'use_bn_a': True, 'use_bn_v': False}}, 'batch_size': 32, 'learning_rate': 0.001, 'grad_clip': 1.0, 'patience': 10, 'weight_decay': 0.001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/1] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(1284, 375, 5)\n",
            "(1284, 500, 20)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(229, 375, 5)\n",
            "(229, 500, 20)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(686, 375, 5)\n",
            "(686, 500, 20)\n",
            "Ongoing with num_workers=2\n",
            "Using BN_a\n",
            "MMSA - The model has 69761 trainable parameters\n",
            "ongoing with bienc\n",
            "------- Ongoing with the ReduceLROnPlateau -----------\n",
            "100% 41/41 [00:17<00:00,  2.31it/s]\n",
            "MMSA - TRAIN-(bienc) [1/1/1] >> loss: 1.3449  Has0_acc_2: 0.5561  Has0_F1_score: 0.5397  Non0_acc_2: 0.5467  Non0_F1_score: 0.5297  Mult_acc_5: 0.2017  Mult_acc_7: 0.2002  MAE: 1.3475  Corr: 0.0713 \n",
            "100% 8/8 [00:02<00:00,  3.11it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5284  Has0_F1_score: 0.5328  Non0_acc_2: 0.5231  Non0_F1_score: 0.5257  Mult_acc_5: 0.2183  Mult_acc_7: 0.2183  MAE: 1.4808  Corr: 0.0205  Loss: 1.5055 \n",
            "***********************************************Saving Model at checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "100% 41/41 [00:15<00:00,  2.69it/s]\n",
            "MMSA - TRAIN-(bienc) [1/2/1] >> loss: 1.3049  Has0_acc_2: 0.5732  Has0_F1_score: 0.5681  Non0_acc_2: 0.5654  Non0_F1_score: 0.5599  Mult_acc_5: 0.2033  Mult_acc_7: 0.2033  MAE: 1.3043  Corr: 0.1272 \n",
            "100% 8/8 [00:02<00:00,  2.94it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5721  Has0_F1_score: 0.4679  Non0_acc_2: 0.5463  Non0_F1_score: 0.4399  Mult_acc_5: 0.2183  Mult_acc_7: 0.2183  MAE: 1.4153  Corr: 0.0406  Loss: 1.3818 \n",
            "***********************************************Saving Model at checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "100% 41/41 [00:15<00:00,  2.66it/s]\n",
            "MMSA - TRAIN-(bienc) [1/3/1] >> loss: 1.3187  Has0_acc_2: 0.5771  Has0_F1_score: 0.5121  Non0_acc_2: 0.5646  Non0_F1_score: 0.4979  Mult_acc_5: 0.1947  Mult_acc_7: 0.1947  MAE: 1.3027  Corr: 0.1121 \n",
            "100% 8/8 [00:02<00:00,  3.26it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5764  Has0_F1_score: 0.5317  Non0_acc_2: 0.5509  Non0_F1_score: 0.5065  Mult_acc_5: 0.1921  Mult_acc_7: 0.1921  MAE: 1.4205  Corr: 0.0521  Loss: 1.4528 \n",
            "100% 41/41 [00:15<00:00,  2.71it/s]\n",
            "MMSA - TRAIN-(bienc) [2/4/1] >> loss: 1.3041  Has0_acc_2: 0.5880  Has0_F1_score: 0.5677  Non0_acc_2: 0.5784  Non0_F1_score: 0.5576  Mult_acc_5: 0.1908  Mult_acc_7: 0.1908  MAE: 1.2921  Corr: 0.1544 \n",
            "100% 8/8 [00:03<00:00,  2.03it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5895  Has0_F1_score: 0.5561  Non0_acc_2: 0.5694  Non0_F1_score: 0.5356  Mult_acc_5: 0.1965  Mult_acc_7: 0.1965  MAE: 1.4244  Corr: 0.0731  Loss: 1.4303 \n",
            "100% 41/41 [00:15<00:00,  2.69it/s]\n",
            "MMSA - TRAIN-(bienc) [3/5/1] >> loss: 1.2914  Has0_acc_2: 0.5787  Has0_F1_score: 0.5520  Non0_acc_2: 0.5678  Non0_F1_score: 0.5407  Mult_acc_5: 0.1939  Mult_acc_7: 0.1939  MAE: 1.2858  Corr: 0.1631 \n",
            "100% 8/8 [00:02<00:00,  2.94it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5983  Has0_F1_score: 0.5543  Non0_acc_2: 0.5787  Non0_F1_score: 0.5339  Mult_acc_5: 0.2183  Mult_acc_7: 0.2140  MAE: 1.4389  Corr: 0.0889  Loss: 1.4612 \n",
            "100% 41/41 [00:15<00:00,  2.69it/s]\n",
            "MMSA - TRAIN-(bienc) [4/6/1] >> loss: 1.2703  Has0_acc_2: 0.6160  Has0_F1_score: 0.6132  Non0_acc_2: 0.6141  Non0_F1_score: 0.6106  Mult_acc_5: 0.2002  Mult_acc_7: 0.2002  MAE: 1.2687  Corr: 0.2218 \n",
            "100% 8/8 [00:02<00:00,  3.04it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6201  Has0_F1_score: 0.5398  Non0_acc_2: 0.5972  Non0_F1_score: 0.5155  Mult_acc_5: 0.2358  Mult_acc_7: 0.2140  MAE: 1.4447  Corr: 0.1054  Loss: 1.5039 \n",
            "100% 41/41 [00:15<00:00,  2.69it/s]\n",
            "MMSA - TRAIN-(bienc) [5/7/1] >> loss: 1.2623  Has0_acc_2: 0.5966  Has0_F1_score: 0.5833  Non0_acc_2: 0.5890  Non0_F1_score: 0.5752  Mult_acc_5: 0.1955  Mult_acc_7: 0.1955  MAE: 1.2721  Corr: 0.2073 \n",
            "100% 8/8 [00:02<00:00,  2.97it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6201  Has0_F1_score: 0.5990  Non0_acc_2: 0.6111  Non0_F1_score: 0.5885  Mult_acc_5: 0.2271  Mult_acc_7: 0.2271  MAE: 1.3725  Corr: 0.1800  Loss: 1.3434 \n",
            "***********************************************Saving Model at checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "100% 41/41 [00:15<00:00,  2.67it/s]\n",
            "MMSA - TRAIN-(bienc) [1/8/1] >> loss: 1.264  Has0_acc_2: 0.6238  Has0_F1_score: 0.5997  Non0_acc_2: 0.6166  Non0_F1_score: 0.5917  Mult_acc_5: 0.2033  Mult_acc_7: 0.2017  MAE: 1.2534  Corr: 0.2484 \n",
            "100% 8/8 [00:03<00:00,  2.04it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6026  Has0_F1_score: 0.5322  Non0_acc_2: 0.5787  Non0_F1_score: 0.5074  Mult_acc_5: 0.2576  Mult_acc_7: 0.2314  MAE: 1.4313  Corr: 0.1257  Loss: 1.4111 \n",
            "100% 41/41 [00:14<00:00,  2.77it/s]\n",
            "MMSA - TRAIN-(bienc) [2/9/1] >> loss: 1.2325  Has0_acc_2: 0.6199  Has0_F1_score: 0.6118  Non0_acc_2: 0.6109  Non0_F1_score: 0.6026  Mult_acc_5: 0.2227  Mult_acc_7: 0.2220  MAE: 1.2517  Corr: 0.2413 \n",
            "100% 8/8 [00:02<00:00,  2.86it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6026  Has0_F1_score: 0.5785  Non0_acc_2: 0.5926  Non0_F1_score: 0.5666  Mult_acc_5: 0.2445  Mult_acc_7: 0.2358  MAE: 1.3881  Corr: 0.1588  Loss: 1.3353 \n",
            "***********************************************Saving Model at checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "100% 41/41 [00:15<00:00,  2.56it/s]\n",
            "MMSA - TRAIN-(bienc) [1/10/1] >> loss: 1.2537  Has0_acc_2: 0.6168  Has0_F1_score: 0.6134  Non0_acc_2: 0.6117  Non0_F1_score: 0.6078  Mult_acc_5: 0.2017  Mult_acc_7: 0.2017  MAE: 1.2415  Corr: 0.2806 \n",
            "100% 8/8 [00:02<00:00,  3.22it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6332  Has0_F1_score: 0.5701  Non0_acc_2: 0.6157  Non0_F1_score: 0.5507  Mult_acc_5: 0.2445  Mult_acc_7: 0.2052  MAE: 1.4203  Corr: 0.1682  Loss: 1.4303 \n",
            "100% 41/41 [00:14<00:00,  2.75it/s]\n",
            "MMSA - TRAIN-(bienc) [2/11/1] >> loss: 1.251  Has0_acc_2: 0.6254  Has0_F1_score: 0.6139  Non0_acc_2: 0.6190  Non0_F1_score: 0.6072  Mult_acc_5: 0.2072  Mult_acc_7: 0.2064  MAE: 1.2445  Corr: 0.2761 \n",
            "100% 8/8 [00:03<00:00,  2.67it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6201  Has0_F1_score: 0.5398  Non0_acc_2: 0.5972  Non0_F1_score: 0.5155  Mult_acc_5: 0.2271  Mult_acc_7: 0.1965  MAE: 1.4191  Corr: 0.1655  Loss: 1.4437 \n",
            "100% 41/41 [00:15<00:00,  2.60it/s]\n",
            "MMSA - TRAIN-(bienc) [3/12/1] >> loss: 1.2457  Has0_acc_2: 0.6246  Has0_F1_score: 0.6224  Non0_acc_2: 0.6198  Non0_F1_score: 0.6172  Mult_acc_5: 0.2048  Mult_acc_7: 0.2048  MAE: 1.2422  Corr: 0.2626 \n",
            "100% 8/8 [00:03<00:00,  2.32it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.5939  Has0_F1_score: 0.4805  Non0_acc_2: 0.5694  Non0_F1_score: 0.4531  Mult_acc_5: 0.2402  Mult_acc_7: 0.1965  MAE: 1.4701  Corr: 0.1824  Loss: 1.4608 \n",
            "100% 41/41 [00:15<00:00,  2.73it/s]\n",
            "MMSA - TRAIN-(bienc) [4/13/1] >> loss: 1.2229  Has0_acc_2: 0.6293  Has0_F1_score: 0.6001  Non0_acc_2: 0.6190  Non0_F1_score: 0.5895  Mult_acc_5: 0.2118  Mult_acc_7: 0.2079  MAE: 1.2347  Corr: 0.2796 \n",
            "100% 8/8 [00:02<00:00,  2.82it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6026  Has0_F1_score: 0.5863  Non0_acc_2: 0.5926  Non0_F1_score: 0.5749  Mult_acc_5: 0.2096  Mult_acc_7: 0.2096  MAE: 1.3778  Corr: 0.1824  Loss: 1.3776 \n",
            "100% 41/41 [00:14<00:00,  2.76it/s]\n",
            "MMSA - TRAIN-(bienc) [5/14/1] >> loss: 1.2334  Has0_acc_2: 0.6223  Has0_F1_score: 0.6240  Non0_acc_2: 0.6223  Non0_F1_score: 0.6232  Mult_acc_5: 0.1970  Mult_acc_7: 0.1970  MAE: 1.2477  Corr: 0.2931 \n",
            "100% 8/8 [00:02<00:00,  2.94it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6376  Has0_F1_score: 0.5733  Non0_acc_2: 0.6204  Non0_F1_score: 0.5541  Mult_acc_5: 0.2489  Mult_acc_7: 0.2052  MAE: 1.4313  Corr: 0.1814  Loss: 1.3898 \n",
            "100% 41/41 [00:15<00:00,  2.72it/s]\n",
            "MMSA - TRAIN-(bienc) [6/15/1] >> loss: 1.24  Has0_acc_2: 0.6168  Has0_F1_score: 0.6096  Non0_acc_2: 0.6101  Non0_F1_score: 0.6025  Mult_acc_5: 0.1994  Mult_acc_7: 0.1986  MAE: 1.2385  Corr: 0.2844 \n",
            "100% 8/8 [00:03<00:00,  2.28it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6201  Has0_F1_score: 0.5302  Non0_acc_2: 0.5972  Non0_F1_score: 0.5054  Mult_acc_5: 0.2445  Mult_acc_7: 0.1965  MAE: 1.4792  Corr: 0.1819  Loss: 1.4451 \n",
            "100% 41/41 [00:15<00:00,  2.70it/s]\n",
            "MMSA - TRAIN-(bienc) [7/16/1] >> loss: 1.2424  Has0_acc_2: 0.6207  Has0_F1_score: 0.6205  Non0_acc_2: 0.6158  Non0_F1_score: 0.6150  Mult_acc_5: 0.2072  Mult_acc_7: 0.2048  MAE: 1.2352  Corr: 0.2892 \n",
            "100% 8/8 [00:02<00:00,  3.01it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6332  Has0_F1_score: 0.5579  Non0_acc_2: 0.6111  Non0_F1_score: 0.5345  Mult_acc_5: 0.2576  Mult_acc_7: 0.2096  MAE: 1.4688  Corr: 0.1722  Loss: 1.4439 \n",
            "100% 41/41 [00:16<00:00,  2.53it/s]\n",
            "MMSA - TRAIN-(bienc) [8/17/1] >> loss: 1.251  Has0_acc_2: 0.6262  Has0_F1_score: 0.6149  Non0_acc_2: 0.6198  Non0_F1_score: 0.6081  Mult_acc_5: 0.2079  Mult_acc_7: 0.2072  MAE: 1.2380  Corr: 0.2783 \n",
            "100% 8/8 [00:02<00:00,  3.11it/s]\n",
            "MMSA - VAL-(bienc) >>  Has0_acc_2: 0.6376  Has0_F1_score: 0.5733  Non0_acc_2: 0.6204  Non0_F1_score: 0.5541  Mult_acc_5: 0.2271  Mult_acc_7: 0.2052  MAE: 1.3876  Corr: 0.2014  Loss: 1.3358 \n",
            "***************** Loading Model from checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "100% 22/22 [00:08<00:00,  2.58it/s]\n",
            "MMSA - TEST-(bienc) >>  Has0_acc_2: 0.5598  Has0_F1_score: 0.5516  Non0_acc_2: 0.5549  Non0_F1_score: 0.5482  Mult_acc_5: 0.1953  Mult_acc_7: 0.1953  MAE: 1.4016  Corr: 0.2236  Loss: 1.4014 \n",
            "MMSA - Result for seed 1990: {'Has0_acc_2': 0.5598, 'Has0_F1_score': 0.5516, 'Non0_acc_2': 0.5549, 'Non0_F1_score': 0.5482, 'Mult_acc_5': 0.1953, 'Mult_acc_7': 0.1953, 'MAE': 1.4016, 'Corr': 0.2236, 'Loss': 1.4014, 'seed': 1990}\n",
            "MMSA - Results saved to MMSA/results/bienc_mosi/bienc-mosi/normal/mosi_avg.csv.\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "  -m bienc \\\n",
        "  -d mosi \\\n",
        "  -g 0 \\\n",
        "  --exp-name bienc-mosi \\\n",
        "  -c MMSA/config/regression/deepmlf/mosi/my_bienc.json \\\n",
        "  --res-save-dir MMSA/results/bienc_mosi \\\n",
        "  -n 2 \\\n",
        "  -s 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76kNRurDG_Ji",
        "outputId": "4cf1d840-058d-4b24-c1fe-0da02ef4be15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "no change     /root/.zshrc\n",
            "no change     /root/.config/fish/config.fish\n",
            "no change     /root/.xonshrc\n",
            "no change     /root/.tcshrc\n",
            "No action taken.\n",
            "2025-11-08 11:54:44.821992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/functions.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "MMSA - {'model_name': 'msalm', 'dataset_name': 'mosi', 'featurePath': '/content/drive/MyDrive/MSA-Datasets/CMU-MOSI/Processed/unaligned_50.pkl', 'seq_lens': [50, 375, 500], 'feature_dims': [768, 5, 20], 'train_samples': 1284, 'num_classes': 3, 'language': 'en', 'KeyEval': 'MAE', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 30, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': True, 'update_labels_patience': 2, 'H': 3.0, 'del_model': True, 'max_token_len': 50, 'pad_token': '<|endoftext|>', 'lm': 'gpt2', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.1, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 15, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 12, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [3, 5, 7, 8, 9, 10, 11], 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.2, 'p_apply': 0.3, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': 'checkpoints/bienc-mosi/bienc-mosi-1990.pth', 'feature_dims': [768, 1024, 768], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 50, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.3, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': False, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.001, 'weight_decay_av': 0.001, 'learning_rate_av': 0.0001, 'learning_rate_mmgpt': 0.0001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990, 1991]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/2] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(1284, 375, 5)\n",
            "(1284, 500, 20)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(229, 375, 5)\n",
            "(229, 500, 20)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(686, 375, 5)\n",
            "(686, 500, 20)\n",
            "Ongoing with num_workers=2\n",
            "ca list is: [3, 5, 7, 8, 9, 10, 11]\n",
            "initializing SoftPerm\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 0\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 2\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 3\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 4\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 5\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 6\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Parsing decoder block: 0\n",
            "Parsing decoder block: 1\n",
            "Parsing decoder block: 2\n",
            "Parsing decoder block: 3\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 4\n",
            "Parsing decoder block: 5\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 6\n",
            "Parsing decoder block: 7\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 8\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 9\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 10\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 11\n",
            "COpying---------------------------------\n",
            "No normalization is used\n",
            "----------------->>> Pretrained AudioVisual Encoder <<<<<----------------\n",
            "No normalization is used\n",
            "----------------------- Loading AV encoder from checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "Copied param embed_positions_a._float_tensor\n",
            "Copied param embed_positions_v._float_tensor\n",
            "Copied param proj_a.weight\n",
            "Copied param proj_v.weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.0.linear1.weight\n",
            "Copied param enc_a.layers.0.linear1.bias\n",
            "Copied param enc_a.layers.0.linear2.weight\n",
            "Copied param enc_a.layers.0.linear2.bias\n",
            "Copied param enc_a.layers.0.norm1.weight\n",
            "Copied param enc_a.layers.0.norm1.bias\n",
            "Copied param enc_a.layers.0.norm2.weight\n",
            "Copied param enc_a.layers.0.norm2.bias\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.1.linear1.weight\n",
            "Copied param enc_a.layers.1.linear1.bias\n",
            "Copied param enc_a.layers.1.linear2.weight\n",
            "Copied param enc_a.layers.1.linear2.bias\n",
            "Copied param enc_a.layers.1.norm1.weight\n",
            "Copied param enc_a.layers.1.norm1.bias\n",
            "Copied param enc_a.layers.1.norm2.weight\n",
            "Copied param enc_a.layers.1.norm2.bias\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.2.linear1.weight\n",
            "Copied param enc_a.layers.2.linear1.bias\n",
            "Copied param enc_a.layers.2.linear2.weight\n",
            "Copied param enc_a.layers.2.linear2.bias\n",
            "Copied param enc_a.layers.2.norm1.weight\n",
            "Copied param enc_a.layers.2.norm1.bias\n",
            "Copied param enc_a.layers.2.norm2.weight\n",
            "Copied param enc_a.layers.2.norm2.bias\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.0.linear1.weight\n",
            "Copied param enc_v.layers.0.linear1.bias\n",
            "Copied param enc_v.layers.0.linear2.weight\n",
            "Copied param enc_v.layers.0.linear2.bias\n",
            "Copied param enc_v.layers.0.norm1.weight\n",
            "Copied param enc_v.layers.0.norm1.bias\n",
            "Copied param enc_v.layers.0.norm2.weight\n",
            "Copied param enc_v.layers.0.norm2.bias\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.1.linear1.weight\n",
            "Copied param enc_v.layers.1.linear1.bias\n",
            "Copied param enc_v.layers.1.linear2.weight\n",
            "Copied param enc_v.layers.1.linear2.bias\n",
            "Copied param enc_v.layers.1.norm1.weight\n",
            "Copied param enc_v.layers.1.norm1.bias\n",
            "Copied param enc_v.layers.1.norm2.weight\n",
            "Copied param enc_v.layers.1.norm2.bias\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.2.linear1.weight\n",
            "Copied param enc_v.layers.2.linear1.bias\n",
            "Copied param enc_v.layers.2.linear2.weight\n",
            "Copied param enc_v.layers.2.linear2.bias\n",
            "Copied param enc_v.layers.2.norm1.weight\n",
            "Copied param enc_v.layers.2.norm1.bias\n",
            "Copied param enc_v.layers.2.norm2.weight\n",
            "Copied param enc_v.layers.2.norm2.bias\n",
            "Copied param fusion.weight\n",
            "Copied param fusion.bias\n",
            "Copied param clf.weight\n",
            "Copied param clf.bias\n",
            "------------------ Adding LNorm ------------------------\n",
            "MMSA - The model has 166279523 trainable parameters\n",
            "ongoing with msalm\n",
            "3.ca_layer.alpha_1\n",
            "3.ca_layer.alpha_2\n",
            "3.ca_layer.ln_1.weight\n",
            "3.ca_layer.ln_1.bias\n",
            "3.ca_layer.ln_2.weight\n",
            "3.ca_layer.ln_2.bias\n",
            "3.ca_layer.attn.W_q.weight\n",
            "3.ca_layer.attn.W_kv.weight\n",
            "3.ca_layer.attn.W_o.weight\n",
            "3.ca_layer.mlp.c_fc.weight\n",
            "3.ca_layer.mlp.c_fc.bias\n",
            "3.ca_layer.mlp.c_proj.weight\n",
            "3.ca_layer.mlp.c_proj.bias\n",
            "5.ca_layer.alpha_1\n",
            "5.ca_layer.alpha_2\n",
            "5.ca_layer.ln_1.weight\n",
            "5.ca_layer.ln_1.bias\n",
            "5.ca_layer.ln_2.weight\n",
            "5.ca_layer.ln_2.bias\n",
            "5.ca_layer.attn.W_q.weight\n",
            "5.ca_layer.attn.W_kv.weight\n",
            "5.ca_layer.attn.W_o.weight\n",
            "5.ca_layer.mlp.c_fc.weight\n",
            "5.ca_layer.mlp.c_fc.bias\n",
            "5.ca_layer.mlp.c_proj.weight\n",
            "5.ca_layer.mlp.c_proj.bias\n",
            "7.ca_layer.alpha_1\n",
            "7.ca_layer.alpha_2\n",
            "7.ca_layer.ln_1.weight\n",
            "7.ca_layer.ln_1.bias\n",
            "7.ca_layer.ln_2.weight\n",
            "7.ca_layer.ln_2.bias\n",
            "7.ca_layer.attn.W_q.weight\n",
            "7.ca_layer.attn.W_kv.weight\n",
            "7.ca_layer.attn.W_o.weight\n",
            "7.ca_layer.mlp.c_fc.weight\n",
            "7.ca_layer.mlp.c_fc.bias\n",
            "7.ca_layer.mlp.c_proj.weight\n",
            "7.ca_layer.mlp.c_proj.bias\n",
            "8.ca_layer.alpha_1\n",
            "8.ca_layer.alpha_2\n",
            "8.ca_layer.ln_1.weight\n",
            "8.ca_layer.ln_1.bias\n",
            "8.ca_layer.ln_2.weight\n",
            "8.ca_layer.ln_2.bias\n",
            "8.ca_layer.attn.W_q.weight\n",
            "8.ca_layer.attn.W_kv.weight\n",
            "8.ca_layer.attn.W_o.weight\n",
            "8.ca_layer.mlp.c_fc.weight\n",
            "8.ca_layer.mlp.c_fc.bias\n",
            "8.ca_layer.mlp.c_proj.weight\n",
            "8.ca_layer.mlp.c_proj.bias\n",
            "9.ca_layer.alpha_1\n",
            "9.ca_layer.alpha_2\n",
            "9.ca_layer.ln_1.weight\n",
            "9.ca_layer.ln_1.bias\n",
            "9.ca_layer.ln_2.weight\n",
            "9.ca_layer.ln_2.bias\n",
            "9.ca_layer.attn.W_q.weight\n",
            "9.ca_layer.attn.W_kv.weight\n",
            "9.ca_layer.attn.W_o.weight\n",
            "9.ca_layer.mlp.c_fc.weight\n",
            "9.ca_layer.mlp.c_fc.bias\n",
            "9.ca_layer.mlp.c_proj.weight\n",
            "9.ca_layer.mlp.c_proj.bias\n",
            "10.ca_layer.alpha_1\n",
            "10.ca_layer.alpha_2\n",
            "10.ca_layer.ln_1.weight\n",
            "10.ca_layer.ln_1.bias\n",
            "10.ca_layer.ln_2.weight\n",
            "10.ca_layer.ln_2.bias\n",
            "10.ca_layer.attn.W_q.weight\n",
            "10.ca_layer.attn.W_kv.weight\n",
            "10.ca_layer.attn.W_o.weight\n",
            "10.ca_layer.mlp.c_fc.weight\n",
            "10.ca_layer.mlp.c_fc.bias\n",
            "10.ca_layer.mlp.c_proj.weight\n",
            "10.ca_layer.mlp.c_proj.bias\n",
            "11.ca_layer.alpha_1\n",
            "11.ca_layer.alpha_2\n",
            "11.ca_layer.ln_1.weight\n",
            "11.ca_layer.ln_1.bias\n",
            "11.ca_layer.ln_2.weight\n",
            "11.ca_layer.ln_2.bias\n",
            "11.ca_layer.attn.W_q.weight\n",
            "11.ca_layer.attn.W_kv.weight\n",
            "11.ca_layer.attn.W_o.weight\n",
            "11.ca_layer.mlp.c_fc.weight\n",
            "11.ca_layer.mlp.c_fc.bias\n",
            "11.ca_layer.mlp.c_proj.weight\n",
            "11.ca_layer.mlp.c_proj.bias\n",
            "0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.W_task_0.0.weight\n",
            "Model.W_task_0.0.bias\n",
            "Model.W_task_0.1.weight\n",
            "Model.W_task_0.1.bias\n",
            "Model.W_task_1.weight\n",
            "Model.W_task_1.bias\n",
            "Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Model.av_encoder.proj_a.weight\n",
            "Model.av_encoder.proj_v.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm2.bias\n",
            "Model.av_encoder.fusion.weight\n",
            "Model.av_encoder.fusion.bias\n",
            "Model.av_encoder.clf.weight\n",
            "Model.av_encoder.clf.bias\n",
            "Model.LN.weight\n",
            "Model.LN.bias\n",
            "The total number of trainable parameters is 41.84 M\n",
            "Model.lang_encoder.transformer.wte.0.embedding.weight\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wpe.0.positional.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.ln_f.weight\n",
            "Model.lang_encoder.transformer.ln_f.bias\n",
            "Model.W_task_0.0.weight\n",
            "Using grad with decay in Model.W_task_0.0.weight\n",
            "Model.W_task_0.0.bias\n",
            "Using grad with no decay in Model.W_task_0.0.bias\n",
            "Model.W_task_0.1.weight\n",
            "Using grad with decay in Model.W_task_0.1.weight\n",
            "Model.W_task_0.1.bias\n",
            "Using grad with no decay in Model.W_task_0.1.bias\n",
            "Model.W_task_1.weight\n",
            "Using grad with decay in Model.W_task_1.weight\n",
            "Model.W_task_1.bias\n",
            "Using grad with no decay in Model.W_task_1.bias\n",
            "Model.W_bn.weight\n",
            "Using grad with decay in Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Using grad with no decay in Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Using grad with decay in Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Using grad with no decay in Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Using grad with decay in Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Using grad with no decay in Model.W_av.bias\n",
            "Model.LN.weight\n",
            "Using grad with decay in Model.LN.weight\n",
            "Model.LN.bias\n",
            "Using grad with no decay in Model.LN.bias\n",
            "Will be using warmup for 5 steps\n",
            "  0% 0/41 [00:00<?, ?it/s]NOT USING BF16---------------------\n",
            "task_logits\n",
            "tensor([[-1.6240e-01],\n",
            "        [-7.1739e-02],\n",
            "        [-2.0510e-01],\n",
            "        [-1.2419e-01],\n",
            "        [-6.2668e-02],\n",
            "        [ 1.9530e-01],\n",
            "        [ 2.8669e-01],\n",
            "        [ 3.5471e-01],\n",
            "        [ 1.1763e+00],\n",
            "        [-1.8461e-02],\n",
            "        [-2.4414e-01],\n",
            "        [ 5.1735e-01],\n",
            "        [-3.1152e-01],\n",
            "        [-3.5333e-01],\n",
            "        [-6.4289e-03],\n",
            "        [ 3.6191e-02],\n",
            "        [ 8.5707e-02],\n",
            "        [-1.3214e-02],\n",
            "        [-3.3839e-01],\n",
            "        [-4.8746e-01],\n",
            "        [-4.2712e-02],\n",
            "        [-1.8178e-01],\n",
            "        [ 3.8106e-01],\n",
            "        [-2.3506e-01],\n",
            "        [-8.9168e-02],\n",
            "        [ 3.9660e-02],\n",
            "        [ 7.6799e-04],\n",
            "        [-3.8609e-01],\n",
            "        [-5.9150e-01],\n",
            "        [-2.3962e-01],\n",
            "        [-2.4419e-01],\n",
            "        [ 6.0516e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "--------------------------------------------\n",
            "text_logits\n",
            "tensor([[-2.1704],\n",
            "        [-2.5898],\n",
            "        [-2.0447],\n",
            "        [-2.3261],\n",
            "        [-3.6988],\n",
            "        [-1.7524],\n",
            "        [-0.2281],\n",
            "        [-2.5515],\n",
            "        [-2.9492],\n",
            "        [-1.7784],\n",
            "        [-2.0773],\n",
            "        [-1.5616],\n",
            "        [-1.3822],\n",
            "        [-0.3249],\n",
            "        [-2.4162],\n",
            "        [-1.6285],\n",
            "        [ 1.5517],\n",
            "        [-2.4619],\n",
            "        [-2.6068],\n",
            "        [-1.4924],\n",
            "        [-1.7506],\n",
            "        [-2.8699],\n",
            "        [-0.7564],\n",
            "        [-2.1865],\n",
            "        [-2.1122],\n",
            "        [-1.9171],\n",
            "        [-2.1497],\n",
            "        [-2.0719],\n",
            "        [-1.6432],\n",
            "        [-1.9461],\n",
            "        [-1.0647],\n",
            "        [-3.4518]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "--------------------------------------------\n",
            "av_logits\n",
            "tensor([-0.4050,  0.4067, -0.4816,  0.3383,  0.1241, -0.2281, -0.2843, -0.2831,\n",
            "         0.0312, -0.1069, -0.4158, -0.2941, -0.2796,  0.0223, -0.4337, -0.2341,\n",
            "        -0.4008,  0.1553, -0.1157, -0.2026, -0.2719, -0.0407, -0.2724, -0.3899,\n",
            "        -0.3811,  0.5712, -0.2939, -0.3681,  0.2805, -0.4118, -0.3553,  0.2822],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward3>)\n",
            "--------------------------------------------\n",
            "bn_logits\n",
            "tensor([[[-0.5132],\n",
            "         [ 1.4378],\n",
            "         [-1.6295],\n",
            "         [ 0.2070],\n",
            "         [-0.8346],\n",
            "         [-3.1784],\n",
            "         [ 1.7379],\n",
            "         [ 1.3243],\n",
            "         [ 0.3414],\n",
            "         [ 2.6512],\n",
            "         [ 0.4657],\n",
            "         [ 1.9183]],\n",
            "\n",
            "        [[-0.4157],\n",
            "         [-0.0420],\n",
            "         [ 1.2773],\n",
            "         [-1.1745],\n",
            "         [ 1.6760],\n",
            "         [-1.2343],\n",
            "         [-1.4625],\n",
            "         [-1.4716],\n",
            "         [-0.8465],\n",
            "         [-1.9756],\n",
            "         [ 0.3350],\n",
            "         [ 0.1608]],\n",
            "\n",
            "        [[-1.1593],\n",
            "         [-2.3305],\n",
            "         [-1.1426],\n",
            "         [ 2.8140],\n",
            "         [-0.8955],\n",
            "         [ 0.4310],\n",
            "         [-0.3200],\n",
            "         [-2.7355],\n",
            "         [-1.7441],\n",
            "         [-0.6067],\n",
            "         [-1.7565],\n",
            "         [ 0.9474]],\n",
            "\n",
            "        [[ 0.8700],\n",
            "         [-0.7355],\n",
            "         [ 2.5118],\n",
            "         [-0.4605],\n",
            "         [ 1.1572],\n",
            "         [-0.6021],\n",
            "         [-0.2035],\n",
            "         [ 3.6835],\n",
            "         [-0.1507],\n",
            "         [-3.3317],\n",
            "         [ 1.0906],\n",
            "         [-0.3634]],\n",
            "\n",
            "        [[-0.8956],\n",
            "         [-1.0356],\n",
            "         [-4.3916],\n",
            "         [-2.9980],\n",
            "         [-3.9631],\n",
            "         [-2.5564],\n",
            "         [-1.7086],\n",
            "         [-0.6013],\n",
            "         [ 0.4916],\n",
            "         [ 0.4556],\n",
            "         [-2.0567],\n",
            "         [-1.0989]],\n",
            "\n",
            "        [[ 1.0299],\n",
            "         [ 0.3913],\n",
            "         [-3.8803],\n",
            "         [ 0.4520],\n",
            "         [ 0.0254],\n",
            "         [-0.3165],\n",
            "         [-0.0068],\n",
            "         [-0.3810],\n",
            "         [-0.0186],\n",
            "         [-0.5486],\n",
            "         [-0.9927],\n",
            "         [-1.2050]],\n",
            "\n",
            "        [[ 1.2634],\n",
            "         [-0.7929],\n",
            "         [-1.1141],\n",
            "         [ 0.2254],\n",
            "         [-0.0409],\n",
            "         [ 0.8675],\n",
            "         [-0.9989],\n",
            "         [ 0.5771],\n",
            "         [-0.8941],\n",
            "         [-1.0743],\n",
            "         [-2.5494],\n",
            "         [ 1.4640]],\n",
            "\n",
            "        [[-0.0281],\n",
            "         [-0.5646],\n",
            "         [ 1.0822],\n",
            "         [ 0.4096],\n",
            "         [-2.1759],\n",
            "         [-0.6629],\n",
            "         [-1.6160],\n",
            "         [-1.7422],\n",
            "         [-1.3955],\n",
            "         [-1.7450],\n",
            "         [-0.6988],\n",
            "         [-1.1349]],\n",
            "\n",
            "        [[-0.2045],\n",
            "         [-1.5778],\n",
            "         [ 0.8836],\n",
            "         [ 0.6449],\n",
            "         [-1.6994],\n",
            "         [-1.1408],\n",
            "         [-0.1553],\n",
            "         [-0.6211],\n",
            "         [ 2.8721],\n",
            "         [-0.9288],\n",
            "         [-0.4684],\n",
            "         [ 0.3617]],\n",
            "\n",
            "        [[ 0.6337],\n",
            "         [-0.2796],\n",
            "         [-1.1226],\n",
            "         [-0.9092],\n",
            "         [-1.4080],\n",
            "         [-1.9411],\n",
            "         [-0.1708],\n",
            "         [-1.2588],\n",
            "         [ 1.3967],\n",
            "         [-0.5863],\n",
            "         [-1.6410],\n",
            "         [-2.5052]],\n",
            "\n",
            "        [[-1.7382],\n",
            "         [ 0.3285],\n",
            "         [-1.3301],\n",
            "         [-1.2246],\n",
            "         [-0.6476],\n",
            "         [-1.3518],\n",
            "         [-2.9730],\n",
            "         [-1.9714],\n",
            "         [ 0.3443],\n",
            "         [-0.6858],\n",
            "         [-1.1301],\n",
            "         [-1.0190]],\n",
            "\n",
            "        [[-1.3420],\n",
            "         [-2.0675],\n",
            "         [ 0.7605],\n",
            "         [-2.2173],\n",
            "         [-2.2343],\n",
            "         [-0.7674],\n",
            "         [-3.1936],\n",
            "         [-0.2860],\n",
            "         [-1.3211],\n",
            "         [-0.1138],\n",
            "         [-0.6935],\n",
            "         [-0.2797]],\n",
            "\n",
            "        [[-2.1428],\n",
            "         [ 2.2306],\n",
            "         [ 0.1696],\n",
            "         [-1.3451],\n",
            "         [-0.8012],\n",
            "         [ 0.2888],\n",
            "         [ 1.5364],\n",
            "         [-1.2326],\n",
            "         [-1.5117],\n",
            "         [-0.7807],\n",
            "         [-0.7834],\n",
            "         [-1.4847]],\n",
            "\n",
            "        [[-1.2366],\n",
            "         [-2.9716],\n",
            "         [-0.4474],\n",
            "         [-0.8789],\n",
            "         [-2.6637],\n",
            "         [ 0.5722],\n",
            "         [ 0.2856],\n",
            "         [-1.6173],\n",
            "         [-1.7537],\n",
            "         [-2.5267],\n",
            "         [-3.4929],\n",
            "         [-3.0319]],\n",
            "\n",
            "        [[-1.0408],\n",
            "         [-0.8644],\n",
            "         [-1.4077],\n",
            "         [-0.2097],\n",
            "         [-4.1827],\n",
            "         [-2.1827],\n",
            "         [-0.5765],\n",
            "         [ 0.8821],\n",
            "         [-0.1991],\n",
            "         [-1.1354],\n",
            "         [-1.5121],\n",
            "         [-1.1934]],\n",
            "\n",
            "        [[ 1.7162],\n",
            "         [-0.0303],\n",
            "         [ 1.4955],\n",
            "         [-0.2569],\n",
            "         [-0.9358],\n",
            "         [-0.5231],\n",
            "         [-2.4005],\n",
            "         [ 0.0218],\n",
            "         [-0.6414],\n",
            "         [-0.5282],\n",
            "         [-1.5395],\n",
            "         [-1.2469]],\n",
            "\n",
            "        [[ 0.6398],\n",
            "         [-1.1201],\n",
            "         [ 2.4438],\n",
            "         [ 1.4065],\n",
            "         [-0.2506],\n",
            "         [-2.5494],\n",
            "         [-0.9301],\n",
            "         [-1.9207],\n",
            "         [-1.1726],\n",
            "         [-1.6030],\n",
            "         [ 0.7115],\n",
            "         [-2.4154]],\n",
            "\n",
            "        [[-0.2339],\n",
            "         [-0.9399],\n",
            "         [-1.8204],\n",
            "         [-0.3576],\n",
            "         [-1.3201],\n",
            "         [-0.2290],\n",
            "         [-0.9315],\n",
            "         [-1.0195],\n",
            "         [-1.1361],\n",
            "         [ 1.3217],\n",
            "         [-1.4876],\n",
            "         [-0.0912]],\n",
            "\n",
            "        [[-0.3920],\n",
            "         [-1.2436],\n",
            "         [ 0.2197],\n",
            "         [ 1.7892],\n",
            "         [ 0.2003],\n",
            "         [-2.3129],\n",
            "         [ 0.2934],\n",
            "         [-1.5148],\n",
            "         [-0.7218],\n",
            "         [-2.1622],\n",
            "         [-0.6910],\n",
            "         [-0.4110]],\n",
            "\n",
            "        [[-1.1702],\n",
            "         [-0.9283],\n",
            "         [-0.9691],\n",
            "         [-1.1270],\n",
            "         [-1.1080],\n",
            "         [-1.0605],\n",
            "         [ 4.1161],\n",
            "         [-4.0230],\n",
            "         [-1.3906],\n",
            "         [-2.6737],\n",
            "         [-0.7763],\n",
            "         [ 2.6111]],\n",
            "\n",
            "        [[-0.5225],\n",
            "         [-0.3024],\n",
            "         [-2.9046],\n",
            "         [-5.3903],\n",
            "         [ 0.0414],\n",
            "         [-1.0543],\n",
            "         [ 0.2997],\n",
            "         [ 2.4293],\n",
            "         [-0.7142],\n",
            "         [ 0.1684],\n",
            "         [-0.6795],\n",
            "         [-4.0892]],\n",
            "\n",
            "        [[ 0.1087],\n",
            "         [-2.4542],\n",
            "         [-1.3784],\n",
            "         [-0.7129],\n",
            "         [ 0.2724],\n",
            "         [-2.2751],\n",
            "         [-2.1347],\n",
            "         [-2.8630],\n",
            "         [-1.1097],\n",
            "         [-2.7834],\n",
            "         [-0.5921],\n",
            "         [-0.9172]],\n",
            "\n",
            "        [[-1.9541],\n",
            "         [ 0.2153],\n",
            "         [-0.3258],\n",
            "         [-0.9992],\n",
            "         [-0.9340],\n",
            "         [-2.9889],\n",
            "         [ 0.8891],\n",
            "         [-0.9925],\n",
            "         [-2.0078],\n",
            "         [-1.6324],\n",
            "         [ 0.0490],\n",
            "         [ 2.0360]],\n",
            "\n",
            "        [[-0.4151],\n",
            "         [ 1.0502],\n",
            "         [-1.1029],\n",
            "         [-0.0284],\n",
            "         [ 0.0549],\n",
            "         [-1.0163],\n",
            "         [-6.1582],\n",
            "         [-1.7405],\n",
            "         [-4.1843],\n",
            "         [-0.4958],\n",
            "         [-0.4587],\n",
            "         [-2.8218]],\n",
            "\n",
            "        [[-0.0093],\n",
            "         [-3.3663],\n",
            "         [ 1.2980],\n",
            "         [-0.8810],\n",
            "         [-2.3995],\n",
            "         [-2.8446],\n",
            "         [-1.0895],\n",
            "         [-0.8154],\n",
            "         [-1.0197],\n",
            "         [ 0.2015],\n",
            "         [-1.4850],\n",
            "         [-0.9897]],\n",
            "\n",
            "        [[-0.5512],\n",
            "         [-1.4749],\n",
            "         [-1.4040],\n",
            "         [-0.8105],\n",
            "         [-1.2376],\n",
            "         [ 0.7890],\n",
            "         [-0.7319],\n",
            "         [-2.8305],\n",
            "         [-0.6109],\n",
            "         [-1.9601],\n",
            "         [-2.8683],\n",
            "         [-1.2858]],\n",
            "\n",
            "        [[ 4.7881],\n",
            "         [-1.0688],\n",
            "         [-0.3282],\n",
            "         [-1.3605],\n",
            "         [-0.4838],\n",
            "         [-1.2868],\n",
            "         [ 0.3669],\n",
            "         [ 0.1286],\n",
            "         [-0.6131],\n",
            "         [-0.0843],\n",
            "         [ 0.2760],\n",
            "         [-4.4596]],\n",
            "\n",
            "        [[-1.0105],\n",
            "         [-2.4544],\n",
            "         [-1.3388],\n",
            "         [-0.6765],\n",
            "         [-1.1442],\n",
            "         [-0.0664],\n",
            "         [-2.5033],\n",
            "         [ 0.4092],\n",
            "         [-1.0759],\n",
            "         [-1.6171],\n",
            "         [-2.2050],\n",
            "         [-0.8122]],\n",
            "\n",
            "        [[-1.3743],\n",
            "         [-1.8345],\n",
            "         [-1.8286],\n",
            "         [-2.4548],\n",
            "         [-2.2182],\n",
            "         [-1.1018],\n",
            "         [-0.9439],\n",
            "         [-1.8145],\n",
            "         [-2.8718],\n",
            "         [-2.8696],\n",
            "         [-2.5332],\n",
            "         [-1.3199]],\n",
            "\n",
            "        [[-2.1279],\n",
            "         [-1.1402],\n",
            "         [ 1.3944],\n",
            "         [-1.9734],\n",
            "         [-0.4278],\n",
            "         [-1.0893],\n",
            "         [-1.4502],\n",
            "         [-0.7796],\n",
            "         [ 0.4740],\n",
            "         [-1.6329],\n",
            "         [-0.7166],\n",
            "         [-1.2426]],\n",
            "\n",
            "        [[-1.0498],\n",
            "         [-1.6843],\n",
            "         [-0.6173],\n",
            "         [-1.7136],\n",
            "         [-1.3364],\n",
            "         [-1.2525],\n",
            "         [-0.8996],\n",
            "         [-2.4190],\n",
            "         [-0.5758],\n",
            "         [-0.3716],\n",
            "         [-2.0087],\n",
            "         [-1.7355]],\n",
            "\n",
            "        [[-0.3762],\n",
            "         [-1.9099],\n",
            "         [-3.3940],\n",
            "         [-1.1665],\n",
            "         [ 0.4626],\n",
            "         [ 3.0286],\n",
            "         [ 0.8765],\n",
            "         [-2.0787],\n",
            "         [-0.1337],\n",
            "         [ 1.3283],\n",
            "         [-0.7211],\n",
            "         [-2.5386]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "--------------------------------------------\n",
            "lm_logits\n",
            "None\n",
            "--------------------------------------------\n",
            "Feature_f\n",
            "tensor([[0.0000, 0.4576, 0.0000,  ..., 1.5790, 0.4985, 0.0000],\n",
            "        [0.2342, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6072],\n",
            "        [0.7655, 0.0000, 1.8728,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.4814, 0.0000, 0.1310,  ..., 0.0000, 0.0000, 0.7215],\n",
            "        [0.2948, 0.0838, 0.1758,  ..., 0.0000, 0.0000, 0.4191],\n",
            "        [0.0000, 0.8339, 0.0000,  ..., 2.1106, 1.7599, 0.0000]],\n",
            "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
            "--------------------------------------------\n",
            "Feature_t\n",
            "tensor([[ 1.2567, -1.4742,  0.9243,  ...,  0.5920,  1.2976, -0.2844],\n",
            "        [-0.3499, -0.5456, -0.3496,  ..., -0.0378, -0.1865, -0.3275],\n",
            "        [-0.5143, -0.1276, -0.1453,  ...,  0.5150,  0.4104, -0.1268],\n",
            "        ...,\n",
            "        [-0.3875, -0.6576, -0.1287,  ..., -0.0291,  0.2812, -0.5492],\n",
            "        [ 0.1599, -0.7275, -1.5405,  ...,  0.9996, -0.5269, -0.0706],\n",
            "        [ 0.4439, -1.5378,  0.1223,  ...,  0.4465,  0.5049, -0.5586]],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "--------------------------------------------\n",
            "Feature_av\n",
            "tensor([[-8.9990e-01, -1.9063e+00,  7.7913e-01,  7.7331e-01, -1.8035e-01,\n",
            "          5.1373e-02,  7.6151e-01, -1.2767e+00,  5.9646e-01,  6.2797e-01,\n",
            "          2.8492e-01,  2.7055e-01, -1.4654e-01, -1.3889e+00,  2.6032e+00,\n",
            "          9.9885e-02, -7.3153e-01,  2.8057e-01, -6.2612e-01,  9.3577e-01,\n",
            "         -4.2755e-01,  6.2940e-01, -7.2287e-01,  1.6405e+00, -1.1829e+00,\n",
            "         -3.6545e-01, -2.2821e-01, -1.2198e-01,  3.6241e-01, -4.9162e-01],\n",
            "        [ 1.3459e+00,  1.6022e+00, -1.5761e-01, -3.7729e-01, -2.1435e-01,\n",
            "          1.6629e+00,  5.7337e-03,  7.2764e-01, -4.4619e-01, -1.6099e+00,\n",
            "         -9.4431e-01,  7.9239e-02,  3.6189e-01,  1.7929e-01,  7.3190e-01,\n",
            "          9.5205e-01, -4.0505e-01,  2.6674e-01, -3.7175e-01, -8.0905e-02,\n",
            "         -8.3214e-01,  4.4229e-01,  8.4743e-01, -1.5281e+00, -6.6954e-02,\n",
            "          2.7797e-01, -1.4098e+00, -9.8651e-01, -4.8129e-01,  4.2911e-01],\n",
            "        [ 2.9999e-02, -9.0317e-01,  8.8943e-01,  7.1747e-01, -3.2638e-01,\n",
            "          8.9986e-01,  3.1862e-01, -1.0836e+00,  4.9557e-01, -2.3263e-01,\n",
            "         -6.7690e-01,  3.8211e-01,  6.8602e-01, -1.1178e+00,  3.0988e+00,\n",
            "          6.3478e-01, -8.4263e-01,  8.8474e-01, -6.2607e-01,  6.1219e-01,\n",
            "         -1.0110e+00,  1.0052e-01, -3.5025e-01,  3.4599e-01, -1.0811e+00,\n",
            "         -8.3186e-01, -4.9768e-01, -4.1839e-01,  2.4449e-01, -3.4112e-01],\n",
            "        [ 1.7070e+00,  8.3654e-01,  1.7181e-01, -1.6555e-01,  2.1696e-03,\n",
            "          1.5332e+00,  2.2858e-01,  1.9299e-01,  5.2836e-01, -5.5199e-01,\n",
            "         -7.5997e-01, -3.5641e-01,  3.0963e-01, -6.2676e-01,  1.7394e+00,\n",
            "          5.4158e-01,  3.9158e-02,  4.5164e-02, -8.7670e-01,  2.9156e-02,\n",
            "         -9.4440e-01,  2.4461e-01,  7.6625e-01, -1.0586e+00, -1.1396e+00,\n",
            "         -5.3721e-01, -1.0849e+00, -1.1252e+00,  3.1394e-01, -2.1994e-03],\n",
            "        [ 1.1357e+00,  6.5531e-01,  6.2136e-01,  2.1945e-01, -4.4350e-01,\n",
            "          1.5372e+00,  1.6824e-01, -3.6107e-02, -1.5554e-01, -1.2691e+00,\n",
            "         -9.7888e-01,  2.6056e-01,  4.3778e-01, -3.8241e-01,  2.0346e+00,\n",
            "          9.2470e-01, -6.2446e-01,  3.1647e-01, -5.5726e-01,  4.6092e-01,\n",
            "         -1.1231e+00,  2.8536e-01,  7.9073e-02, -7.4611e-01, -5.2637e-01,\n",
            "         -2.5340e-01, -1.1430e+00, -1.0297e+00, -2.7197e-01,  4.0418e-01],\n",
            "        [ 8.4693e-01,  2.0810e-01,  8.4307e-01,  1.1456e-01, -1.2413e-01,\n",
            "          1.3686e+00, -2.3751e-01, -7.5046e-01,  1.1707e-01, -1.2471e+00,\n",
            "         -1.4545e+00,  2.7184e-01,  1.1605e+00, -3.3049e-01,  2.6004e+00,\n",
            "          7.3273e-01, -6.2245e-01,  1.0306e+00, -3.8911e-01, -7.6385e-02,\n",
            "         -1.0747e+00, -2.8218e-01, -9.2605e-02, -8.4760e-01, -4.1780e-02,\n",
            "         -1.4466e+00, -6.1381e-01, -2.3378e-02,  8.8510e-02,  2.7189e-01],\n",
            "        [ 7.7183e-01, -2.7948e-02,  8.3846e-01, -1.6072e-01,  3.6460e-01,\n",
            "          1.3269e+00, -2.7730e-01, -9.2325e-01,  1.4823e-01, -1.1206e+00,\n",
            "         -1.5026e+00,  1.7256e-01,  1.5865e+00, -2.5239e-01,  2.4849e+00,\n",
            "          6.1417e-01, -3.1797e-01,  1.2908e+00, -3.7258e-01, -4.1002e-01,\n",
            "         -1.1360e+00, -6.6179e-01,  1.8741e-02, -9.1789e-01,  2.4330e-02,\n",
            "         -1.8526e+00, -2.6297e-01,  3.0783e-01,  3.0567e-01, -5.8888e-02],\n",
            "        [ 3.1334e-01, -4.7338e-01,  6.9710e-01,  4.1966e-01, -4.7808e-01,\n",
            "          1.3204e+00,  5.3309e-01, -5.1434e-01,  5.3943e-01, -6.5397e-01,\n",
            "         -4.3308e-01,  3.7777e-01,  2.8559e-01, -9.9662e-01,  2.7773e+00,\n",
            "          8.0419e-01, -9.9598e-01,  3.6815e-01, -6.8313e-01,  6.8623e-01,\n",
            "         -9.8875e-01,  4.7740e-01, -1.8465e-01,  1.3721e-02, -1.1263e+00,\n",
            "         -2.5900e-01, -8.4739e-01, -8.4018e-01, -5.7525e-02, -8.1045e-02],\n",
            "        [ 1.2680e+00,  1.0560e+00,  1.1592e-01, -2.8454e-01, -2.2064e-01,\n",
            "          1.7790e+00, -3.4795e-02,  6.7849e-02,  1.4231e-01, -1.4036e+00,\n",
            "         -1.3429e+00, -2.5407e-02,  1.1214e+00, -2.2392e-01,  1.7411e+00,\n",
            "          6.7835e-01, -5.2674e-01,  6.4933e-01, -2.5796e-01, -1.5384e-01,\n",
            "         -1.1113e+00, -1.6538e-02,  3.2979e-01, -1.4158e+00, -2.4166e-01,\n",
            "         -8.0650e-01, -9.1429e-01, -5.1059e-01,  1.2746e-02,  5.2931e-01],\n",
            "        [ 1.0359e+00,  5.5625e-01,  6.7051e-01,  1.5420e-01,  8.3140e-02,\n",
            "          1.3304e+00, -1.1662e-01, -3.3378e-01,  4.0282e-01, -1.0296e+00,\n",
            "         -1.1523e+00,  1.5539e-01,  9.9813e-01, -5.6956e-01,  2.4382e+00,\n",
            "          5.8869e-01, -2.9920e-01,  9.4989e-01, -4.9723e-01, -1.7481e-01,\n",
            "         -1.1883e+00, -1.6546e-01,  1.3766e-01, -9.8366e-01, -5.3683e-01,\n",
            "         -1.3063e+00, -6.1209e-01, -5.2071e-01,  1.0576e-01, -1.2049e-01],\n",
            "        [-4.7331e-01, -1.3533e+00,  1.3875e+00,  9.2239e-01, -4.3593e-01,\n",
            "          5.2752e-01,  5.8313e-01, -1.0494e+00,  1.7060e-01,  5.5438e-02,\n",
            "         -9.5820e-03,  5.4266e-01, -1.5351e-01, -1.0067e+00,  3.0432e+00,\n",
            "          6.8616e-01, -8.1807e-01,  1.8340e-01, -5.6978e-01,  9.5925e-01,\n",
            "         -9.2803e-01,  4.9195e-01, -6.3764e-01,  8.7369e-01, -1.0728e+00,\n",
            "         -4.7610e-01, -5.3750e-01, -6.3365e-01,  1.5930e-01, -4.3091e-01],\n",
            "        [ 6.0924e-01, -1.7927e-01,  1.1574e+00,  2.7240e-01, -2.6423e-01,\n",
            "          1.3159e+00, -2.8998e-03, -7.1666e-01,  1.4513e-01, -1.0923e+00,\n",
            "         -1.1849e+00,  3.9828e-01,  8.9548e-01, -4.0174e-01,  2.6465e+00,\n",
            "          7.1748e-01, -9.0327e-01,  8.5496e-01, -4.7830e-01,  2.9672e-01,\n",
            "         -1.0217e+00, -1.1674e-01,  2.5072e-03, -6.0593e-01, -3.1085e-01,\n",
            "         -1.1734e+00, -7.9052e-01, -2.5507e-01,  3.5185e-02,  1.5063e-01],\n",
            "        [ 8.2104e-01, -1.2634e-01,  9.9298e-01, -1.9980e-01,  9.0542e-02,\n",
            "          1.3987e+00, -1.3509e-01, -8.4144e-01,  2.4878e-01, -1.3611e+00,\n",
            "         -1.4931e+00,  1.1486e-01,  1.4744e+00, -1.5342e-01,  2.4891e+00,\n",
            "          5.8991e-01, -6.9825e-01,  1.0733e+00, -3.2928e-01, -7.1735e-02,\n",
            "         -1.0651e+00, -6.4267e-01, -3.9747e-02, -1.0015e+00,  1.6422e-01,\n",
            "         -1.6833e+00, -3.0694e-01,  2.1493e-01,  1.9341e-01,  2.8260e-01],\n",
            "        [ 1.2453e+00,  4.5790e-01,  4.9453e-01,  9.6305e-03,  6.3418e-02,\n",
            "          1.5863e+00, -1.2690e-01, -3.3700e-01, -1.2033e-01, -1.3117e+00,\n",
            "         -1.3630e+00,  1.1667e-01,  9.3471e-01,  3.6706e-02,  2.0253e+00,\n",
            "          8.7075e-01, -5.8258e-01,  8.7403e-01, -4.9246e-01,  3.2000e-02,\n",
            "         -1.0845e+00, -1.4136e-01,  4.7209e-01, -1.1618e+00, -4.7611e-02,\n",
            "         -9.8275e-01, -9.9540e-01, -4.5918e-01, -1.2793e-01,  1.1509e-01],\n",
            "        [ 5.9303e-02, -1.0874e+00,  1.3069e+00,  6.8302e-01, -4.7709e-01,\n",
            "          8.1820e-01,  2.3354e-01, -1.0079e+00,  2.3780e-01, -3.6027e-01,\n",
            "         -5.4137e-01,  5.4883e-01,  2.2918e-01, -7.9755e-01,  3.1287e+00,\n",
            "          8.0572e-01, -7.8634e-01,  6.0936e-01, -5.9601e-01,  7.6051e-01,\n",
            "         -9.7112e-01, -6.6171e-03, -2.7988e-01,  3.8196e-01, -8.0521e-01,\n",
            "         -7.3774e-01, -5.7201e-01, -4.6955e-01,  1.0442e-01, -4.1140e-01],\n",
            "        [ 7.4268e-01, -8.2699e-02,  7.3087e-01,  1.9464e-01, -1.4233e-01,\n",
            "          1.2841e+00,  2.0577e-01, -5.8799e-01,  3.2382e-01, -9.0412e-01,\n",
            "         -9.5410e-01,  2.6455e-01,  8.5157e-01, -7.6074e-01,  2.8347e+00,\n",
            "          6.4131e-01, -7.2156e-01,  7.2443e-01, -5.0706e-01,  3.2680e-01,\n",
            "         -1.1195e+00, -1.5881e-02, -1.5309e-01, -5.6592e-01, -5.1855e-01,\n",
            "         -9.8831e-01, -7.4909e-01, -4.8735e-01,  6.0582e-02,  7.2514e-02],\n",
            "        [ 2.0227e-01, -6.8994e-01,  1.3289e+00,  4.0407e-01, -3.3826e-01,\n",
            "          9.4371e-01,  1.0434e-01, -1.1996e+00,  2.8896e-01, -5.1789e-01,\n",
            "         -1.0113e+00,  4.1056e-01,  9.2500e-01, -8.2800e-01,  3.2164e+00,\n",
            "          9.1701e-01, -6.8025e-01,  7.0866e-01, -7.2381e-01,  3.7048e-01,\n",
            "         -9.9056e-01, -4.8872e-01, -6.1402e-01, -4.4738e-02, -4.4414e-01,\n",
            "         -1.2989e+00, -2.3649e-01, -9.3723e-02,  2.6373e-01,  1.1625e-01],\n",
            "        [ 1.5605e+00,  1.0633e+00,  8.7123e-02, -9.1606e-01,  3.4806e-01,\n",
            "          1.7366e+00, -3.6471e-01, -1.6417e-01,  1.9465e-03, -1.6806e+00,\n",
            "         -1.6518e+00, -1.9307e-01,  1.5320e+00,  3.7401e-01,  1.1830e+00,\n",
            "          4.6867e-01, -1.0471e-01,  1.0015e+00, -1.6624e-01, -8.4957e-01,\n",
            "         -1.1246e+00, -5.7681e-01,  4.4859e-01, -1.5618e+00,  5.3168e-01,\n",
            "         -1.3861e+00, -4.1988e-01,  1.4818e-01,  9.1587e-02,  5.8340e-01],\n",
            "        [ 4.3485e-01,  1.2837e-01,  3.7221e-01,  4.2489e-01, -7.8083e-01,\n",
            "          1.3357e+00,  5.4489e-01,  2.8214e-02,  4.1844e-01, -7.8590e-01,\n",
            "         -3.4365e-01,  3.3631e-01, -5.7210e-02, -1.0287e+00,  2.2226e+00,\n",
            "          8.8885e-01, -8.9139e-01,  2.5168e-01, -5.2429e-01,  5.5699e-01,\n",
            "         -9.1109e-01,  7.9275e-01, -6.6936e-02, -1.6120e-01, -1.1105e+00,\n",
            "          3.3084e-01, -1.0765e+00, -1.1748e+00, -2.9522e-01,  1.4062e-01],\n",
            "        [ 4.7789e-01, -3.9994e-01,  1.1200e+00,  5.6017e-01, -2.9791e-01,\n",
            "          1.1257e+00,  2.7922e-01, -8.5160e-01,  1.4168e-01, -8.0544e-01,\n",
            "         -7.4933e-01,  3.4037e-01,  4.1569e-01, -6.8432e-01,  2.7850e+00,\n",
            "          9.2399e-01, -1.0141e+00,  5.5292e-01, -6.1291e-01,  8.1151e-01,\n",
            "         -9.5933e-01,  1.4387e-01, -2.8235e-01, -1.5101e-01, -5.7641e-01,\n",
            "         -6.8271e-01, -8.5511e-01, -8.1979e-01, -7.1236e-02,  1.3553e-01],\n",
            "        [-9.6012e-01, -1.6935e+00,  6.6525e-01,  8.6001e-01, -5.9228e-01,\n",
            "          5.3646e-02,  8.9490e-01, -7.9000e-01,  5.1025e-01,  4.4910e-01,\n",
            "          5.4669e-01,  4.3638e-01, -8.7125e-01, -1.2972e+00,  2.2311e+00,\n",
            "          6.6508e-02, -8.5271e-01, -9.8325e-02, -5.2323e-01,  1.2762e+00,\n",
            "         -3.1108e-01,  1.0588e+00, -6.4111e-01,  1.7733e+00, -1.3458e+00,\n",
            "          3.3336e-01, -4.5435e-01, -5.0958e-01,  7.7973e-02, -2.9298e-01],\n",
            "        [ 1.2269e+00,  3.5010e-01,  7.6079e-01,  2.7813e-01, -2.8569e-01,\n",
            "          1.4608e+00, -1.1711e-02, -1.8642e-01,  5.3675e-01, -7.8901e-01,\n",
            "         -9.7860e-01,  6.0055e-02,  6.3616e-01, -6.5801e-01,  2.3162e+00,\n",
            "          7.6295e-01, -3.7522e-01,  5.6203e-01, -6.8401e-01,  2.8755e-01,\n",
            "         -1.1571e+00, -1.1431e-02,  2.8707e-01, -8.5356e-01, -9.0222e-01,\n",
            "         -9.2822e-01, -9.8395e-01, -9.9094e-01,  2.5559e-01,  1.4875e-02],\n",
            "        [ 7.0611e-01,  1.7052e-01,  8.0926e-01,  2.9498e-01, -1.6949e-02,\n",
            "          1.3921e+00,  1.7766e-02, -6.4415e-01,  3.0443e-01, -8.9251e-01,\n",
            "         -1.1030e+00,  2.8198e-01,  1.1437e+00, -6.7262e-01,  2.6227e+00,\n",
            "          6.9410e-01, -6.2993e-01,  9.7652e-01, -4.7121e-01,  6.0981e-02,\n",
            "         -1.1456e+00, -7.0060e-02, -1.0273e-01, -7.2317e-01, -6.1992e-01,\n",
            "         -1.1627e+00, -7.6468e-01, -4.7877e-01,  1.4509e-01, -1.2233e-01],\n",
            "        [ 7.0687e-03, -7.2147e-01,  9.4597e-01,  6.3262e-01, -7.2215e-01,\n",
            "          8.2740e-01,  3.5257e-01, -8.0322e-01,  4.4902e-01, -5.9133e-01,\n",
            "         -6.7488e-01,  4.9389e-01,  1.5918e-01, -1.0336e+00,  2.9581e+00,\n",
            "          8.7060e-01, -9.3000e-01,  6.3867e-01, -5.2168e-01,  7.3983e-01,\n",
            "         -1.0115e+00,  3.5203e-01, -3.0305e-01,  3.8642e-01, -8.7128e-01,\n",
            "         -4.8383e-01, -5.9796e-01, -4.2692e-01,  7.9651e-02, -2.0017e-01],\n",
            "        [ 5.6317e-01, -5.7496e-01,  1.0847e+00,  2.6103e-01,  1.0978e-01,\n",
            "          1.2174e+00, -4.2698e-02, -1.0986e+00,  1.0504e-01, -9.2692e-01,\n",
            "         -1.1001e+00,  2.9085e-01,  1.0656e+00, -4.4649e-01,  2.9937e+00,\n",
            "          7.5296e-01, -6.3290e-01,  1.0233e+00, -4.8802e-01,  1.3778e-01,\n",
            "         -1.1680e+00, -1.7768e-01, -6.0338e-02, -4.4289e-01, -4.1203e-01,\n",
            "         -1.4336e+00, -5.2186e-01, -3.3651e-02,  2.2166e-01, -2.6636e-01],\n",
            "        [ 1.4127e+00,  2.1231e+00, -6.9060e-01, -6.0788e-01, -3.0333e-01,\n",
            "          1.5045e+00,  7.6111e-03,  1.3243e+00, -4.6535e-01, -1.4015e+00,\n",
            "         -6.7937e-01, -1.1968e-02,  2.4902e-01,  3.5383e-01, -2.6648e-01,\n",
            "          9.0993e-01, -2.0587e-01, -9.6928e-02, -2.0856e-01, -3.5572e-01,\n",
            "         -7.1344e-01,  4.5835e-01,  9.4685e-01, -1.6653e+00, -5.9043e-04,\n",
            "          8.0628e-01, -1.2836e+00, -1.1542e+00, -6.3850e-01,  6.5262e-01],\n",
            "        [-8.8723e-01, -1.5928e+00,  6.1646e-01,  9.3461e-01, -4.0391e-01,\n",
            "          8.4434e-02,  9.6097e-01, -8.8889e-01,  5.7357e-01,  5.6834e-01,\n",
            "          5.9484e-01,  2.6077e-01, -6.8934e-01, -1.3381e+00,  2.2558e+00,\n",
            "          1.1732e-02, -8.6824e-01, -1.5177e-01, -4.8462e-01,  1.1080e+00,\n",
            "         -3.3194e-01,  1.2837e+00, -5.6506e-01,  1.5927e+00, -1.4861e+00,\n",
            "          2.9012e-01, -5.9401e-01, -5.0278e-01,  1.0321e-01, -4.5450e-01],\n",
            "        [-4.2546e-01, -1.3693e+00,  1.1980e+00,  9.5446e-01, -4.4910e-01,\n",
            "          6.0232e-01,  5.1533e-01, -9.9610e-01,  1.5388e-01, -1.8207e-01,\n",
            "         -1.2819e-01,  5.2755e-01, -2.9335e-01, -7.1929e-01,  2.8600e+00,\n",
            "          7.4225e-01, -9.5376e-01,  2.6699e-01, -5.3662e-01,  1.1549e+00,\n",
            "         -7.3000e-01,  6.0263e-01, -2.6132e-01,  8.0190e-01, -1.0564e+00,\n",
            "         -2.2104e-01, -9.2111e-01, -6.1340e-01, -1.1421e-02, -5.1229e-01],\n",
            "        [ 1.4456e+00,  1.4430e+00,  3.9503e-02, -4.9428e-01, -1.7162e-01,\n",
            "          1.6918e+00, -1.6913e-01,  5.4007e-01, -2.4891e-01, -1.6239e+00,\n",
            "         -1.1539e+00,  8.8705e-02,  9.0749e-01,  2.0482e-01,  8.2042e-01,\n",
            "          8.5571e-01, -5.0289e-01,  5.2138e-01, -4.3067e-01, -1.4331e-01,\n",
            "         -9.3170e-01, -5.0391e-02,  6.6428e-01, -1.6031e+00,  1.3209e-01,\n",
            "         -2.2931e-01, -1.1975e+00, -7.4376e-01, -3.1661e-01,  6.5616e-01],\n",
            "        [ 2.4218e-01, -5.9768e-01,  1.1005e+00,  5.6765e-01, -5.4446e-02,\n",
            "          9.1632e-01,  1.5424e-01, -1.0836e+00,  3.4453e-01, -5.5743e-01,\n",
            "         -9.3097e-01,  3.1901e-01,  8.3606e-01, -7.7600e-01,  3.1770e+00,\n",
            "          5.9661e-01, -6.7627e-01,  9.3164e-01, -4.6677e-01,  3.1344e-01,\n",
            "         -1.0786e+00, -6.6413e-03, -3.0839e-01, -9.8576e-02, -8.2482e-01,\n",
            "         -1.2412e+00, -4.4041e-01, -2.5328e-01,  2.5006e-01, -3.5412e-01],\n",
            "        [-1.4160e-01, -1.2564e+00,  7.9684e-01,  7.9770e-01, -5.4950e-01,\n",
            "          5.8694e-01,  6.6909e-01, -8.0470e-01,  6.2666e-01,  2.9422e-02,\n",
            "         -1.5873e-02,  3.7755e-01, -1.3679e-01, -1.3262e+00,  2.8166e+00,\n",
            "          2.8284e-01, -9.3402e-01,  3.1160e-01, -7.0333e-01,  9.2041e-01,\n",
            "         -7.1729e-01,  6.6184e-01, -4.4618e-01,  9.5905e-01, -1.3064e+00,\n",
            "         -2.3818e-01, -4.9275e-01, -5.3834e-01,  8.8924e-02, -3.1791e-01],\n",
            "        [ 4.1390e-01,  8.6715e-01, -3.1197e-01,  6.3288e-03, -8.0340e-01,\n",
            "          1.4527e+00,  8.1691e-01,  1.0328e+00,  3.0417e-02, -6.2414e-01,\n",
            "          2.8673e-01,  3.6348e-01, -6.6360e-01, -5.9736e-01,  7.3032e-01,\n",
            "          1.0375e+00, -7.3359e-01, -6.2229e-01, -5.0102e-01,  6.3891e-01,\n",
            "         -6.8807e-01,  1.1762e+00,  3.9835e-01, -4.0575e-01, -9.6424e-01,\n",
            "          1.4445e+00, -1.5759e+00, -1.8292e+00, -6.5152e-01,  2.7569e-01]],\n",
            "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
            "--------------------------------------------\n",
            "Feature_bn\n",
            "tensor([[-0.2607, -0.0490, -0.2391,  ...,  0.2377,  0.2455, -0.4793],\n",
            "        [-0.4384, -0.5920, -0.0998,  ..., -0.0805,  0.3504, -0.5385],\n",
            "        [-0.4910, -0.1842, -0.2911,  ...,  0.0182,  0.0933, -0.3557],\n",
            "        ...,\n",
            "        [-0.3869, -0.0203, -0.3527,  ..., -0.1529,  0.1781, -0.2640],\n",
            "        [-0.0941, -0.3292, -0.5455,  ...,  0.1264,  0.4184, -0.6171],\n",
            "        [-0.7649, -0.1076, -0.0052,  ...,  0.0871,  0.0885, -0.5696]],\n",
            "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
            "--------------------------------------------\n",
            "LM LOGSSSSSSSSSSSSSSSSSSS None\n",
            "Should not be here\n",
            "  0% 0/41 [00:03<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/deepmlf/experiments/regression/mult_base.py\", line 32, in <module>\n",
            "    MMSA_run(\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/run.py\", line 390, in MMSA_run\n",
            "    result = _run(args, num_workers, is_tune)\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/run.py\", line 445, in _run\n",
            "    epoch_results = trainer.do_train(model, dataloader, return_epoch_results=from_sena)\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/trains/singleTask/MSALM.py\", line 652, in do_train\n",
            "    lm_logits = lm_logits[:, :self.args.max_token_len, :].contiguous()\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "-m msalm \\\n",
        "-d mosi \\\n",
        "-g 0 \\\n",
        "--exp-name deepmlf-base-mosi \\\n",
        "-c MMSA/config/regression/deepmlf/mosi/base_mosi_colab.json \\\n",
        "--res-save-dir MMSA/results/deepmlf \\\n",
        "-n 2 \\\n",
        "-s 1990 \\\n",
        "-s 1991"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS8UlQMzBQq_",
        "outputId": "4b407884-6e32-4162-8c25-b4cb92571f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "no change     /root/.zshrc\n",
            "no change     /root/.config/fish/config.fish\n",
            "no change     /root/.xonshrc\n",
            "no change     /root/.tcshrc\n",
            "No action taken.\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/functions.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "MMSA - {'model_name': 'msalm', 'dataset_name': 'mosi', 'featurePath': '/content/drive/MyDrive/MSA-Datasets/CMU-MOSI/Processed/unaligned_50.pkl', 'seq_lens': [50, 375, 500], 'feature_dims': [768, 5, 20], 'train_samples': 1284, 'num_classes': 3, 'language': 'en', 'KeyEval': 'MAE', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 30, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': True, 'update_labels_patience': 2, 'H': 3.0, 'del_model': True, 'max_token_len': 50, 'pad_token': '<|endoftext|>', 'lm': 'gpt2', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.1, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 15, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 12, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [3, 5, 7, 8, 9, 10, 11], 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.2, 'p_apply': 0.3, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': 'checkpoints/bienc-mosi/bienc-mosi-1990.pth', 'feature_dims': [768, 1024, 768], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 50, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.3, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': False, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.001, 'weight_decay_av': 0.001, 'learning_rate_av': 0.0001, 'learning_rate_mmgpt': 0.0001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990, 1991]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/2] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(1284, 375, 5)\n",
            "(1284, 500, 20)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(229, 375, 5)\n",
            "(229, 500, 20)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Using GPT LM\n",
            "All the sequence lengths are L:50, A:375, V:500\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1284, 375, 5)\n",
            "vision features are (1284, 500, 20)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "(686, 375, 5)\n",
            "(686, 500, 20)\n",
            "Ongoing with num_workers=2\n",
            "aaaaaaaaaaaaaaaaaaaaaaaaa gpt2\n",
            "ca list is: [3, 5, 7, 8, 9, 10, 11]\n",
            "initializing SoftPerm\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 0\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 2\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 3\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 4\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 5\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 6\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Parsing decoder block: 0\n",
            "Parsing decoder block: 1\n",
            "Parsing decoder block: 2\n",
            "Parsing decoder block: 3\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 4\n",
            "Parsing decoder block: 5\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 6\n",
            "Parsing decoder block: 7\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 8\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 9\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 10\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 11\n",
            "COpying---------------------------------\n",
            "No normalization is used\n",
            "----------------->>> Pretrained AudioVisual Encoder <<<<<----------------\n",
            "No normalization is used\n",
            "----------------------- Loading AV encoder from checkpoints/bienc-mosi/bienc-mosi-1990.pth\n",
            "Copied param embed_positions_a._float_tensor\n",
            "Copied param embed_positions_v._float_tensor\n",
            "Copied param proj_a.weight\n",
            "Copied param proj_v.weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.0.linear1.weight\n",
            "Copied param enc_a.layers.0.linear1.bias\n",
            "Copied param enc_a.layers.0.linear2.weight\n",
            "Copied param enc_a.layers.0.linear2.bias\n",
            "Copied param enc_a.layers.0.norm1.weight\n",
            "Copied param enc_a.layers.0.norm1.bias\n",
            "Copied param enc_a.layers.0.norm2.weight\n",
            "Copied param enc_a.layers.0.norm2.bias\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.1.linear1.weight\n",
            "Copied param enc_a.layers.1.linear1.bias\n",
            "Copied param enc_a.layers.1.linear2.weight\n",
            "Copied param enc_a.layers.1.linear2.bias\n",
            "Copied param enc_a.layers.1.norm1.weight\n",
            "Copied param enc_a.layers.1.norm1.bias\n",
            "Copied param enc_a.layers.1.norm2.weight\n",
            "Copied param enc_a.layers.1.norm2.bias\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.2.linear1.weight\n",
            "Copied param enc_a.layers.2.linear1.bias\n",
            "Copied param enc_a.layers.2.linear2.weight\n",
            "Copied param enc_a.layers.2.linear2.bias\n",
            "Copied param enc_a.layers.2.norm1.weight\n",
            "Copied param enc_a.layers.2.norm1.bias\n",
            "Copied param enc_a.layers.2.norm2.weight\n",
            "Copied param enc_a.layers.2.norm2.bias\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.0.linear1.weight\n",
            "Copied param enc_v.layers.0.linear1.bias\n",
            "Copied param enc_v.layers.0.linear2.weight\n",
            "Copied param enc_v.layers.0.linear2.bias\n",
            "Copied param enc_v.layers.0.norm1.weight\n",
            "Copied param enc_v.layers.0.norm1.bias\n",
            "Copied param enc_v.layers.0.norm2.weight\n",
            "Copied param enc_v.layers.0.norm2.bias\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.1.linear1.weight\n",
            "Copied param enc_v.layers.1.linear1.bias\n",
            "Copied param enc_v.layers.1.linear2.weight\n",
            "Copied param enc_v.layers.1.linear2.bias\n",
            "Copied param enc_v.layers.1.norm1.weight\n",
            "Copied param enc_v.layers.1.norm1.bias\n",
            "Copied param enc_v.layers.1.norm2.weight\n",
            "Copied param enc_v.layers.1.norm2.bias\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.2.linear1.weight\n",
            "Copied param enc_v.layers.2.linear1.bias\n",
            "Copied param enc_v.layers.2.linear2.weight\n",
            "Copied param enc_v.layers.2.linear2.bias\n",
            "Copied param enc_v.layers.2.norm1.weight\n",
            "Copied param enc_v.layers.2.norm1.bias\n",
            "Copied param enc_v.layers.2.norm2.weight\n",
            "Copied param enc_v.layers.2.norm2.bias\n",
            "Copied param fusion.weight\n",
            "Copied param fusion.bias\n",
            "Copied param clf.weight\n",
            "Copied param clf.bias\n",
            "------------------ Adding LNorm ------------------------\n",
            "MMSA - The model has 166279523 trainable parameters\n",
            "ongoing with msalm\n",
            "3.ca_layer.alpha_1\n",
            "3.ca_layer.alpha_2\n",
            "3.ca_layer.ln_1.weight\n",
            "3.ca_layer.ln_1.bias\n",
            "3.ca_layer.ln_2.weight\n",
            "3.ca_layer.ln_2.bias\n",
            "3.ca_layer.attn.W_q.weight\n",
            "3.ca_layer.attn.W_kv.weight\n",
            "3.ca_layer.attn.W_o.weight\n",
            "3.ca_layer.mlp.c_fc.weight\n",
            "3.ca_layer.mlp.c_fc.bias\n",
            "3.ca_layer.mlp.c_proj.weight\n",
            "3.ca_layer.mlp.c_proj.bias\n",
            "5.ca_layer.alpha_1\n",
            "5.ca_layer.alpha_2\n",
            "5.ca_layer.ln_1.weight\n",
            "5.ca_layer.ln_1.bias\n",
            "5.ca_layer.ln_2.weight\n",
            "5.ca_layer.ln_2.bias\n",
            "5.ca_layer.attn.W_q.weight\n",
            "5.ca_layer.attn.W_kv.weight\n",
            "5.ca_layer.attn.W_o.weight\n",
            "5.ca_layer.mlp.c_fc.weight\n",
            "5.ca_layer.mlp.c_fc.bias\n",
            "5.ca_layer.mlp.c_proj.weight\n",
            "5.ca_layer.mlp.c_proj.bias\n",
            "7.ca_layer.alpha_1\n",
            "7.ca_layer.alpha_2\n",
            "7.ca_layer.ln_1.weight\n",
            "7.ca_layer.ln_1.bias\n",
            "7.ca_layer.ln_2.weight\n",
            "7.ca_layer.ln_2.bias\n",
            "7.ca_layer.attn.W_q.weight\n",
            "7.ca_layer.attn.W_kv.weight\n",
            "7.ca_layer.attn.W_o.weight\n",
            "7.ca_layer.mlp.c_fc.weight\n",
            "7.ca_layer.mlp.c_fc.bias\n",
            "7.ca_layer.mlp.c_proj.weight\n",
            "7.ca_layer.mlp.c_proj.bias\n",
            "8.ca_layer.alpha_1\n",
            "8.ca_layer.alpha_2\n",
            "8.ca_layer.ln_1.weight\n",
            "8.ca_layer.ln_1.bias\n",
            "8.ca_layer.ln_2.weight\n",
            "8.ca_layer.ln_2.bias\n",
            "8.ca_layer.attn.W_q.weight\n",
            "8.ca_layer.attn.W_kv.weight\n",
            "8.ca_layer.attn.W_o.weight\n",
            "8.ca_layer.mlp.c_fc.weight\n",
            "8.ca_layer.mlp.c_fc.bias\n",
            "8.ca_layer.mlp.c_proj.weight\n",
            "8.ca_layer.mlp.c_proj.bias\n",
            "9.ca_layer.alpha_1\n",
            "9.ca_layer.alpha_2\n",
            "9.ca_layer.ln_1.weight\n",
            "9.ca_layer.ln_1.bias\n",
            "9.ca_layer.ln_2.weight\n",
            "9.ca_layer.ln_2.bias\n",
            "9.ca_layer.attn.W_q.weight\n",
            "9.ca_layer.attn.W_kv.weight\n",
            "9.ca_layer.attn.W_o.weight\n",
            "9.ca_layer.mlp.c_fc.weight\n",
            "9.ca_layer.mlp.c_fc.bias\n",
            "9.ca_layer.mlp.c_proj.weight\n",
            "9.ca_layer.mlp.c_proj.bias\n",
            "10.ca_layer.alpha_1\n",
            "10.ca_layer.alpha_2\n",
            "10.ca_layer.ln_1.weight\n",
            "10.ca_layer.ln_1.bias\n",
            "10.ca_layer.ln_2.weight\n",
            "10.ca_layer.ln_2.bias\n",
            "10.ca_layer.attn.W_q.weight\n",
            "10.ca_layer.attn.W_kv.weight\n",
            "10.ca_layer.attn.W_o.weight\n",
            "10.ca_layer.mlp.c_fc.weight\n",
            "10.ca_layer.mlp.c_fc.bias\n",
            "10.ca_layer.mlp.c_proj.weight\n",
            "10.ca_layer.mlp.c_proj.bias\n",
            "11.ca_layer.alpha_1\n",
            "11.ca_layer.alpha_2\n",
            "11.ca_layer.ln_1.weight\n",
            "11.ca_layer.ln_1.bias\n",
            "11.ca_layer.ln_2.weight\n",
            "11.ca_layer.ln_2.bias\n",
            "11.ca_layer.attn.W_q.weight\n",
            "11.ca_layer.attn.W_kv.weight\n",
            "11.ca_layer.attn.W_o.weight\n",
            "11.ca_layer.mlp.c_fc.weight\n",
            "11.ca_layer.mlp.c_fc.bias\n",
            "11.ca_layer.mlp.c_proj.weight\n",
            "11.ca_layer.mlp.c_proj.bias\n",
            "0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.W_task_0.0.weight\n",
            "Model.W_task_0.0.bias\n",
            "Model.W_task_0.1.weight\n",
            "Model.W_task_0.1.bias\n",
            "Model.W_task_1.weight\n",
            "Model.W_task_1.bias\n",
            "Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Model.av_encoder.proj_a.weight\n",
            "Model.av_encoder.proj_v.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm2.bias\n",
            "Model.av_encoder.fusion.weight\n",
            "Model.av_encoder.fusion.bias\n",
            "Model.av_encoder.clf.weight\n",
            "Model.av_encoder.clf.bias\n",
            "Model.LN.weight\n",
            "Model.LN.bias\n",
            "The total number of trainable parameters is 41.84 M\n",
            "Model.lang_encoder.transformer.wte.0.embedding.weight\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wpe.0.positional.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.3.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.ln_f.weight\n",
            "Model.lang_encoder.transformer.ln_f.bias\n",
            "Model.W_task_0.0.weight\n",
            "Using grad with decay in Model.W_task_0.0.weight\n",
            "Model.W_task_0.0.bias\n",
            "Using grad with no decay in Model.W_task_0.0.bias\n",
            "Model.W_task_0.1.weight\n",
            "Using grad with decay in Model.W_task_0.1.weight\n",
            "Model.W_task_0.1.bias\n",
            "Using grad with no decay in Model.W_task_0.1.bias\n",
            "Model.W_task_1.weight\n",
            "Using grad with decay in Model.W_task_1.weight\n",
            "Model.W_task_1.bias\n",
            "Using grad with no decay in Model.W_task_1.bias\n",
            "Model.W_bn.weight\n",
            "Using grad with decay in Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Using grad with no decay in Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Using grad with decay in Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Using grad with no decay in Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Using grad with decay in Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Using grad with no decay in Model.W_av.bias\n",
            "Model.LN.weight\n",
            "Using grad with decay in Model.LN.weight\n",
            "Model.LN.bias\n",
            "Using grad with no decay in Model.LN.bias\n",
            "Will be using warmup for 5 steps\n",
            "  0% 0/41 [00:00<?, ?it/s]Should not be here\n",
            "  0% 0/41 [00:03<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/deepmlf/experiments/regression/mult_base.py\", line 32, in <module>\n",
            "    MMSA_run(\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/run.py\", line 390, in MMSA_run\n",
            "    result = _run(args, num_workers, is_tune)\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/run.py\", line 445, in _run\n",
            "    epoch_results = trainer.do_train(model, dataloader, return_epoch_results=from_sena)\n",
            "  File \"/content/deepmlf/experiments/regression/../../MMSA/trains/singleTask/MSALM.py\", line 645, in do_train\n",
            "    lm_logits = lm_logits[:, :self.args.max_token_len, :].contiguous()\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "-m msalm \\\n",
        "-d mosi \\\n",
        "-g 0 \\\n",
        "--exp-name deepmlf-base-mosi \\\n",
        "-c MMSA/config/regression/deepmlf/mosi/best.json \\\n",
        "--res-save-dir MMSA/results/deepmlf \\\n",
        "-n 2 \\\n",
        "-s 1990 \\\n",
        "-s 1991"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMQ6trt5N9BG"
      },
      "outputs": [],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "  -m bienc \\\n",
        "  -d mosei \\\n",
        "  -g 0 \\\n",
        "  --exp-name bienc-mosei \\\n",
        "  -c MMSA/config/regression/deepmlf/mosei/bienc.json \\\n",
        "  --res-save-dir MMSA/results/bienc_mosei \\\n",
        "  -n 2 \\\n",
        "  -s 1990"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVB0JoIvX2Zd"
      },
      "outputs": [],
      "source": [
        "python experiments/regression/mult_base.py \\\n",
        "-m msalm \\\n",
        "-d sims \\\n",
        "-g 0 \\\n",
        "--exp-name deepmlf-base-sims \\\n",
        "-c MMSA/config/regression/deepmlf/sims/base_best.json \\\n",
        "--res-save-dir MMSA/results/deepmlf \\\n",
        "-n 2 \\\n",
        "-s 1990 \\\n",
        "-s 1991"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AV Encoder Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgpX3f182r1D",
        "outputId": "a321941f-bab0-4772-f86a-5a75fd91c752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "no change     /root/.zshrc\n",
            "no change     /root/.config/fish/config.fish\n",
            "no change     /root/.xonshrc\n",
            "no change     /root/.tcshrc\n",
            "No action taken.\n",
            "2025-11-08 14:35:13.099401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/functions.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "MMSA - {'model_name': 'bienc', 'dataset_name': 'sims', 'featurePath': '/content/drive/MyDrive/MSA-Datasets/CH-SIMS/Processed/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 8, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'update_epochs': 4, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'fs2': False, 'max_token_len': 39, 'gpt': {'mm_dropout': 0.1, 'mm_layer': [3, 5, 7, 9, 11], 'layer_dropout': 0.2, 'dense': True, 'tie_ffn': True}, 'av_enc': {'d_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 39, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_ln': False, 'use_bn': True}, 'batch_size': 32, 'learning_rate': 0.0005, 'grad_clip': 1.0, 'patience': 10, 'weight_decay': 0.001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/1] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Map (num_proc=8):   0% 0/1368 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  12% 171/1368 [00:13<01:37, 12.25 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  25% 342/1368 [00:20<00:56, 18.30 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  38% 513/1368 [00:26<00:39, 21.76 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  50% 684/1368 [00:33<00:30, 22.09 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  62% 855/1368 [00:38<00:20, 25.43 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  75% 1026/1368 [00:46<00:14, 24.24 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):   0% 0/1368 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "(1368, 400, 33)\n",
            "(1368, 55, 709)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Map (num_proc=8):   0% 0/456 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  12% 57/456 [00:03<00:27, 14.62 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  25% 114/456 [00:07<00:22, 15.38 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  38% 171/456 [00:10<00:15, 17.83 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  50% 228/456 [00:12<00:11, 20.04 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  62% 285/456 [00:14<00:07, 22.02 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  75% 342/456 [00:16<00:04, 24.65 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):   0% 0/456 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "(456, 400, 33)\n",
            "(456, 55, 709)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Map (num_proc=8):   0% 0/457 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  13% 58/457 [00:03<00:24, 16.31 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  25% 115/457 [00:06<00:18, 18.63 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  38% 172/457 [00:09<00:15, 18.24 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  50% 229/457 [00:11<00:10, 21.44 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  63% 286/457 [00:13<00:07, 24.01 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):  75% 343/457 [00:15<00:04, 24.98 examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=8):   0% 0/457 [00:00<?, ? examples/s]/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "(457, 400, 33)\n",
            "(457, 55, 709)\n",
            "Ongoing with num_workers=2\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "MMSA - The model has 92745 trainable parameters\n",
            "ongoing with bienc\n",
            "------- Ongoing with the ReduceLROnPlateau -----------\n",
            "100% 43/43 [00:10<00:00,  4.09it/s]\n",
            "MMSA - TRAIN-(bienc) [1/1/1] >> loss: 0.8127  Mult_acc_2: 0.5709  Mult_acc_3: 0.4269  Mult_acc_5: 0.2230  F1_score: 0.5818  MAE: 0.7332  Corr: 0.0116 \n",
            "100% 15/15 [00:02<00:00,  6.34it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6425  Mult_acc_3: 0.4276  Mult_acc_5: 0.1601  F1_score: 0.6225  MAE: 0.6315  Corr: 0.0456  Loss: 0.6296 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  4.93it/s]\n",
            "MMSA - TRAIN-(bienc) [1/2/1] >> loss: 0.672  Mult_acc_2: 0.5980  Mult_acc_3: 0.4481  Mult_acc_5: 0.2266  F1_score: 0.5969  MAE: 0.6520  Corr: 0.0585 \n",
            "100% 15/15 [00:02<00:00,  6.19it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6447  Mult_acc_3: 0.4496  Mult_acc_5: 0.1754  F1_score: 0.5989  MAE: 0.6090  Corr: 0.0816  Loss: 0.6169 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:06<00:00,  6.26it/s]\n",
            "MMSA - TRAIN-(bienc) [1/3/1] >> loss: 0.638  Mult_acc_2: 0.6001  Mult_acc_3: 0.4569  Mult_acc_5: 0.2259  F1_score: 0.5924  MAE: 0.6317  Corr: 0.0570 \n",
            "100% 15/15 [00:03<00:00,  3.90it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6820  Mult_acc_3: 0.5044  Mult_acc_5: 0.2061  F1_score: 0.6028  MAE: 0.6046  Corr: 0.1140  Loss: 0.5975 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:07<00:00,  6.07it/s]\n",
            "MMSA - TRAIN-(bienc) [1/4/1] >> loss: 0.6037  Mult_acc_2: 0.6425  Mult_acc_3: 0.4591  Mult_acc_5: 0.2186  F1_score: 0.6223  MAE: 0.5995  Corr: 0.1363 \n",
            "100% 15/15 [00:02<00:00,  6.30it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6930  Mult_acc_3: 0.5197  Mult_acc_5: 0.2149  F1_score: 0.5977  MAE: 0.6047  Corr: 0.1465  Loss: 0.5901 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:09<00:00,  4.68it/s]\n",
            "MMSA - TRAIN-(bienc) [1/5/1] >> loss: 0.6022  Mult_acc_2: 0.6579  Mult_acc_3: 0.4883  Mult_acc_5: 0.2230  F1_score: 0.6289  MAE: 0.5971  Corr: 0.1314 \n",
            "100% 15/15 [00:02<00:00,  6.21it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6930  Mult_acc_3: 0.5219  Mult_acc_5: 0.2171  F1_score: 0.6178  MAE: 0.5994  Corr: 0.1520  Loss: 0.5971 \n",
            "100% 43/43 [00:08<00:00,  5.19it/s]\n",
            "MMSA - TRAIN-(bienc) [2/6/1] >> loss: 0.5902  Mult_acc_2: 0.6667  Mult_acc_3: 0.4905  Mult_acc_5: 0.2295  F1_score: 0.6524  MAE: 0.5857  Corr: 0.1810 \n",
            "100% 15/15 [00:02<00:00,  5.32it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6579  Mult_acc_3: 0.4518  Mult_acc_5: 0.2018  F1_score: 0.6460  MAE: 0.5849  Corr: 0.1892  Loss: 0.5829 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:06<00:00,  6.15it/s]\n",
            "MMSA - TRAIN-(bienc) [1/7/1] >> loss: 0.5748  Mult_acc_2: 0.6645  Mult_acc_3: 0.5102  Mult_acc_5: 0.2288  F1_score: 0.6308  MAE: 0.5737  Corr: 0.2187 \n",
            "100% 15/15 [00:02<00:00,  5.74it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6250  Mult_acc_3: 0.4189  Mult_acc_5: 0.1732  F1_score: 0.6342  MAE: 0.5893  Corr: 0.2350  Loss: 0.5920 \n",
            "100% 43/43 [00:08<00:00,  4.94it/s]\n",
            "MMSA - TRAIN-(bienc) [2/8/1] >> loss: 0.5755  Mult_acc_2: 0.6513  Mult_acc_3: 0.4963  Mult_acc_5: 0.2193  F1_score: 0.6331  MAE: 0.5733  Corr: 0.2392 \n",
            "100% 15/15 [00:02<00:00,  6.31it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7215  Mult_acc_3: 0.5329  Mult_acc_5: 0.2193  F1_score: 0.6673  MAE: 0.5717  Corr: 0.2668  Loss: 0.5680 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  4.91it/s]\n",
            "MMSA - TRAIN-(bienc) [1/9/1] >> loss: 0.5552  Mult_acc_2: 0.6930  Mult_acc_3: 0.5088  Mult_acc_5: 0.2361  F1_score: 0.6723  MAE: 0.5534  Corr: 0.2984 \n",
            "100% 15/15 [00:02<00:00,  6.37it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6996  Mult_acc_3: 0.5175  Mult_acc_5: 0.2193  F1_score: 0.6719  MAE: 0.5694  Corr: 0.2849  Loss: 0.5679 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:07<00:00,  5.95it/s]\n",
            "MMSA - TRAIN-(bienc) [1/10/1] >> loss: 0.5392  Mult_acc_2: 0.7032  Mult_acc_3: 0.5461  Mult_acc_5: 0.2442  F1_score: 0.6941  MAE: 0.5370  Corr: 0.3555 \n",
            "100% 15/15 [00:03<00:00,  4.00it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7105  Mult_acc_3: 0.5241  Mult_acc_5: 0.2171  F1_score: 0.6927  MAE: 0.5568  Corr: 0.3308  Loss: 0.5528 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:07<00:00,  6.00it/s]\n",
            "MMSA - TRAIN-(bienc) [1/11/1] >> loss: 0.5542  Mult_acc_2: 0.7105  Mult_acc_3: 0.5497  Mult_acc_5: 0.2493  F1_score: 0.7035  MAE: 0.5496  Corr: 0.3398 \n",
            "100% 15/15 [00:02<00:00,  6.27it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6842  Mult_acc_3: 0.5219  Mult_acc_5: 0.2303  F1_score: 0.6910  MAE: 0.5618  Corr: 0.3594  Loss: 0.5570 \n",
            "100% 43/43 [00:08<00:00,  4.95it/s]\n",
            "MMSA - TRAIN-(bienc) [2/12/1] >> loss: 0.5448  Mult_acc_2: 0.7156  Mult_acc_3: 0.5599  Mult_acc_5: 0.2580  F1_score: 0.7057  MAE: 0.5392  Corr: 0.3692 \n",
            "100% 15/15 [00:02<00:00,  6.44it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.5636  Mult_acc_5: 0.2303  F1_score: 0.7335  MAE: 0.5407  Corr: 0.4062  Loss: 0.5463 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  5.03it/s]\n",
            "MMSA - TRAIN-(bienc) [1/13/1] >> loss: 0.5304  Mult_acc_2: 0.7054  Mult_acc_3: 0.5468  Mult_acc_5: 0.2412  F1_score: 0.7021  MAE: 0.5295  Corr: 0.3933 \n",
            "100% 15/15 [00:02<00:00,  5.31it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6842  Mult_acc_3: 0.5000  Mult_acc_5: 0.1908  F1_score: 0.6937  MAE: 0.5571  Corr: 0.3824  Loss: 0.5616 \n",
            "100% 43/43 [00:06<00:00,  6.17it/s]\n",
            "MMSA - TRAIN-(bienc) [2/14/1] >> loss: 0.5204  Mult_acc_2: 0.7383  Mult_acc_3: 0.5760  Mult_acc_5: 0.2705  F1_score: 0.7320  MAE: 0.5172  Corr: 0.4351 \n",
            "100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.5768  Mult_acc_5: 0.2675  F1_score: 0.7420  MAE: 0.5377  Corr: 0.4165  Loss: 0.5333 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:07<00:00,  5.46it/s]\n",
            "MMSA - TRAIN-(bienc) [1/15/1] >> loss: 0.514  Mult_acc_2: 0.7230  Mult_acc_3: 0.5885  Mult_acc_5: 0.2917  F1_score: 0.7190  MAE: 0.5100  Corr: 0.4469 \n",
            "100% 15/15 [00:02<00:00,  6.48it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.5526  Mult_acc_5: 0.2588  F1_score: 0.7181  MAE: 0.5393  Corr: 0.4161  Loss: 0.5484 \n",
            "100% 43/43 [00:08<00:00,  4.80it/s]\n",
            "MMSA - TRAIN-(bienc) [2/16/1] >> loss: 0.5147  Mult_acc_2: 0.7332  Mult_acc_3: 0.5797  Mult_acc_5: 0.2763  F1_score: 0.7346  MAE: 0.5100  Corr: 0.4553 \n",
            "100% 15/15 [00:02<00:00,  6.37it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.5768  Mult_acc_5: 0.2851  F1_score: 0.7225  MAE: 0.5378  Corr: 0.4212  Loss: 0.5515 \n",
            "100% 43/43 [00:07<00:00,  5.38it/s]\n",
            "MMSA - TRAIN-(bienc) [3/17/1] >> loss: 0.5029  Mult_acc_2: 0.7485  Mult_acc_3: 0.5870  Mult_acc_5: 0.2844  F1_score: 0.7442  MAE: 0.5013  Corr: 0.4888 \n",
            "100% 15/15 [00:02<00:00,  5.11it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7412  Mult_acc_3: 0.5768  Mult_acc_5: 0.2851  F1_score: 0.7378  MAE: 0.5295  Corr: 0.4401  Loss: 0.5341 \n",
            "100% 43/43 [00:06<00:00,  6.31it/s]\n",
            "MMSA - TRAIN-(bienc) [4/18/1] >> loss: 0.4984  Mult_acc_2: 0.7332  Mult_acc_3: 0.5928  Mult_acc_5: 0.2887  F1_score: 0.7353  MAE: 0.4945  Corr: 0.4977 \n",
            "100% 15/15 [00:02<00:00,  6.20it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6820  Mult_acc_3: 0.5702  Mult_acc_5: 0.2697  F1_score: 0.6928  MAE: 0.5379  Corr: 0.4360  Loss: 0.5289 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  5.05it/s]\n",
            "MMSA - TRAIN-(bienc) [1/19/1] >> loss: 0.5008  Mult_acc_2: 0.7332  Mult_acc_3: 0.6038  Mult_acc_5: 0.2975  F1_score: 0.7320  MAE: 0.4936  Corr: 0.4892 \n",
            "100% 15/15 [00:02<00:00,  6.43it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.5724  Mult_acc_5: 0.2632  F1_score: 0.7339  MAE: 0.5263  Corr: 0.4441  Loss: 0.5263 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  5.04it/s]\n",
            "MMSA - TRAIN-(bienc) [1/20/1] >> loss: 0.4965  Mult_acc_2: 0.7471  Mult_acc_3: 0.6075  Mult_acc_5: 0.3063  F1_score: 0.7472  MAE: 0.4927  Corr: 0.4876 \n",
            "100% 15/15 [00:02<00:00,  6.34it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6601  Mult_acc_3: 0.5592  Mult_acc_5: 0.2588  F1_score: 0.6730  MAE: 0.5384  Corr: 0.4351  Loss: 0.5507 \n",
            "100% 43/43 [00:06<00:00,  6.18it/s]\n",
            "MMSA - TRAIN-(bienc) [2/21/1] >> loss: 0.4988  Mult_acc_2: 0.7573  Mult_acc_3: 0.6170  Mult_acc_5: 0.3143  F1_score: 0.7543  MAE: 0.4930  Corr: 0.4957 \n",
            "100% 15/15 [00:03<00:00,  4.19it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.5592  Mult_acc_5: 0.2566  F1_score: 0.7319  MAE: 0.5290  Corr: 0.4357  Loss: 0.5286 \n",
            "100% 43/43 [00:07<00:00,  5.97it/s]\n",
            "MMSA - TRAIN-(bienc) [3/22/1] >> loss: 0.4829  Mult_acc_2: 0.7551  Mult_acc_3: 0.6096  Mult_acc_5: 0.3158  F1_score: 0.7529  MAE: 0.4800  Corr: 0.5147 \n",
            "100% 15/15 [00:02<00:00,  6.51it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.5702  Mult_acc_5: 0.2939  F1_score: 0.7301  MAE: 0.5230  Corr: 0.4373  Loss: 0.5201 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  4.99it/s]\n",
            "MMSA - TRAIN-(bienc) [1/23/1] >> loss: 0.483  Mult_acc_2: 0.7478  Mult_acc_3: 0.6140  Mult_acc_5: 0.3107  F1_score: 0.7477  MAE: 0.4794  Corr: 0.5218 \n",
            "100% 15/15 [00:02<00:00,  6.41it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7127  Mult_acc_3: 0.5702  Mult_acc_5: 0.2873  F1_score: 0.7136  MAE: 0.5299  Corr: 0.4327  Loss: 0.5507 \n",
            "100% 43/43 [00:08<00:00,  5.14it/s]\n",
            "MMSA - TRAIN-(bienc) [2/24/1] >> loss: 0.4822  Mult_acc_2: 0.7493  Mult_acc_3: 0.6053  Mult_acc_5: 0.3260  F1_score: 0.7508  MAE: 0.4735  Corr: 0.5334 \n",
            "100% 15/15 [00:02<00:00,  5.62it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.5855  Mult_acc_5: 0.3048  F1_score: 0.7160  MAE: 0.5183  Corr: 0.4540  Loss: 0.5353 \n",
            "100% 43/43 [00:07<00:00,  6.00it/s]\n",
            "MMSA - TRAIN-(bienc) [3/25/1] >> loss: 0.4909  Mult_acc_2: 0.7456  Mult_acc_3: 0.6053  Mult_acc_5: 0.3070  F1_score: 0.7423  MAE: 0.4887  Corr: 0.5037 \n",
            "100% 15/15 [00:03<00:00,  5.00it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.5789  Mult_acc_5: 0.3246  F1_score: 0.7197  MAE: 0.5253  Corr: 0.4586  Loss: 0.5209 \n",
            "100% 43/43 [00:07<00:00,  5.67it/s]\n",
            "MMSA - TRAIN-(bienc) [4/26/1] >> loss: 0.4819  Mult_acc_2: 0.7734  Mult_acc_3: 0.6265  Mult_acc_5: 0.3304  F1_score: 0.7691  MAE: 0.4762  Corr: 0.5332 \n",
            "100% 15/15 [00:02<00:00,  6.47it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.5965  Mult_acc_5: 0.3311  F1_score: 0.7239  MAE: 0.5180  Corr: 0.4604  Loss: 0.5276 \n",
            "100% 43/43 [00:08<00:00,  5.01it/s]\n",
            "MMSA - TRAIN-(bienc) [5/27/1] >> loss: 0.4796  Mult_acc_2: 0.7632  Mult_acc_3: 0.6148  Mult_acc_5: 0.3173  F1_score: 0.7591  MAE: 0.4749  Corr: 0.5306 \n",
            "100% 15/15 [00:02<00:00,  6.55it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.5636  Mult_acc_5: 0.2851  F1_score: 0.7228  MAE: 0.5261  Corr: 0.4377  Loss: 0.5269 \n",
            "100% 43/43 [00:07<00:00,  5.92it/s]\n",
            "MMSA - TRAIN-(bienc) [6/28/1] >> loss: 0.4733  Mult_acc_2: 0.7668  Mult_acc_3: 0.6316  Mult_acc_5: 0.3121  F1_score: 0.7647  MAE: 0.4709  Corr: 0.5476 \n",
            "100% 15/15 [00:03<00:00,  4.32it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.5702  Mult_acc_5: 0.2873  F1_score: 0.7238  MAE: 0.5163  Corr: 0.4512  Loss: 0.5219 \n",
            "100% 43/43 [00:06<00:00,  6.41it/s]\n",
            "MMSA - TRAIN-(bienc) [7/29/1] >> loss: 0.4788  Mult_acc_2: 0.7566  Mult_acc_3: 0.6250  Mult_acc_5: 0.3194  F1_score: 0.7540  MAE: 0.4731  Corr: 0.5358 \n",
            "100% 15/15 [00:02<00:00,  6.67it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.5658  Mult_acc_5: 0.2982  F1_score: 0.7162  MAE: 0.5192  Corr: 0.4433  Loss: 0.5341 \n",
            "100% 43/43 [00:08<00:00,  5.13it/s]\n",
            "MMSA - TRAIN-(bienc) [8/30/1] >> loss: 0.4878  Mult_acc_2: 0.7471  Mult_acc_3: 0.5950  Mult_acc_5: 0.3048  F1_score: 0.7396  MAE: 0.4830  Corr: 0.5097 \n",
            "100% 15/15 [00:02<00:00,  6.59it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6996  Mult_acc_3: 0.5680  Mult_acc_5: 0.2763  F1_score: 0.7073  MAE: 0.5166  Corr: 0.4537  Loss: 0.5109 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  5.13it/s]\n",
            "MMSA - TRAIN-(bienc) [1/31/1] >> loss: 0.4823  Mult_acc_2: 0.7420  Mult_acc_3: 0.6031  Mult_acc_5: 0.3048  F1_score: 0.7410  MAE: 0.4787  Corr: 0.5192 \n",
            "100% 15/15 [00:02<00:00,  6.03it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.5746  Mult_acc_5: 0.2829  F1_score: 0.7257  MAE: 0.5091  Corr: 0.4620  Loss: 0.5271 \n",
            "100% 43/43 [00:06<00:00,  6.40it/s]\n",
            "MMSA - TRAIN-(bienc) [2/32/1] >> loss: 0.4657  Mult_acc_2: 0.7639  Mult_acc_3: 0.6257  Mult_acc_5: 0.2975  F1_score: 0.7630  MAE: 0.4632  Corr: 0.5596 \n",
            "100% 15/15 [00:02<00:00,  5.09it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6952  Mult_acc_3: 0.5504  Mult_acc_5: 0.2697  F1_score: 0.6998  MAE: 0.5166  Corr: 0.4556  Loss: 0.5256 \n",
            "100% 43/43 [00:07<00:00,  5.44it/s]\n",
            "MMSA - TRAIN-(bienc) [3/33/1] >> loss: 0.4691  Mult_acc_2: 0.7602  Mult_acc_3: 0.6192  Mult_acc_5: 0.3202  F1_score: 0.7580  MAE: 0.4654  Corr: 0.5524 \n",
            "100% 15/15 [00:02<00:00,  6.64it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.5899  Mult_acc_5: 0.2917  F1_score: 0.7238  MAE: 0.5088  Corr: 0.4562  Loss: 0.5201 \n",
            "100% 43/43 [00:08<00:00,  4.98it/s]\n",
            "MMSA - TRAIN-(bienc) [4/34/1] >> loss: 0.4677  Mult_acc_2: 0.7478  Mult_acc_3: 0.6133  Mult_acc_5: 0.3260  F1_score: 0.7479  MAE: 0.4617  Corr: 0.5551 \n",
            "100% 15/15 [00:02<00:00,  6.60it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.5833  Mult_acc_5: 0.3311  F1_score: 0.7200  MAE: 0.5110  Corr: 0.4623  Loss: 0.5085 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:07<00:00,  6.00it/s]\n",
            "MMSA - TRAIN-(bienc) [1/35/1] >> loss: 0.4667  Mult_acc_2: 0.7573  Mult_acc_3: 0.6199  Mult_acc_5: 0.3326  F1_score: 0.7558  MAE: 0.4618  Corr: 0.5585 \n",
            "100% 15/15 [00:03<00:00,  4.05it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.5789  Mult_acc_5: 0.2982  F1_score: 0.7131  MAE: 0.5146  Corr: 0.4583  Loss: 0.5172 \n",
            "100% 43/43 [00:07<00:00,  5.89it/s]\n",
            "MMSA - TRAIN-(bienc) [2/36/1] >> loss: 0.4728  Mult_acc_2: 0.7602  Mult_acc_3: 0.6257  Mult_acc_5: 0.3231  F1_score: 0.7571  MAE: 0.4664  Corr: 0.5477 \n",
            "100% 15/15 [00:02<00:00,  6.33it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7127  Mult_acc_3: 0.5724  Mult_acc_5: 0.3026  F1_score: 0.7146  MAE: 0.5152  Corr: 0.4497  Loss: 0.5274 \n",
            "100% 43/43 [00:08<00:00,  4.96it/s]\n",
            "MMSA - TRAIN-(bienc) [3/37/1] >> loss: 0.4643  Mult_acc_2: 0.7683  Mult_acc_3: 0.6330  Mult_acc_5: 0.3297  F1_score: 0.7688  MAE: 0.4615  Corr: 0.5604 \n",
            "100% 15/15 [00:02<00:00,  6.24it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.5899  Mult_acc_5: 0.2873  F1_score: 0.7257  MAE: 0.5117  Corr: 0.4584  Loss: 0.5129 \n",
            "100% 43/43 [00:08<00:00,  5.22it/s]\n",
            "MMSA - TRAIN-(bienc) [4/38/1] >> loss: 0.4604  Mult_acc_2: 0.7756  Mult_acc_3: 0.6221  Mult_acc_5: 0.3385  F1_score: 0.7718  MAE: 0.4508  Corr: 0.5754 \n",
            "100% 15/15 [00:02<00:00,  6.15it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.6974  Mult_acc_3: 0.5724  Mult_acc_5: 0.3004  F1_score: 0.7049  MAE: 0.5194  Corr: 0.4551  Loss: 0.5261 \n",
            "100% 43/43 [00:06<00:00,  6.26it/s]\n",
            "MMSA - TRAIN-(bienc) [5/39/1] >> loss: 0.4712  Mult_acc_2: 0.7727  Mult_acc_3: 0.6389  Mult_acc_5: 0.3472  F1_score: 0.7699  MAE: 0.4653  Corr: 0.5509 \n",
            "100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.5899  Mult_acc_5: 0.2982  F1_score: 0.7198  MAE: 0.5152  Corr: 0.4356  Loss: 0.5101 \n",
            "100% 43/43 [00:07<00:00,  5.51it/s]\n",
            "MMSA - TRAIN-(bienc) [6/40/1] >> loss: 0.4618  Mult_acc_2: 0.7463  Mult_acc_3: 0.6213  Mult_acc_5: 0.3289  F1_score: 0.7497  MAE: 0.4571  Corr: 0.5634 \n",
            "100% 15/15 [00:02<00:00,  5.97it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.5921  Mult_acc_5: 0.3180  F1_score: 0.7141  MAE: 0.5124  Corr: 0.4543  Loss: 0.5073 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:08<00:00,  5.03it/s]\n",
            "MMSA - TRAIN-(bienc) [1/41/1] >> loss: 0.4656  Mult_acc_2: 0.7632  Mult_acc_3: 0.6287  Mult_acc_5: 0.3341  F1_score: 0.7568  MAE: 0.4607  Corr: 0.5527 \n",
            "100% 15/15 [00:02<00:00,  6.34it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.5658  Mult_acc_5: 0.2961  F1_score: 0.7168  MAE: 0.5083  Corr: 0.4550  Loss: 0.5101 \n",
            "100% 43/43 [00:07<00:00,  5.87it/s]\n",
            "MMSA - TRAIN-(bienc) [2/42/1] >> loss: 0.4655  Mult_acc_2: 0.7690  Mult_acc_3: 0.6294  Mult_acc_5: 0.3377  F1_score: 0.7669  MAE: 0.4605  Corr: 0.5554 \n",
            "100% 15/15 [00:03<00:00,  4.47it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.5833  Mult_acc_5: 0.3092  F1_score: 0.7289  MAE: 0.5052  Corr: 0.4580  Loss: 0.5056 \n",
            "***********************************************Saving Model at checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 43/43 [00:06<00:00,  6.28it/s]\n",
            "MMSA - TRAIN-(bienc) [1/43/1] >> loss: 0.4562  Mult_acc_2: 0.7697  Mult_acc_3: 0.6411  Mult_acc_5: 0.3399  F1_score: 0.7682  MAE: 0.4501  Corr: 0.5745 \n",
            "100% 15/15 [00:02<00:00,  6.45it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.5768  Mult_acc_5: 0.3224  F1_score: 0.7217  MAE: 0.5099  Corr: 0.4556  Loss: 0.5333 \n",
            "100% 43/43 [00:08<00:00,  5.08it/s]\n",
            "MMSA - TRAIN-(bienc) [2/44/1] >> loss: 0.4515  Mult_acc_2: 0.7763  Mult_acc_3: 0.6367  Mult_acc_5: 0.3333  F1_score: 0.7744  MAE: 0.4440  Corr: 0.5928 \n",
            "100% 15/15 [00:02<00:00,  6.60it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.5724  Mult_acc_5: 0.2763  F1_score: 0.7156  MAE: 0.5085  Corr: 0.4554  Loss: 0.5137 \n",
            "100% 43/43 [00:08<00:00,  5.11it/s]\n",
            "MMSA - TRAIN-(bienc) [3/45/1] >> loss: 0.4671  Mult_acc_2: 0.7675  Mult_acc_3: 0.6148  Mult_acc_5: 0.3194  F1_score: 0.7623  MAE: 0.4628  Corr: 0.5525 \n",
            "100% 15/15 [00:02<00:00,  6.14it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7083  Mult_acc_3: 0.5724  Mult_acc_5: 0.2741  F1_score: 0.7148  MAE: 0.5262  Corr: 0.4433  Loss: 0.5367 \n",
            "100% 43/43 [00:07<00:00,  6.14it/s]\n",
            "MMSA - TRAIN-(bienc) [4/46/1] >> loss: 0.4464  Mult_acc_2: 0.7727  Mult_acc_3: 0.6469  Mult_acc_5: 0.3428  F1_score: 0.7713  MAE: 0.4419  Corr: 0.6017 \n",
            "100% 15/15 [00:03<00:00,  4.50it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.5855  Mult_acc_5: 0.3246  F1_score: 0.7277  MAE: 0.5130  Corr: 0.4549  Loss: 0.5198 \n",
            "100% 43/43 [00:07<00:00,  5.75it/s]\n",
            "MMSA - TRAIN-(bienc) [5/47/1] >> loss: 0.4455  Mult_acc_2: 0.7822  Mult_acc_3: 0.6404  Mult_acc_5: 0.3575  F1_score: 0.7779  MAE: 0.4376  Corr: 0.5984 \n",
            "100% 15/15 [00:02<00:00,  6.45it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.5789  Mult_acc_5: 0.2982  F1_score: 0.7143  MAE: 0.5133  Corr: 0.4362  Loss: 0.5216 \n",
            "100% 43/43 [00:08<00:00,  4.95it/s]\n",
            "MMSA - TRAIN-(bienc) [6/48/1] >> loss: 0.4498  Mult_acc_2: 0.7785  Mult_acc_3: 0.6382  Mult_acc_5: 0.3319  F1_score: 0.7748  MAE: 0.4447  Corr: 0.5886 \n",
            "100% 15/15 [00:02<00:00,  5.96it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7061  Mult_acc_3: 0.5680  Mult_acc_5: 0.3026  F1_score: 0.7055  MAE: 0.5064  Corr: 0.4520  Loss: 0.5131 \n",
            "100% 43/43 [00:08<00:00,  5.32it/s]\n",
            "MMSA - TRAIN-(bienc) [7/49/1] >> loss: 0.4613  Mult_acc_2: 0.7785  Mult_acc_3: 0.6243  Mult_acc_5: 0.3472  F1_score: 0.7757  MAE: 0.4495  Corr: 0.5759 \n",
            "100% 15/15 [00:02<00:00,  5.36it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7127  Mult_acc_3: 0.5702  Mult_acc_5: 0.2961  F1_score: 0.7078  MAE: 0.5159  Corr: 0.4403  Loss: 0.5279 \n",
            "100% 43/43 [00:07<00:00,  6.14it/s]\n",
            "MMSA - TRAIN-(bienc) [8/50/1] >> loss: 0.4525  Mult_acc_2: 0.7712  Mult_acc_3: 0.6499  Mult_acc_5: 0.3238  F1_score: 0.7697  MAE: 0.4480  Corr: 0.5870 \n",
            "100% 15/15 [00:02<00:00,  5.79it/s]\n",
            "MMSA - VAL-(bienc) >>  Mult_acc_2: 0.7215  Mult_acc_3: 0.5855  Mult_acc_5: 0.3114  F1_score: 0.6971  MAE: 0.5254  Corr: 0.4169  Loss: 0.5154 \n",
            "***************** Loading Model from checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "100% 15/15 [00:03<00:00,  4.32it/s]\n",
            "MMSA - TEST-(bienc) >>  Mult_acc_2: 0.7593  Mult_acc_3: 0.6127  Mult_acc_5: 0.2998  F1_score: 0.7556  MAE: 0.4816  Corr: 0.5214  Loss: 0.4884 \n",
            "MMSA - Result for seed 1990: {'Mult_acc_2': 0.7593, 'Mult_acc_3': 0.6127, 'Mult_acc_5': 0.2998, 'F1_score': 0.7556, 'MAE': 0.4816, 'Corr': 0.5214, 'Loss': 0.4884, 'seed': 1990}\n",
            "MMSA - Results saved to MMSA/results/bienc_sims/bienc-sims/normal/sims_avg.csv.\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "  -m bienc \\\n",
        "  -d sims \\\n",
        "  -g 0 \\\n",
        "  --exp-name bienc-sims \\\n",
        "  -c MMSA/config/regression/deepmlf/sims/bienc.json \\\n",
        "  --res-save-dir MMSA/results/bienc_sims \\\n",
        "  -n 2 \\\n",
        "  -s 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT-base Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no change     /content/miniconda3/condabin/conda\n",
            "no change     /content/miniconda3/bin/conda\n",
            "no change     /content/miniconda3/bin/conda-env\n",
            "no change     /content/miniconda3/bin/activate\n",
            "no change     /content/miniconda3/bin/deactivate\n",
            "no change     /content/miniconda3/etc/profile.d/conda.sh\n",
            "no change     /content/miniconda3/etc/fish/conf.d/conda.fish\n",
            "no change     /content/miniconda3/shell/condabin/Conda.psm1\n",
            "no change     /content/miniconda3/shell/condabin/conda-hook.ps1\n",
            "no change     /content/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh\n",
            "no change     /content/miniconda3/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "no change     /root/.zshrc\n",
            "no change     /root/.config/fish/config.fish\n",
            "no change     /root/.xonshrc\n",
            "no change     /root/.tcshrc\n",
            "No action taken.\n",
            "2025-11-08 15:20:05.513428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/deepmlf/experiments/regression/../../MMSA/utils/functions.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml\n",
            "MMSA - ======================================== Program Start ========================================\n",
            "MMSA - Running with args:\n",
            "MMSA - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/content/drive/MyDrive/MSA-Datasets/CH-SIMS/Processed/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 10, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': 'gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 100, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 20, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': 'checkpoints/bienc-sims/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0001, 'learning_rate_mmgpt': 0.0001, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}\n",
            "MMSA - Seeds: [1990, 1991]\n",
            "MMSA - ------------------------------ Running with seed 1990 [1/2] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(1368, 400, 33)\n",
            "(1368, 55, 709)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(456, 400, 33)\n",
            "(456, 55, 709)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(457, 400, 33)\n",
            "(457, 55, 709)\n",
            "Ongoing with num_workers=2\n",
            "ca list is: [5, 6, 7, 8, 9, 10, 11]\n",
            "initializing SoftPerm\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 0\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 2\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 3\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 4\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 5\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 6\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Parsing decoder block: 0\n",
            "Parsing decoder block: 1\n",
            "Parsing decoder block: 2\n",
            "Parsing decoder block: 3\n",
            "Parsing decoder block: 4\n",
            "Parsing decoder block: 5\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 6\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 7\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 8\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 9\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 10\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 11\n",
            "COpying---------------------------------\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "----------------->>> Pretrained AudioVisual Encoder <<<<<----------------\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "----------------------- Loading AV encoder from checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "Copied param embed_positions_a._float_tensor\n",
            "Copied param embed_positions_v._float_tensor\n",
            "Copied param BN_a.weight\n",
            "Copied param BN_a.bias\n",
            "Copied param BN_a.running_mean\n",
            "Copied param BN_a.running_var\n",
            "Copied param BN_a.num_batches_tracked\n",
            "Copied param BN_v.weight\n",
            "Copied param BN_v.bias\n",
            "Copied param BN_v.running_mean\n",
            "Copied param BN_v.running_var\n",
            "Copied param BN_v.num_batches_tracked\n",
            "Copied param proj_a.weight\n",
            "Copied param proj_v.weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.0.linear1.weight\n",
            "Copied param enc_a.layers.0.linear1.bias\n",
            "Copied param enc_a.layers.0.linear2.weight\n",
            "Copied param enc_a.layers.0.linear2.bias\n",
            "Copied param enc_a.layers.0.norm1.weight\n",
            "Copied param enc_a.layers.0.norm1.bias\n",
            "Copied param enc_a.layers.0.norm2.weight\n",
            "Copied param enc_a.layers.0.norm2.bias\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.1.linear1.weight\n",
            "Copied param enc_a.layers.1.linear1.bias\n",
            "Copied param enc_a.layers.1.linear2.weight\n",
            "Copied param enc_a.layers.1.linear2.bias\n",
            "Copied param enc_a.layers.1.norm1.weight\n",
            "Copied param enc_a.layers.1.norm1.bias\n",
            "Copied param enc_a.layers.1.norm2.weight\n",
            "Copied param enc_a.layers.1.norm2.bias\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.2.linear1.weight\n",
            "Copied param enc_a.layers.2.linear1.bias\n",
            "Copied param enc_a.layers.2.linear2.weight\n",
            "Copied param enc_a.layers.2.linear2.bias\n",
            "Copied param enc_a.layers.2.norm1.weight\n",
            "Copied param enc_a.layers.2.norm1.bias\n",
            "Copied param enc_a.layers.2.norm2.weight\n",
            "Copied param enc_a.layers.2.norm2.bias\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.0.linear1.weight\n",
            "Copied param enc_v.layers.0.linear1.bias\n",
            "Copied param enc_v.layers.0.linear2.weight\n",
            "Copied param enc_v.layers.0.linear2.bias\n",
            "Copied param enc_v.layers.0.norm1.weight\n",
            "Copied param enc_v.layers.0.norm1.bias\n",
            "Copied param enc_v.layers.0.norm2.weight\n",
            "Copied param enc_v.layers.0.norm2.bias\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.1.linear1.weight\n",
            "Copied param enc_v.layers.1.linear1.bias\n",
            "Copied param enc_v.layers.1.linear2.weight\n",
            "Copied param enc_v.layers.1.linear2.bias\n",
            "Copied param enc_v.layers.1.norm1.weight\n",
            "Copied param enc_v.layers.1.norm1.bias\n",
            "Copied param enc_v.layers.1.norm2.weight\n",
            "Copied param enc_v.layers.1.norm2.bias\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.2.linear1.weight\n",
            "Copied param enc_v.layers.2.linear1.bias\n",
            "Copied param enc_v.layers.2.linear2.weight\n",
            "Copied param enc_v.layers.2.linear2.bias\n",
            "Copied param enc_v.layers.2.norm1.weight\n",
            "Copied param enc_v.layers.2.norm1.bias\n",
            "Copied param enc_v.layers.2.norm2.weight\n",
            "Copied param enc_v.layers.2.norm2.bias\n",
            "Copied param fusion.weight\n",
            "Copied param fusion.bias\n",
            "Copied param clf.weight\n",
            "Copied param clf.bias\n",
            "------------------ Adding LNorm ------------------------\n",
            "MMSA - The model has 143937589 trainable parameters\n",
            "ongoing with msalm\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "False\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "5.ca_layer.alpha_1\n",
            "5.ca_layer.alpha_2\n",
            "5.ca_layer.ln_1.weight\n",
            "5.ca_layer.ln_1.bias\n",
            "5.ca_layer.ln_2.weight\n",
            "5.ca_layer.ln_2.bias\n",
            "5.ca_layer.attn.W_q.weight\n",
            "5.ca_layer.attn.W_kv.weight\n",
            "5.ca_layer.attn.W_o.weight\n",
            "5.ca_layer.mlp.c_fc.weight\n",
            "5.ca_layer.mlp.c_fc.bias\n",
            "5.ca_layer.mlp.c_proj.weight\n",
            "5.ca_layer.mlp.c_proj.bias\n",
            "6.ca_layer.alpha_1\n",
            "6.ca_layer.alpha_2\n",
            "6.ca_layer.ln_1.weight\n",
            "6.ca_layer.ln_1.bias\n",
            "6.ca_layer.ln_2.weight\n",
            "6.ca_layer.ln_2.bias\n",
            "6.ca_layer.attn.W_q.weight\n",
            "6.ca_layer.attn.W_kv.weight\n",
            "6.ca_layer.attn.W_o.weight\n",
            "6.ca_layer.mlp.c_fc.weight\n",
            "6.ca_layer.mlp.c_fc.bias\n",
            "6.ca_layer.mlp.c_proj.weight\n",
            "6.ca_layer.mlp.c_proj.bias\n",
            "7.ca_layer.alpha_1\n",
            "7.ca_layer.alpha_2\n",
            "7.ca_layer.ln_1.weight\n",
            "7.ca_layer.ln_1.bias\n",
            "7.ca_layer.ln_2.weight\n",
            "7.ca_layer.ln_2.bias\n",
            "7.ca_layer.attn.W_q.weight\n",
            "7.ca_layer.attn.W_kv.weight\n",
            "7.ca_layer.attn.W_o.weight\n",
            "7.ca_layer.mlp.c_fc.weight\n",
            "7.ca_layer.mlp.c_fc.bias\n",
            "7.ca_layer.mlp.c_proj.weight\n",
            "7.ca_layer.mlp.c_proj.bias\n",
            "8.ca_layer.alpha_1\n",
            "8.ca_layer.alpha_2\n",
            "8.ca_layer.ln_1.weight\n",
            "8.ca_layer.ln_1.bias\n",
            "8.ca_layer.ln_2.weight\n",
            "8.ca_layer.ln_2.bias\n",
            "8.ca_layer.attn.W_q.weight\n",
            "8.ca_layer.attn.W_kv.weight\n",
            "8.ca_layer.attn.W_o.weight\n",
            "8.ca_layer.mlp.c_fc.weight\n",
            "8.ca_layer.mlp.c_fc.bias\n",
            "8.ca_layer.mlp.c_proj.weight\n",
            "8.ca_layer.mlp.c_proj.bias\n",
            "9.ca_layer.alpha_1\n",
            "9.ca_layer.alpha_2\n",
            "9.ca_layer.ln_1.weight\n",
            "9.ca_layer.ln_1.bias\n",
            "9.ca_layer.ln_2.weight\n",
            "9.ca_layer.ln_2.bias\n",
            "9.ca_layer.attn.W_q.weight\n",
            "9.ca_layer.attn.W_kv.weight\n",
            "9.ca_layer.attn.W_o.weight\n",
            "9.ca_layer.mlp.c_fc.weight\n",
            "9.ca_layer.mlp.c_fc.bias\n",
            "9.ca_layer.mlp.c_proj.weight\n",
            "9.ca_layer.mlp.c_proj.bias\n",
            "10.ca_layer.alpha_1\n",
            "10.ca_layer.alpha_2\n",
            "10.ca_layer.ln_1.weight\n",
            "10.ca_layer.ln_1.bias\n",
            "10.ca_layer.ln_2.weight\n",
            "10.ca_layer.ln_2.bias\n",
            "10.ca_layer.attn.W_q.weight\n",
            "10.ca_layer.attn.W_kv.weight\n",
            "10.ca_layer.attn.W_o.weight\n",
            "10.ca_layer.mlp.c_fc.weight\n",
            "10.ca_layer.mlp.c_fc.bias\n",
            "10.ca_layer.mlp.c_proj.weight\n",
            "10.ca_layer.mlp.c_proj.bias\n",
            "11.ca_layer.alpha_1\n",
            "11.ca_layer.alpha_2\n",
            "11.ca_layer.ln_1.weight\n",
            "11.ca_layer.ln_1.bias\n",
            "11.ca_layer.ln_2.weight\n",
            "11.ca_layer.ln_2.bias\n",
            "11.ca_layer.attn.W_q.weight\n",
            "11.ca_layer.attn.W_kv.weight\n",
            "11.ca_layer.attn.W_o.weight\n",
            "11.ca_layer.mlp.c_fc.weight\n",
            "11.ca_layer.mlp.c_fc.bias\n",
            "11.ca_layer.mlp.c_proj.weight\n",
            "11.ca_layer.mlp.c_proj.bias\n",
            "0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Model.av_encoder.BN_a.weight\n",
            "Model.av_encoder.BN_a.bias\n",
            "Model.av_encoder.BN_v.weight\n",
            "Model.av_encoder.BN_v.bias\n",
            "Model.av_encoder.proj_a.weight\n",
            "Model.av_encoder.proj_v.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm2.bias\n",
            "Model.av_encoder.fusion.weight\n",
            "Model.av_encoder.fusion.bias\n",
            "Model.av_encoder.clf.weight\n",
            "Model.av_encoder.clf.bias\n",
            "Model.LN.weight\n",
            "Model.LN.bias\n",
            "The total number of trainable parameters is 41.87 M\n",
            "Model.lang_encoder.transformer.wte.0.embedding.weight\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wpe.0.positional.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.ln_f.weight\n",
            "Model.lang_encoder.transformer.ln_f.bias\n",
            "Model.W_task.0.weight\n",
            "Using grad with decay in Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Using grad with no decay in Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Using grad with decay in Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Using grad with no decay in Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Using grad with decay in Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Using grad with no decay in Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Using grad with decay in Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Using grad with no decay in Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Using grad with decay in Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Using grad with no decay in Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Using grad with decay in Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Using grad with no decay in Model.W_av.bias\n",
            "Model.LN.weight\n",
            "Using grad with decay in Model.LN.weight\n",
            "Model.LN.bias\n",
            "Using grad with no decay in Model.LN.bias\n",
            "Will be using warmup for 5 steps\n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [1/1/1] >> loss: 0.6618  Mult_acc_2: 0.6447  Mult_acc_3: 0.4839  Mult_acc_5: 0.2339  F1_score: 0.5927  MAE: 0.6516  Corr: 0.0104  clm loss: 7.7582 total loss: 11.0544 bn loss: 1.0229 av loss: 0.6478 text loss: 0.9637\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([4.3296e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-6.6979e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([7.7549e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-8.0292e-05], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.83it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.6930  Mult_acc_3: 0.5285  Mult_acc_5: 0.2544  F1_score: 0.5977  MAE: 0.6583  Corr: 0.0320  Loss: 0.6660 \n",
            "100% 43/43 [00:21<00:00,  2.05it/s]\n",
            "MMSA - TRAIN-(msalm) [1/2/1] >> loss: 0.6084  Mult_acc_2: 0.6864  Mult_acc_3: 0.5154  Mult_acc_5: 0.2405  F1_score: 0.6414  MAE: 0.5985  Corr: 0.2154  clm loss: 7.0519 total loss: 10.0505 bn loss: 0.8561 av loss: 0.6259 text loss: 0.9082\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-9.1490e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.92it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7061  Mult_acc_3: 0.5439  Mult_acc_5: 0.2763  F1_score: 0.6342  MAE: 0.5984  Corr: 0.2712  Loss: 0.6200 \n",
            "100% 43/43 [00:21<00:00,  2.00it/s]\n",
            "MMSA - TRAIN-(msalm) [1/3/1] >> loss: 0.5554  Mult_acc_2: 0.6974  Mult_acc_3: 0.5716  Mult_acc_5: 0.3194  F1_score: 0.6755  MAE: 0.5308  Corr: 0.3887  clm loss: 6.3172 total loss: 9.0146 bn loss: 0.7163 av loss: 0.6085 text loss: 0.8172\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0008], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0008], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0008], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([8.0591e-05], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.95it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.5548  Mult_acc_5: 0.3026  F1_score: 0.6799  MAE: 0.5568  Corr: 0.3861  Loss: 0.5790 \n",
            "100% 43/43 [00:22<00:00,  1.91it/s]\n",
            "MMSA - TRAIN-(msalm) [1/4/1] >> loss: 0.541  Mult_acc_2: 0.7332  Mult_acc_3: 0.5892  Mult_acc_5: 0.3275  F1_score: 0.7230  MAE: 0.5087  Corr: 0.4588  clm loss: 5.7095 total loss: 8.2205 bn loss: 0.6308 av loss: 0.5901 text loss: 0.749\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0008], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([4.1500e-05], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.94it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.5768  Mult_acc_5: 0.3268  F1_score: 0.7093  MAE: 0.5247  Corr: 0.4370  Loss: 0.5536 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [1/5/1] >> loss: 0.5143  Mult_acc_2: 0.7544  Mult_acc_3: 0.6111  Mult_acc_5: 0.3348  F1_score: 0.7462  MAE: 0.4790  Corr: 0.5261  clm loss: 5.1959 total loss: 7.5625 bn loss: 0.5898 av loss: 0.5788 text loss: 0.6837\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0015], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.97it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.5899  Mult_acc_5: 0.3399  F1_score: 0.7150  MAE: 0.5136  Corr: 0.4693  Loss: 0.5464 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [1/6/1] >> loss: 0.5046  Mult_acc_2: 0.7463  Mult_acc_3: 0.6162  Mult_acc_5: 0.3735  F1_score: 0.7387  MAE: 0.4683  Corr: 0.5443  clm loss: 4.7468 total loss: 7.0186 bn loss: 0.5653 av loss: 0.562 text loss: 0.6399\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0024], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-8.0502e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.27it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.5965  Mult_acc_5: 0.3640  F1_score: 0.7389  MAE: 0.4912  Corr: 0.4978  Loss: 0.5163 \n",
            "100% 43/43 [00:22<00:00,  1.91it/s]\n",
            "MMSA - TRAIN-(msalm) [1/7/1] >> loss: 0.5056  Mult_acc_2: 0.7639  Mult_acc_3: 0.6133  Mult_acc_5: 0.3458  F1_score: 0.7526  MAE: 0.4775  Corr: 0.5258  clm loss: 4.4695 total loss: 6.6869 bn loss: 0.5434 av loss: 0.5481 text loss: 0.6204\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0015], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0022], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.82it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.5965  Mult_acc_5: 0.3465  F1_score: 0.7266  MAE: 0.4948  Corr: 0.4806  Loss: 0.5218 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [2/8/1] >> loss: 0.4863  Mult_acc_2: 0.7566  Mult_acc_3: 0.6228  Mult_acc_5: 0.3355  F1_score: 0.7436  MAE: 0.4719  Corr: 0.5400  clm loss: 4.1892 total loss: 6.3542 bn loss: 0.532 av loss: 0.5339 text loss: 0.6129\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0031], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0022], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.06it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.5921  Mult_acc_5: 0.3618  F1_score: 0.7299  MAE: 0.4990  Corr: 0.4774  Loss: 0.5233 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [3/9/1] >> loss: 0.4658  Mult_acc_2: 0.7763  Mult_acc_3: 0.6250  Mult_acc_5: 0.3480  F1_score: 0.7695  MAE: 0.4480  Corr: 0.5926  clm loss: 3.9165 total loss: 6.005 bn loss: 0.5134 av loss: 0.5097 text loss: 0.5996\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-1.1154e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-1.9807e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-3.5355e-05], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0034], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([2.6200e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.89it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.5987  Mult_acc_5: 0.3487  F1_score: 0.7344  MAE: 0.5049  Corr: 0.4800  Loss: 0.5490 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [4/10/1] >> loss: 0.4634  Mult_acc_2: 0.7829  Mult_acc_3: 0.6462  Mult_acc_5: 0.3823  F1_score: 0.7772  MAE: 0.4425  Corr: 0.5808  clm loss: 3.7917 total loss: 5.8424 bn loss: 0.5037 av loss: 0.4979 text loss: 0.5857\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0036], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([9.2214e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.32it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6031  Mult_acc_5: 0.3443  F1_score: 0.7283  MAE: 0.4996  Corr: 0.4862  Loss: 0.5434 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [5/11/1] >> loss: 0.4676  Mult_acc_2: 0.7719  Mult_acc_3: 0.6382  Mult_acc_5: 0.3618  F1_score: 0.7670  MAE: 0.4505  Corr: 0.5870  clm loss: 3.6388 total loss: 5.6683 bn loss: 0.5022 av loss: 0.4912 text loss: 0.5685\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0037], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-7.0895e-05], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.79it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.6096  Mult_acc_5: 0.3465  F1_score: 0.7209  MAE: 0.4886  Corr: 0.4928  Loss: 0.5259 \n",
            "100% 43/43 [00:23<00:00,  1.87it/s]\n",
            "MMSA - TRAIN-(msalm) [6/12/1] >> loss: 0.4411  Mult_acc_2: 0.7792  Mult_acc_3: 0.6425  Mult_acc_5: 0.3852  F1_score: 0.7735  MAE: 0.4255  Corr: 0.6130  clm loss: 3.5576 total loss: 5.5455 bn loss: 0.4965 av loss: 0.4811 text loss: 0.5694\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0039], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([9.2269e-05], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0022], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.88it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.6996  Mult_acc_3: 0.6162  Mult_acc_5: 0.3465  F1_score: 0.7054  MAE: 0.4868  Corr: 0.5033  Loss: 0.5220 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [7/13/1] >> loss: 0.4411  Mult_acc_2: 0.7829  Mult_acc_3: 0.6499  Mult_acc_5: 0.3808  F1_score: 0.7789  MAE: 0.4278  Corr: 0.6112  clm loss: 3.4053 total loss: 5.3637 bn loss: 0.4856 av loss: 0.4699 text loss: 0.5618\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0040], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0023], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.78it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7215  Mult_acc_3: 0.6206  Mult_acc_5: 0.3553  F1_score: 0.7257  MAE: 0.4779  Corr: 0.5125  Loss: 0.5127 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [1/14/1] >> loss: 0.4343  Mult_acc_2: 0.7822  Mult_acc_3: 0.6594  Mult_acc_5: 0.3684  F1_score: 0.7786  MAE: 0.4240  Corr: 0.6240  clm loss: 3.3482 total loss: 5.2892 bn loss: 0.4945 av loss: 0.4717 text loss: 0.5405\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0041], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0026], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.78it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.6206  Mult_acc_5: 0.3640  F1_score: 0.7296  MAE: 0.4700  Corr: 0.5253  Loss: 0.5167 \n",
            "100% 43/43 [00:23<00:00,  1.85it/s]\n",
            "MMSA - TRAIN-(msalm) [2/15/1] >> loss: 0.4281  Mult_acc_2: 0.7756  Mult_acc_3: 0.6667  Mult_acc_5: 0.3947  F1_score: 0.7716  MAE: 0.4095  Corr: 0.6405  clm loss: 3.3415 total loss: 5.2725 bn loss: 0.4864 av loss: 0.4674 text loss: 0.5491\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0043], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.07it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6382  Mult_acc_5: 0.3684  F1_score: 0.7432  MAE: 0.4596  Corr: 0.5414  Loss: 0.5064 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [1/16/1] >> loss: 0.428  Mult_acc_2: 0.7792  Mult_acc_3: 0.6491  Mult_acc_5: 0.3808  F1_score: 0.7753  MAE: 0.4096  Corr: 0.6440  clm loss: 3.3021 total loss: 5.2053 bn loss: 0.4826 av loss: 0.4558 text loss: 0.5369\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0044], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0031], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.81it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6250  Mult_acc_5: 0.3553  F1_score: 0.7350  MAE: 0.4518  Corr: 0.5557  Loss: 0.4658 \n",
            "100% 43/43 [00:25<00:00,  1.70it/s]\n",
            "MMSA - TRAIN-(msalm) [1/17/1] >> loss: 0.4261  Mult_acc_2: 0.7749  Mult_acc_3: 0.6477  Mult_acc_5: 0.3684  F1_score: 0.7729  MAE: 0.4119  Corr: 0.6462  clm loss: 3.2656 total loss: 5.1624 bn loss: 0.481 av loss: 0.4649 text loss: 0.5248\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0033], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.85it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.6316  Mult_acc_5: 0.3706  F1_score: 0.7276  MAE: 0.4513  Corr: 0.5619  Loss: 0.4863 \n",
            "100% 43/43 [00:21<00:00,  1.96it/s]\n",
            "MMSA - TRAIN-(msalm) [2/18/1] >> loss: 0.4083  Mult_acc_2: 0.7924  Mult_acc_3: 0.6564  Mult_acc_5: 0.4086  F1_score: 0.7895  MAE: 0.3961  Corr: 0.6653  clm loss: 3.2942 total loss: 5.1555 bn loss: 0.4673 av loss: 0.4583 text loss: 0.5274\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0048], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0036], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.74it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.6404  Mult_acc_5: 0.3925  F1_score: 0.7374  MAE: 0.4461  Corr: 0.5707  Loss: 0.4983 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [3/19/1] >> loss: 0.4095  Mult_acc_2: 0.7939  Mult_acc_3: 0.6776  Mult_acc_5: 0.4101  F1_score: 0.7943  MAE: 0.3927  Corr: 0.6751  clm loss: 3.2989 total loss: 5.15 bn loss: 0.4631 av loss: 0.4522 text loss: 0.5263\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0049], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.91it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.6316  Mult_acc_5: 0.3640  F1_score: 0.7282  MAE: 0.4310  Corr: 0.5905  Loss: 0.4683 \n",
            "100% 43/43 [00:23<00:00,  1.80it/s]\n",
            "MMSA - TRAIN-(msalm) [4/20/1] >> loss: 0.4107  Mult_acc_2: 0.7895  Mult_acc_3: 0.6718  Mult_acc_5: 0.3933  F1_score: 0.7890  MAE: 0.3965  Corr: 0.6714  clm loss: 3.2521 total loss: 5.096 bn loss: 0.4598 av loss: 0.4537 text loss: 0.5196\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0050], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.50it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.6382  Mult_acc_5: 0.3860  F1_score: 0.7402  MAE: 0.4283  Corr: 0.5948  Loss: 0.4547 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [1/21/1] >> loss: 0.3981  Mult_acc_2: 0.7844  Mult_acc_3: 0.6674  Mult_acc_5: 0.4094  F1_score: 0.7846  MAE: 0.3860  Corr: 0.6830  clm loss: 3.2114 total loss: 5.0309 bn loss: 0.4588 av loss: 0.458 text loss: 0.5046\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0051], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0044], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.23it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7215  Mult_acc_3: 0.6184  Mult_acc_5: 0.3618  F1_score: 0.7248  MAE: 0.4307  Corr: 0.5927  Loss: 0.4542 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [1/22/1] >> loss: 0.3959  Mult_acc_2: 0.8026  Mult_acc_3: 0.6791  Mult_acc_5: 0.3918  F1_score: 0.8034  MAE: 0.3810  Corr: 0.6995  clm loss: 3.1871 total loss: 4.971 bn loss: 0.4497 av loss: 0.4505 text loss: 0.4878\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0051], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0045], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.94it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.6184  Mult_acc_5: 0.3728  F1_score: 0.7382  MAE: 0.4265  Corr: 0.5953  Loss: 0.4541 \n",
            "100% 43/43 [00:22<00:00,  1.89it/s]\n",
            "MMSA - TRAIN-(msalm) [1/23/1] >> loss: 0.3892  Mult_acc_2: 0.8092  Mult_acc_3: 0.6901  Mult_acc_5: 0.4313  F1_score: 0.8093  MAE: 0.3689  Corr: 0.7116  clm loss: 3.1667 total loss: 4.9507 bn loss: 0.4523 av loss: 0.4488 text loss: 0.4938\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0050], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0047], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.77it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7522  Mult_acc_3: 0.6360  Mult_acc_5: 0.3947  F1_score: 0.7519  MAE: 0.4246  Corr: 0.6042  Loss: 0.4590 \n",
            "100% 43/43 [00:24<00:00,  1.77it/s]\n",
            "MMSA - TRAIN-(msalm) [2/24/1] >> loss: 0.3889  Mult_acc_2: 0.7909  Mult_acc_3: 0.6681  Mult_acc_5: 0.3991  F1_score: 0.7930  MAE: 0.3746  Corr: 0.7082  clm loss: 3.1985 total loss: 4.9596 bn loss: 0.4447 av loss: 0.4439 text loss: 0.4836\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0050], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0049], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.69it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7456  Mult_acc_3: 0.6272  Mult_acc_5: 0.3860  F1_score: 0.7466  MAE: 0.4165  Corr: 0.6145  Loss: 0.4424 \n",
            "100% 43/43 [00:21<00:00,  1.98it/s]\n",
            "MMSA - TRAIN-(msalm) [1/25/1] >> loss: 0.3824  Mult_acc_2: 0.8004  Mult_acc_3: 0.6915  Mult_acc_5: 0.4181  F1_score: 0.8020  MAE: 0.3704  Corr: 0.7162  clm loss: 3.1589 total loss: 4.9178 bn loss: 0.439 av loss: 0.4521 text loss: 0.4854\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0045], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0049], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.87it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.6360  Mult_acc_5: 0.3860  F1_score: 0.7550  MAE: 0.4159  Corr: 0.6195  Loss: 0.4525 \n",
            "100% 43/43 [00:24<00:00,  1.78it/s]\n",
            "MMSA - TRAIN-(msalm) [2/26/1] >> loss: 0.3823  Mult_acc_2: 0.7953  Mult_acc_3: 0.7039  Mult_acc_5: 0.4167  F1_score: 0.7979  MAE: 0.3697  Corr: 0.7152  clm loss: 3.1156 total loss: 4.8544 bn loss: 0.4373 av loss: 0.4534 text loss: 0.4657\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0047], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0017], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0051], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.06it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7588  Mult_acc_3: 0.6601  Mult_acc_5: 0.4035  F1_score: 0.7648  MAE: 0.4081  Corr: 0.6353  Loss: 0.4337 \n",
            "100% 43/43 [00:23<00:00,  1.83it/s]\n",
            "MMSA - TRAIN-(msalm) [1/27/1] >> loss: 0.3568  Mult_acc_2: 0.8136  Mult_acc_3: 0.7039  Mult_acc_5: 0.4583  F1_score: 0.8144  MAE: 0.3462  Corr: 0.7525  clm loss: 3.1281 total loss: 4.8042 bn loss: 0.4202 av loss: 0.4435 text loss: 0.4555\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0045], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0053], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.55it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.6447  Mult_acc_5: 0.3794  F1_score: 0.7480  MAE: 0.4164  Corr: 0.6379  Loss: 0.4427 \n",
            "100% 43/43 [00:23<00:00,  1.81it/s]\n",
            "MMSA - TRAIN-(msalm) [2/28/1] >> loss: 0.376  Mult_acc_2: 0.8129  Mult_acc_3: 0.6937  Mult_acc_5: 0.4298  F1_score: 0.8136  MAE: 0.3593  Corr: 0.7317  clm loss: 3.0994 total loss: 4.8216 bn loss: 0.4261 av loss: 0.4483 text loss: 0.4717\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0043], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0054], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.90it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7522  Mult_acc_3: 0.6645  Mult_acc_5: 0.4057  F1_score: 0.7583  MAE: 0.4060  Corr: 0.6360  Loss: 0.4438 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [3/29/1] >> loss: 0.3694  Mult_acc_2: 0.8129  Mult_acc_3: 0.7091  Mult_acc_5: 0.4591  F1_score: 0.8145  MAE: 0.3482  Corr: 0.7434  clm loss: 3.0993 total loss: 4.8037 bn loss: 0.4184 av loss: 0.4449 text loss: 0.4716\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0056], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0040], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0054], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.81it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6645  Mult_acc_5: 0.4232  F1_score: 0.7515  MAE: 0.4084  Corr: 0.6368  Loss: 0.4391 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [4/30/1] >> loss: 0.3567  Mult_acc_2: 0.8136  Mult_acc_3: 0.7098  Mult_acc_5: 0.4554  F1_score: 0.8152  MAE: 0.3436  Corr: 0.7460  clm loss: 3.1165 total loss: 4.7858 bn loss: 0.4058 av loss: 0.4418 text loss: 0.4651\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0069], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0045], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0038], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0054], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.84it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7456  Mult_acc_3: 0.6557  Mult_acc_5: 0.3838  F1_score: 0.7537  MAE: 0.4048  Corr: 0.6452  Loss: 0.4143 \n",
            "100% 43/43 [00:23<00:00,  1.82it/s]\n",
            "MMSA - TRAIN-(msalm) [1/31/1] >> loss: 0.3612  Mult_acc_2: 0.8194  Mult_acc_3: 0.7156  Mult_acc_5: 0.4415  F1_score: 0.8212  MAE: 0.3522  Corr: 0.7502  clm loss: 3.1012 total loss: 4.7945 bn loss: 0.4122 av loss: 0.4548 text loss: 0.465\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0053], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.60it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7522  Mult_acc_3: 0.6557  Mult_acc_5: 0.3816  F1_score: 0.7604  MAE: 0.4074  Corr: 0.6448  Loss: 0.4335 \n",
            "100% 43/43 [00:22<00:00,  1.88it/s]\n",
            "MMSA - TRAIN-(msalm) [2/32/1] >> loss: 0.3462  Mult_acc_2: 0.8092  Mult_acc_3: 0.7113  Mult_acc_5: 0.4678  F1_score: 0.8112  MAE: 0.3273  Corr: 0.7700  clm loss: 3.0654 total loss: 4.6943 bn loss: 0.3943 av loss: 0.4405 text loss: 0.4479\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0069], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0063], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0032], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0053], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.99it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7478  Mult_acc_3: 0.6513  Mult_acc_5: 0.3969  F1_score: 0.7567  MAE: 0.4105  Corr: 0.6420  Loss: 0.4444 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [3/33/1] >> loss: 0.3541  Mult_acc_2: 0.8129  Mult_acc_3: 0.7171  Mult_acc_5: 0.4781  F1_score: 0.8160  MAE: 0.3354  Corr: 0.7483  clm loss: 3.0757 total loss: 4.7204 bn loss: 0.4034 av loss: 0.4451 text loss: 0.4422\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0072], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0080], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0029], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0053], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.63it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7478  Mult_acc_3: 0.6513  Mult_acc_5: 0.3882  F1_score: 0.7559  MAE: 0.4036  Corr: 0.6487  Loss: 0.4324 \n",
            "100% 43/43 [00:22<00:00,  1.89it/s]\n",
            "MMSA - TRAIN-(msalm) [4/34/1] >> loss: 0.3336  Mult_acc_2: 0.8246  Mult_acc_3: 0.7354  Mult_acc_5: 0.4795  F1_score: 0.8274  MAE: 0.3180  Corr: 0.7907  clm loss: 3.0149 total loss: 4.622 bn loss: 0.3943 av loss: 0.4403 text loss: 0.4388\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0082], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0026], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0052], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0060], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.88it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7478  Mult_acc_3: 0.6404  Mult_acc_5: 0.3904  F1_score: 0.7561  MAE: 0.4056  Corr: 0.6496  Loss: 0.4128 \n",
            "100% 43/43 [00:22<00:00,  1.91it/s]\n",
            "MMSA - TRAIN-(msalm) [1/35/1] >> loss: 0.3365  Mult_acc_2: 0.8216  Mult_acc_3: 0.7259  Mult_acc_5: 0.4591  F1_score: 0.8238  MAE: 0.3277  Corr: 0.7798  clm loss: 3.0048 total loss: 4.5925 bn loss: 0.381 av loss: 0.4447 text loss: 0.4255\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0079], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0063], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0023], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0063], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.65it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6513  Mult_acc_5: 0.3925  F1_score: 0.7517  MAE: 0.4031  Corr: 0.6501  Loss: 0.4174 \n",
            "100% 43/43 [00:23<00:00,  1.85it/s]\n",
            "MMSA - TRAIN-(msalm) [2/36/1] >> loss: 0.3426  Mult_acc_2: 0.8319  Mult_acc_3: 0.7200  Mult_acc_5: 0.4656  F1_score: 0.8332  MAE: 0.3312  Corr: 0.7754  clm loss: 3.0023 total loss: 4.599 bn loss: 0.3812 av loss: 0.4483 text loss: 0.4247\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0082], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0059], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0071], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0048], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.37it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7588  Mult_acc_3: 0.6601  Mult_acc_5: 0.4101  F1_score: 0.7653  MAE: 0.3950  Corr: 0.6543  Loss: 0.4074 \n",
            "100% 43/43 [00:22<00:00,  1.95it/s]\n",
            "MMSA - TRAIN-(msalm) [1/37/1] >> loss: 0.3307  Mult_acc_2: 0.8333  Mult_acc_3: 0.7325  Mult_acc_5: 0.5015  F1_score: 0.8356  MAE: 0.3174  Corr: 0.7829  clm loss: 3.0215 total loss: 4.6028 bn loss: 0.3776 av loss: 0.4403 text loss: 0.4327\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0093], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0093], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0046], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.68it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7588  Mult_acc_3: 0.6601  Mult_acc_5: 0.4101  F1_score: 0.7658  MAE: 0.3963  Corr: 0.6549  Loss: 0.4148 \n",
            "100% 43/43 [00:24<00:00,  1.76it/s]\n",
            "MMSA - TRAIN-(msalm) [2/38/1] >> loss: 0.327  Mult_acc_2: 0.8319  Mult_acc_3: 0.7346  Mult_acc_5: 0.4993  F1_score: 0.8344  MAE: 0.3124  Corr: 0.7881  clm loss: 2.9931 total loss: 4.5744 bn loss: 0.381 av loss: 0.4367 text loss: 0.4365\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0089], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0096], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0071], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0097], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0071], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0045], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.96it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6579  Mult_acc_5: 0.3969  F1_score: 0.7631  MAE: 0.4022  Corr: 0.6544  Loss: 0.4177 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [3/39/1] >> loss: 0.3425  Mult_acc_2: 0.8194  Mult_acc_3: 0.7251  Mult_acc_5: 0.4671  F1_score: 0.8217  MAE: 0.3291  Corr: 0.7806  clm loss: 3.0103 total loss: 4.6163 bn loss: 0.3846 av loss: 0.4426 text loss: 0.4363\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0092], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0099], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0100], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0072], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.81it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7675  Mult_acc_3: 0.6645  Mult_acc_5: 0.4079  F1_score: 0.7728  MAE: 0.3930  Corr: 0.6595  Loss: 0.3978 \n",
            "100% 43/43 [00:22<00:00,  1.95it/s]\n",
            "MMSA - TRAIN-(msalm) [1/40/1] >> loss: 0.3283  Mult_acc_2: 0.8253  Mult_acc_3: 0.7361  Mult_acc_5: 0.4737  F1_score: 0.8288  MAE: 0.3171  Corr: 0.7899  clm loss: 2.9901 total loss: 4.5575 bn loss: 0.3794 av loss: 0.4424 text loss: 0.4174\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0095], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0101], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0103], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0041], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.28it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.6513  Mult_acc_5: 0.4101  F1_score: 0.7580  MAE: 0.3957  Corr: 0.6588  Loss: 0.4242 \n",
            "100% 43/43 [00:23<00:00,  1.81it/s]\n",
            "MMSA - TRAIN-(msalm) [2/41/1] >> loss: 0.3167  Mult_acc_2: 0.8377  Mult_acc_3: 0.7368  Mult_acc_5: 0.4993  F1_score: 0.8401  MAE: 0.3029  Corr: 0.8037  clm loss: 2.9738 total loss: 4.5201 bn loss: 0.3688 av loss: 0.4404 text loss: 0.4205\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0098], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0104], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0079], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0107], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0072], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0056], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0045], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.70it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.6404  Mult_acc_5: 0.3794  F1_score: 0.7359  MAE: 0.4052  Corr: 0.6556  Loss: 0.4159 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [3/42/1] >> loss: 0.32  Mult_acc_2: 0.8341  Mult_acc_3: 0.7412  Mult_acc_5: 0.4993  F1_score: 0.8369  MAE: 0.3080  Corr: 0.7968  clm loss: 2.9511 total loss: 4.4752 bn loss: 0.3594 av loss: 0.4383 text loss: 0.4065\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0100], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0108], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0082], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0110], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0084], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0085], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0038], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.86it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7412  Mult_acc_3: 0.6513  Mult_acc_5: 0.4013  F1_score: 0.7504  MAE: 0.3981  Corr: 0.6579  Loss: 0.4080 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [4/43/1] >> loss: 0.3129  Mult_acc_2: 0.8289  Mult_acc_3: 0.7449  Mult_acc_5: 0.5015  F1_score: 0.8314  MAE: 0.2978  Corr: 0.8099  clm loss: 2.9762 total loss: 4.5043 bn loss: 0.3578 av loss: 0.4432 text loss: 0.4142\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0103], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0111], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0084], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0113], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0087], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0059], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0080], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.29it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6469  Mult_acc_5: 0.4101  F1_score: 0.7402  MAE: 0.3973  Corr: 0.6601  Loss: 0.4214 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [5/44/1] >> loss: 0.3125  Mult_acc_2: 0.8385  Mult_acc_3: 0.7529  Mult_acc_5: 0.5175  F1_score: 0.8412  MAE: 0.2957  Corr: 0.8127  clm loss: 2.8926 total loss: 4.4017 bn loss: 0.3527 av loss: 0.4466 text loss: 0.3973\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0106], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0113], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0087], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0117], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0079], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0091], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0088], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0033], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0082], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.67it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7566  Mult_acc_3: 0.6601  Mult_acc_5: 0.4167  F1_score: 0.7644  MAE: 0.3902  Corr: 0.6624  Loss: 0.4115 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [6/45/1] >> loss: 0.307  Mult_acc_2: 0.8311  Mult_acc_3: 0.7507  Mult_acc_5: 0.5051  F1_score: 0.8336  MAE: 0.2916  Corr: 0.8175  clm loss: 2.9194 total loss: 4.4125 bn loss: 0.3493 av loss: 0.4379 text loss: 0.399\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0108], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0116], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0089], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0120], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0093], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0063], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0030], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0085], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.35it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6535  Mult_acc_5: 0.3860  F1_score: 0.7385  MAE: 0.4072  Corr: 0.6569  Loss: 0.4280 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [7/46/1] >> loss: 0.3001  Mult_acc_2: 0.8501  Mult_acc_3: 0.7668  Mult_acc_5: 0.5154  F1_score: 0.8529  MAE: 0.2881  Corr: 0.8265  clm loss: 2.9082 total loss: 4.4038 bn loss: 0.3513 av loss: 0.442 text loss: 0.4023\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0111], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0119], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0091], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0123], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0096], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0092], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.84it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7632  Mult_acc_3: 0.6535  Mult_acc_5: 0.3991  F1_score: 0.7703  MAE: 0.3897  Corr: 0.6675  Loss: 0.4049 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [8/47/1] >> loss: 0.3065  Mult_acc_2: 0.8377  Mult_acc_3: 0.7442  Mult_acc_5: 0.4978  F1_score: 0.8404  MAE: 0.2973  Corr: 0.8167  clm loss: 2.9211 total loss: 4.4445 bn loss: 0.3539 av loss: 0.4392 text loss: 0.4238\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0113], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0121], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0093], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0126], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0085], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0099], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0093], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0025], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0088], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.81it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7522  Mult_acc_3: 0.6557  Mult_acc_5: 0.4145  F1_score: 0.7606  MAE: 0.3889  Corr: 0.6685  Loss: 0.4034 \n",
            "100% 43/43 [00:22<00:00,  1.95it/s]\n",
            "MMSA - TRAIN-(msalm) [9/48/1] >> loss: 0.2961  Mult_acc_2: 0.8523  Mult_acc_3: 0.7617  Mult_acc_5: 0.5270  F1_score: 0.8547  MAE: 0.2849  Corr: 0.8322  clm loss: 2.8791 total loss: 4.3449 bn loss: 0.3397 av loss: 0.4389 text loss: 0.391\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0115], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0124], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0095], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0128], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0102], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0094], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0023], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.21it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6491  Mult_acc_5: 0.3925  F1_score: 0.7405  MAE: 0.3990  Corr: 0.6644  Loss: 0.4294 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [10/49/1] >> loss: 0.2887  Mult_acc_2: 0.8531  Mult_acc_3: 0.7668  Mult_acc_5: 0.5263  F1_score: 0.8559  MAE: 0.2733  Corr: 0.8427  clm loss: 2.8743 total loss: 4.3134 bn loss: 0.3331 av loss: 0.4386 text loss: 0.3786\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0118], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0126], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0097], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0131], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0088], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0105], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0096], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0091], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.79it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6557  Mult_acc_5: 0.4101  F1_score: 0.7524  MAE: 0.3910  Corr: 0.6648  Loss: 0.4146 \n",
            "***************** Loading Model from checkpoints/deepmlf-base-sims/msalm-sims-1990.pth\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0092], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0099], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0100], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0072], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.21it/s]\n",
            "MMSA - TEST-(msalm) >>  Mult_acc_2: 0.7877  Mult_acc_3: 0.6805  Mult_acc_5: 0.4289  F1_score: 0.7926  MAE: 0.3713  Corr: 0.7078  Loss: 0.3805 \n",
            "Deleting stored model from checkpoints/deepmlf-base-sims/msalm-sims-1990.pth\n",
            "MMSA - Result for seed 1990: {'Mult_acc_2': 0.7877, 'Mult_acc_3': 0.6805, 'Mult_acc_5': 0.4289, 'F1_score': 0.7926, 'MAE': 0.3713, 'Corr': 0.7078, 'Loss': 0.3805, 'seed': 1990}\n",
            "MMSA - ------------------------------ Running with seed 1991 [2/2] ------------------------------\n",
            "Loading HF datasets\n",
            "---------------------- Ongoing with TRAIN data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(1368, 400, 33)\n",
            "(1368, 55, 709)\n",
            "---------------------- Ongoing with VALID data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(456, 400, 33)\n",
            "(456, 55, 709)\n",
            "---------------------- Ongoing with TEST data split -----------------------------\n",
            "Using GPT LM\n",
            "Using Chinese LM\n",
            "All the sequence lengths are L:39, A:400, V:55\n",
            "Preprocessing custom M-SENA pickles\n",
            "audio features are (1368, 400, 33)\n",
            "vision features are (1368, 55, 709)\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/content/miniconda3/envs/deepmlf/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Starting processing --------------------->\n",
            "Using chinese causal LM\n",
            "(457, 400, 33)\n",
            "(457, 55, 709)\n",
            "Ongoing with num_workers=2\n",
            "ca list is: [5, 6, 7, 8, 9, 10, 11]\n",
            "initializing SoftPerm\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 0\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 2\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 3\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 4\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 5\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Ongoing with ----- sigmoid ----- gating\n",
            "idx is 6\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Parsing decoder block: 0\n",
            "Parsing decoder block: 1\n",
            "Parsing decoder block: 2\n",
            "Parsing decoder block: 3\n",
            "Parsing decoder block: 4\n",
            "Parsing decoder block: 5\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 6\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 7\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 8\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 9\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 10\n",
            "COpying---------------------------------\n",
            "Parsing decoder block: 11\n",
            "COpying---------------------------------\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "----------------->>> Pretrained AudioVisual Encoder <<<<<----------------\n",
            "Using BN_a\n",
            "Using BN_v\n",
            "----------------------- Loading AV encoder from checkpoints/bienc-sims/bienc-sims-1990.pth\n",
            "Copied param embed_positions_a._float_tensor\n",
            "Copied param embed_positions_v._float_tensor\n",
            "Copied param BN_a.weight\n",
            "Copied param BN_a.bias\n",
            "Copied param BN_a.running_mean\n",
            "Copied param BN_a.running_var\n",
            "Copied param BN_a.num_batches_tracked\n",
            "Copied param BN_v.weight\n",
            "Copied param BN_v.bias\n",
            "Copied param BN_v.running_mean\n",
            "Copied param BN_v.running_var\n",
            "Copied param BN_v.num_batches_tracked\n",
            "Copied param proj_a.weight\n",
            "Copied param proj_v.weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.0.linear1.weight\n",
            "Copied param enc_a.layers.0.linear1.bias\n",
            "Copied param enc_a.layers.0.linear2.weight\n",
            "Copied param enc_a.layers.0.linear2.bias\n",
            "Copied param enc_a.layers.0.norm1.weight\n",
            "Copied param enc_a.layers.0.norm1.bias\n",
            "Copied param enc_a.layers.0.norm2.weight\n",
            "Copied param enc_a.layers.0.norm2.bias\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.1.linear1.weight\n",
            "Copied param enc_a.layers.1.linear1.bias\n",
            "Copied param enc_a.layers.1.linear2.weight\n",
            "Copied param enc_a.layers.1.linear2.bias\n",
            "Copied param enc_a.layers.1.norm1.weight\n",
            "Copied param enc_a.layers.1.norm1.bias\n",
            "Copied param enc_a.layers.1.norm2.weight\n",
            "Copied param enc_a.layers.1.norm2.bias\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_a.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_a.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_a.layers.2.linear1.weight\n",
            "Copied param enc_a.layers.2.linear1.bias\n",
            "Copied param enc_a.layers.2.linear2.weight\n",
            "Copied param enc_a.layers.2.linear2.bias\n",
            "Copied param enc_a.layers.2.norm1.weight\n",
            "Copied param enc_a.layers.2.norm1.bias\n",
            "Copied param enc_a.layers.2.norm2.weight\n",
            "Copied param enc_a.layers.2.norm2.bias\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.0.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.0.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.0.linear1.weight\n",
            "Copied param enc_v.layers.0.linear1.bias\n",
            "Copied param enc_v.layers.0.linear2.weight\n",
            "Copied param enc_v.layers.0.linear2.bias\n",
            "Copied param enc_v.layers.0.norm1.weight\n",
            "Copied param enc_v.layers.0.norm1.bias\n",
            "Copied param enc_v.layers.0.norm2.weight\n",
            "Copied param enc_v.layers.0.norm2.bias\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.1.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.1.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.1.linear1.weight\n",
            "Copied param enc_v.layers.1.linear1.bias\n",
            "Copied param enc_v.layers.1.linear2.weight\n",
            "Copied param enc_v.layers.1.linear2.bias\n",
            "Copied param enc_v.layers.1.norm1.weight\n",
            "Copied param enc_v.layers.1.norm1.bias\n",
            "Copied param enc_v.layers.1.norm2.weight\n",
            "Copied param enc_v.layers.1.norm2.bias\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_weight\n",
            "Copied param enc_v.layers.2.self_attn.in_proj_bias\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.weight\n",
            "Copied param enc_v.layers.2.self_attn.out_proj.bias\n",
            "Copied param enc_v.layers.2.linear1.weight\n",
            "Copied param enc_v.layers.2.linear1.bias\n",
            "Copied param enc_v.layers.2.linear2.weight\n",
            "Copied param enc_v.layers.2.linear2.bias\n",
            "Copied param enc_v.layers.2.norm1.weight\n",
            "Copied param enc_v.layers.2.norm1.bias\n",
            "Copied param enc_v.layers.2.norm2.weight\n",
            "Copied param enc_v.layers.2.norm2.bias\n",
            "Copied param fusion.weight\n",
            "Copied param fusion.bias\n",
            "Copied param clf.weight\n",
            "Copied param clf.bias\n",
            "------------------ Adding LNorm ------------------------\n",
            "MMSA - The model has 143937589 trainable parameters\n",
            "ongoing with msalm\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "False\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "5.ca_layer.alpha_1\n",
            "5.ca_layer.alpha_2\n",
            "5.ca_layer.ln_1.weight\n",
            "5.ca_layer.ln_1.bias\n",
            "5.ca_layer.ln_2.weight\n",
            "5.ca_layer.ln_2.bias\n",
            "5.ca_layer.attn.W_q.weight\n",
            "5.ca_layer.attn.W_kv.weight\n",
            "5.ca_layer.attn.W_o.weight\n",
            "5.ca_layer.mlp.c_fc.weight\n",
            "5.ca_layer.mlp.c_fc.bias\n",
            "5.ca_layer.mlp.c_proj.weight\n",
            "5.ca_layer.mlp.c_proj.bias\n",
            "6.ca_layer.alpha_1\n",
            "6.ca_layer.alpha_2\n",
            "6.ca_layer.ln_1.weight\n",
            "6.ca_layer.ln_1.bias\n",
            "6.ca_layer.ln_2.weight\n",
            "6.ca_layer.ln_2.bias\n",
            "6.ca_layer.attn.W_q.weight\n",
            "6.ca_layer.attn.W_kv.weight\n",
            "6.ca_layer.attn.W_o.weight\n",
            "6.ca_layer.mlp.c_fc.weight\n",
            "6.ca_layer.mlp.c_fc.bias\n",
            "6.ca_layer.mlp.c_proj.weight\n",
            "6.ca_layer.mlp.c_proj.bias\n",
            "7.ca_layer.alpha_1\n",
            "7.ca_layer.alpha_2\n",
            "7.ca_layer.ln_1.weight\n",
            "7.ca_layer.ln_1.bias\n",
            "7.ca_layer.ln_2.weight\n",
            "7.ca_layer.ln_2.bias\n",
            "7.ca_layer.attn.W_q.weight\n",
            "7.ca_layer.attn.W_kv.weight\n",
            "7.ca_layer.attn.W_o.weight\n",
            "7.ca_layer.mlp.c_fc.weight\n",
            "7.ca_layer.mlp.c_fc.bias\n",
            "7.ca_layer.mlp.c_proj.weight\n",
            "7.ca_layer.mlp.c_proj.bias\n",
            "8.ca_layer.alpha_1\n",
            "8.ca_layer.alpha_2\n",
            "8.ca_layer.ln_1.weight\n",
            "8.ca_layer.ln_1.bias\n",
            "8.ca_layer.ln_2.weight\n",
            "8.ca_layer.ln_2.bias\n",
            "8.ca_layer.attn.W_q.weight\n",
            "8.ca_layer.attn.W_kv.weight\n",
            "8.ca_layer.attn.W_o.weight\n",
            "8.ca_layer.mlp.c_fc.weight\n",
            "8.ca_layer.mlp.c_fc.bias\n",
            "8.ca_layer.mlp.c_proj.weight\n",
            "8.ca_layer.mlp.c_proj.bias\n",
            "9.ca_layer.alpha_1\n",
            "9.ca_layer.alpha_2\n",
            "9.ca_layer.ln_1.weight\n",
            "9.ca_layer.ln_1.bias\n",
            "9.ca_layer.ln_2.weight\n",
            "9.ca_layer.ln_2.bias\n",
            "9.ca_layer.attn.W_q.weight\n",
            "9.ca_layer.attn.W_kv.weight\n",
            "9.ca_layer.attn.W_o.weight\n",
            "9.ca_layer.mlp.c_fc.weight\n",
            "9.ca_layer.mlp.c_fc.bias\n",
            "9.ca_layer.mlp.c_proj.weight\n",
            "9.ca_layer.mlp.c_proj.bias\n",
            "10.ca_layer.alpha_1\n",
            "10.ca_layer.alpha_2\n",
            "10.ca_layer.ln_1.weight\n",
            "10.ca_layer.ln_1.bias\n",
            "10.ca_layer.ln_2.weight\n",
            "10.ca_layer.ln_2.bias\n",
            "10.ca_layer.attn.W_q.weight\n",
            "10.ca_layer.attn.W_kv.weight\n",
            "10.ca_layer.attn.W_o.weight\n",
            "10.ca_layer.mlp.c_fc.weight\n",
            "10.ca_layer.mlp.c_fc.bias\n",
            "10.ca_layer.mlp.c_proj.weight\n",
            "10.ca_layer.mlp.c_proj.bias\n",
            "11.ca_layer.alpha_1\n",
            "11.ca_layer.alpha_2\n",
            "11.ca_layer.ln_1.weight\n",
            "11.ca_layer.ln_1.bias\n",
            "11.ca_layer.ln_2.weight\n",
            "11.ca_layer.ln_2.bias\n",
            "11.ca_layer.attn.W_q.weight\n",
            "11.ca_layer.attn.W_kv.weight\n",
            "11.ca_layer.attn.W_o.weight\n",
            "11.ca_layer.mlp.c_fc.weight\n",
            "11.ca_layer.mlp.c_fc.bias\n",
            "11.ca_layer.mlp.c_proj.weight\n",
            "11.ca_layer.mlp.c_proj.bias\n",
            "0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Model.av_encoder.BN_a.weight\n",
            "Model.av_encoder.BN_a.bias\n",
            "Model.av_encoder.BN_v.weight\n",
            "Model.av_encoder.BN_v.bias\n",
            "Model.av_encoder.proj_a.weight\n",
            "Model.av_encoder.proj_v.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_a.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_a.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_a.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_a.layers.2.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.0.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.0.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.0.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.0.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.1.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.1.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.1.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.1.norm2.bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight\n",
            "Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear1.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear1.bias\n",
            "Model.av_encoder.enc_v.layers.2.linear2.weight\n",
            "Model.av_encoder.enc_v.layers.2.linear2.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm1.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm1.bias\n",
            "Model.av_encoder.enc_v.layers.2.norm2.weight\n",
            "Model.av_encoder.enc_v.layers.2.norm2.bias\n",
            "Model.av_encoder.fusion.weight\n",
            "Model.av_encoder.fusion.bias\n",
            "Model.av_encoder.clf.weight\n",
            "Model.av_encoder.clf.bias\n",
            "Model.LN.weight\n",
            "Model.LN.bias\n",
            "The total number of trainable parameters is 41.87 M\n",
            "Model.lang_encoder.transformer.wte.0.embedding.weight\n",
            "Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding\n",
            "Model.lang_encoder.transformer.wpe.0.positional.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_q.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_kv.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.attn.W_o.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight\n",
            "Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias\n",
            "Model.lang_encoder.transformer.ln_f.weight\n",
            "Model.lang_encoder.transformer.ln_f.bias\n",
            "Model.W_task.0.weight\n",
            "Using grad with decay in Model.W_task.0.weight\n",
            "Model.W_task.0.bias\n",
            "Using grad with no decay in Model.W_task.0.bias\n",
            "Model.W_task.1.weight\n",
            "Using grad with decay in Model.W_task.1.weight\n",
            "Model.W_task.1.bias\n",
            "Using grad with no decay in Model.W_task.1.bias\n",
            "Model.W_task.3.weight\n",
            "Using grad with decay in Model.W_task.3.weight\n",
            "Model.W_task.3.bias\n",
            "Using grad with no decay in Model.W_task.3.bias\n",
            "Model.W_bn.weight\n",
            "Using grad with decay in Model.W_bn.weight\n",
            "Model.W_bn.bias\n",
            "Using grad with no decay in Model.W_bn.bias\n",
            "Model.W_text.weight\n",
            "Using grad with decay in Model.W_text.weight\n",
            "Model.W_text.bias\n",
            "Using grad with no decay in Model.W_text.bias\n",
            "Model.W_av.weight\n",
            "Using grad with decay in Model.W_av.weight\n",
            "Model.W_av.bias\n",
            "Using grad with no decay in Model.W_av.bias\n",
            "Model.LN.weight\n",
            "Using grad with decay in Model.LN.weight\n",
            "Model.LN.bias\n",
            "Using grad with no decay in Model.LN.bias\n",
            "Will be using warmup for 5 steps\n",
            "100% 43/43 [00:24<00:00,  1.73it/s]\n",
            "MMSA - TRAIN-(msalm) [1/1/2] >> loss: 0.6411  Mult_acc_2: 0.6133  Mult_acc_3: 0.4503  Mult_acc_5: 0.2186  F1_score: 0.5890  MAE: 0.6384  Corr: 0.0131  clm loss: 7.6792 total loss: 10.734 bn loss: 0.8844 av loss: 0.6193 text loss: 0.9101\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-5.4752e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([2.1955e-05], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([4.7809e-05], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.89it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.6864  Mult_acc_3: 0.5197  Mult_acc_5: 0.2632  F1_score: 0.5969  MAE: 0.6283  Corr: 0.2081  Loss: 0.6390 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [1/2/2] >> loss: 0.5696  Mult_acc_2: 0.6791  Mult_acc_3: 0.5058  Mult_acc_5: 0.2500  F1_score: 0.6652  MAE: 0.5638  Corr: 0.2986  clm loss: 6.8279 total loss: 9.5606 bn loss: 0.7358 av loss: 0.5925 text loss: 0.8349\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-6.7662e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.88it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.5526  Mult_acc_5: 0.3289  F1_score: 0.6733  MAE: 0.5590  Corr: 0.4248  Loss: 0.6035 \n",
            "100% 43/43 [00:22<00:00,  1.87it/s]\n",
            "MMSA - TRAIN-(msalm) [1/3/2] >> loss: 0.514  Mult_acc_2: 0.7230  Mult_acc_3: 0.5906  Mult_acc_5: 0.3194  F1_score: 0.7219  MAE: 0.4934  Corr: 0.4753  clm loss: 6.0301 total loss: 8.5076 bn loss: 0.6287 av loss: 0.5761 text loss: 0.7587\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([6.2292e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0008], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.89it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.5833  Mult_acc_5: 0.3268  F1_score: 0.7124  MAE: 0.5124  Corr: 0.4794  Loss: 0.5542 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [1/4/2] >> loss: 0.5131  Mult_acc_2: 0.7105  Mult_acc_3: 0.6009  Mult_acc_5: 0.3311  F1_score: 0.7132  MAE: 0.4848  Corr: 0.4957  clm loss: 5.3467 total loss: 7.6904 bn loss: 0.582 av loss: 0.5528 text loss: 0.6959\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([1.0925e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.79it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.5855  Mult_acc_5: 0.3509  F1_score: 0.7307  MAE: 0.4812  Corr: 0.4971  Loss: 0.5280 \n",
            "100% 43/43 [00:23<00:00,  1.85it/s]\n",
            "MMSA - TRAIN-(msalm) [1/5/2] >> loss: 0.4943  Mult_acc_2: 0.7434  Mult_acc_3: 0.6023  Mult_acc_5: 0.3553  F1_score: 0.7437  MAE: 0.4734  Corr: 0.5240  clm loss: 4.8454 total loss: 7.0733 bn loss: 0.5486 av loss: 0.5338 text loss: 0.6511\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([3.2390e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0014], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.71it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7478  Mult_acc_3: 0.6031  Mult_acc_5: 0.3662  F1_score: 0.7422  MAE: 0.4742  Corr: 0.5172  Loss: 0.4960 \n",
            "100% 43/43 [00:23<00:00,  1.85it/s]\n",
            "MMSA - TRAIN-(msalm) [1/6/2] >> loss: 0.4757  Mult_acc_2: 0.7493  Mult_acc_3: 0.6075  Mult_acc_5: 0.3626  F1_score: 0.7480  MAE: 0.4576  Corr: 0.5387  clm loss: 4.3711 total loss: 6.51 bn loss: 0.5263 av loss: 0.5156 text loss: 0.6213\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-5.2768e-06], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0013], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0025], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.46it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.6162  Mult_acc_5: 0.3465  F1_score: 0.7382  MAE: 0.4700  Corr: 0.5179  Loss: 0.4841 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [1/7/2] >> loss: 0.4734  Mult_acc_2: 0.7573  Mult_acc_3: 0.6272  Mult_acc_5: 0.3421  F1_score: 0.7558  MAE: 0.4605  Corr: 0.5496  clm loss: 4.077 total loss: 6.1686 bn loss: 0.5216 av loss: 0.5059 text loss: 0.5907\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.30it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6250  Mult_acc_5: 0.3838  F1_score: 0.7231  MAE: 0.4660  Corr: 0.5209  Loss: 0.4755 \n",
            "100% 43/43 [00:26<00:00,  1.65it/s]\n",
            "MMSA - TRAIN-(msalm) [1/8/2] >> loss: 0.4613  Mult_acc_2: 0.7529  Mult_acc_3: 0.6213  Mult_acc_5: 0.3516  F1_score: 0.7508  MAE: 0.4485  Corr: 0.5773  clm loss: 3.8727 total loss: 5.9233 bn loss: 0.5057 av loss: 0.4908 text loss: 0.5929\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-8.1631e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0007], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0005], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0031], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0017], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.03it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7412  Mult_acc_3: 0.6316  Mult_acc_5: 0.3794  F1_score: 0.7378  MAE: 0.4658  Corr: 0.5320  Loss: 0.5067 \n",
            "100% 43/43 [00:22<00:00,  1.95it/s]\n",
            "MMSA - TRAIN-(msalm) [2/9/2] >> loss: 0.4517  Mult_acc_2: 0.7639  Mult_acc_3: 0.6469  Mult_acc_5: 0.3794  F1_score: 0.7646  MAE: 0.4326  Corr: 0.6034  clm loss: 3.7425 total loss: 5.754 bn loss: 0.4949 av loss: 0.4789 text loss: 0.586\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-6.0535e-06], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0001], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([2.8995e-05], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0033], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.21it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.6118  Mult_acc_5: 0.3596  F1_score: 0.7288  MAE: 0.4765  Corr: 0.5327  Loss: 0.5165 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [3/10/2] >> loss: 0.4711  Mult_acc_2: 0.7478  Mult_acc_3: 0.6323  Mult_acc_5: 0.3428  F1_score: 0.7482  MAE: 0.4537  Corr: 0.5625  clm loss: 3.58 total loss: 5.61 bn loss: 0.4951 av loss: 0.4776 text loss: 0.5863\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([1.9495e-05], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-1.4640e-05], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0011], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.65it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.6162  Mult_acc_5: 0.3399  F1_score: 0.7371  MAE: 0.4684  Corr: 0.5321  Loss: 0.4843 \n",
            "100% 43/43 [00:23<00:00,  1.83it/s]\n",
            "MMSA - TRAIN-(msalm) [4/11/2] >> loss: 0.4532  Mult_acc_2: 0.7610  Mult_acc_3: 0.6279  Mult_acc_5: 0.3472  F1_score: 0.7609  MAE: 0.4419  Corr: 0.5733  clm loss: 3.4893 total loss: 5.4716 bn loss: 0.4932 av loss: 0.4708 text loss: 0.5651\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0036], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.39it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6140  Mult_acc_5: 0.3421  F1_score: 0.7270  MAE: 0.4690  Corr: 0.5326  Loss: 0.4973 \n",
            "100% 43/43 [00:22<00:00,  1.91it/s]\n",
            "MMSA - TRAIN-(msalm) [5/12/2] >> loss: 0.4337  Mult_acc_2: 0.7800  Mult_acc_3: 0.6623  Mult_acc_5: 0.3692  F1_score: 0.7790  MAE: 0.4194  Corr: 0.6263  clm loss: 3.4237 total loss: 5.3811 bn loss: 0.4852 av loss: 0.4667 text loss: 0.5718\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-0.0001], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0037], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0014], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.94it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7368  Mult_acc_3: 0.6250  Mult_acc_5: 0.3684  F1_score: 0.7320  MAE: 0.4636  Corr: 0.5370  Loss: 0.5015 \n",
            "100% 43/43 [00:23<00:00,  1.84it/s]\n",
            "MMSA - TRAIN-(msalm) [6/13/2] >> loss: 0.4444  Mult_acc_2: 0.7829  Mult_acc_3: 0.6564  Mult_acc_5: 0.3808  F1_score: 0.7841  MAE: 0.4249  Corr: 0.6155  clm loss: 3.3507 total loss: 5.2923 bn loss: 0.4855 av loss: 0.4612 text loss: 0.5505\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([-2.3522e-06], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0038], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0005], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.80it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6075  Mult_acc_5: 0.3575  F1_score: 0.7308  MAE: 0.4506  Corr: 0.5538  Loss: 0.4899 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [7/14/2] >> loss: 0.4292  Mult_acc_2: 0.7712  Mult_acc_3: 0.6542  Mult_acc_5: 0.3677  F1_score: 0.7724  MAE: 0.4142  Corr: 0.6393  clm loss: 3.3377 total loss: 5.2621 bn loss: 0.4953 av loss: 0.4616 text loss: 0.5382\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0002], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0040], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.77it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.5943  Mult_acc_5: 0.3487  F1_score: 0.7214  MAE: 0.4545  Corr: 0.5608  Loss: 0.4814 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [8/15/2] >> loss: 0.4262  Mult_acc_2: 0.7778  Mult_acc_3: 0.6513  Mult_acc_5: 0.3721  F1_score: 0.7791  MAE: 0.4123  Corr: 0.6478  clm loss: 3.3258 total loss: 5.2488 bn loss: 0.4855 av loss: 0.4575 text loss: 0.5539\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0041], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.71it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.6140  Mult_acc_5: 0.3575  F1_score: 0.7217  MAE: 0.4539  Corr: 0.5635  Loss: 0.4988 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [9/16/2] >> loss: 0.4237  Mult_acc_2: 0.7792  Mult_acc_3: 0.6594  Mult_acc_5: 0.3852  F1_score: 0.7806  MAE: 0.4046  Corr: 0.6622  clm loss: 3.2714 total loss: 5.1467 bn loss: 0.475 av loss: 0.4476 text loss: 0.529\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0006], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0043], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0007], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0009], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0023], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.54it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7018  Mult_acc_3: 0.6140  Mult_acc_5: 0.3662  F1_score: 0.7105  MAE: 0.4527  Corr: 0.5670  Loss: 0.4893 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [10/17/2] >> loss: 0.4027  Mult_acc_2: 0.7895  Mult_acc_3: 0.6630  Mult_acc_5: 0.4006  F1_score: 0.7911  MAE: 0.3870  Corr: 0.6778  clm loss: 3.2494 total loss: 5.1035 bn loss: 0.4637 av loss: 0.452 text loss: 0.5357\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0044], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0017], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0026], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.36it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.6140  Mult_acc_5: 0.3596  F1_score: 0.7255  MAE: 0.4476  Corr: 0.5759  Loss: 0.4743 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [1/18/2] >> loss: 0.4008  Mult_acc_2: 0.8026  Mult_acc_3: 0.6813  Mult_acc_5: 0.4042  F1_score: 0.8034  MAE: 0.3851  Corr: 0.6879  clm loss: 3.2341 total loss: 5.0673 bn loss: 0.456 av loss: 0.4511 text loss: 0.5253\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0045], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0010], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.84it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.6250  Mult_acc_5: 0.3706  F1_score: 0.7255  MAE: 0.4397  Corr: 0.5844  Loss: 0.4771 \n",
            "100% 43/43 [00:23<00:00,  1.87it/s]\n",
            "MMSA - TRAIN-(msalm) [2/19/2] >> loss: 0.3982  Mult_acc_2: 0.7887  Mult_acc_3: 0.6849  Mult_acc_5: 0.4181  F1_score: 0.7908  MAE: 0.3799  Corr: 0.6947  clm loss: 3.2354 total loss: 5.0494 bn loss: 0.4554 av loss: 0.45 text loss: 0.5104\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0030], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.70it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6228  Mult_acc_5: 0.3794  F1_score: 0.7335  MAE: 0.4318  Corr: 0.5897  Loss: 0.4799 \n",
            "100% 43/43 [00:21<00:00,  1.98it/s]\n",
            "MMSA - TRAIN-(msalm) [3/20/2] >> loss: 0.4009  Mult_acc_2: 0.7858  Mult_acc_3: 0.6791  Mult_acc_5: 0.4101  F1_score: 0.7883  MAE: 0.3826  Corr: 0.6872  clm loss: 3.2344 total loss: 5.0495 bn loss: 0.4541 av loss: 0.4531 text loss: 0.507\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0012], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0013], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0032], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.39it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6272  Mult_acc_5: 0.3947  F1_score: 0.7465  MAE: 0.4181  Corr: 0.6011  Loss: 0.4527 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [1/21/2] >> loss: 0.3929  Mult_acc_2: 0.7792  Mult_acc_3: 0.6798  Mult_acc_5: 0.4145  F1_score: 0.7825  MAE: 0.3790  Corr: 0.6984  clm loss: 3.1912 total loss: 4.9813 bn loss: 0.449 av loss: 0.4444 text loss: 0.5038\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0046], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0014], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0033], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.48it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6184  Mult_acc_5: 0.4057  F1_score: 0.7355  MAE: 0.4189  Corr: 0.5986  Loss: 0.4562 \n",
            "100% 43/43 [00:23<00:00,  1.86it/s]\n",
            "MMSA - TRAIN-(msalm) [2/22/2] >> loss: 0.3802  Mult_acc_2: 0.8026  Mult_acc_3: 0.6806  Mult_acc_5: 0.4167  F1_score: 0.8039  MAE: 0.3648  Corr: 0.7223  clm loss: 3.1801 total loss: 4.9315 bn loss: 0.4418 av loss: 0.444 text loss: 0.4853\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0045], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0016], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.57it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.6162  Mult_acc_5: 0.3816  F1_score: 0.7326  MAE: 0.4184  Corr: 0.6051  Loss: 0.4652 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [3/23/2] >> loss: 0.3863  Mult_acc_2: 0.8026  Mult_acc_3: 0.7025  Mult_acc_5: 0.4357  F1_score: 0.8055  MAE: 0.3685  Corr: 0.7055  clm loss: 3.1658 total loss: 4.9375 bn loss: 0.4453 av loss: 0.4522 text loss: 0.4879\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0044], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0017], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.04it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.6996  Mult_acc_3: 0.6206  Mult_acc_5: 0.3860  F1_score: 0.7104  MAE: 0.4233  Corr: 0.6136  Loss: 0.4606 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [4/24/2] >> loss: 0.3737  Mult_acc_2: 0.8129  Mult_acc_3: 0.7003  Mult_acc_5: 0.4189  F1_score: 0.8152  MAE: 0.3618  Corr: 0.7360  clm loss: 3.1619 total loss: 4.8944 bn loss: 0.4345 av loss: 0.4465 text loss: 0.4778\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0042], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0016], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0018], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.01it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7018  Mult_acc_3: 0.6075  Mult_acc_5: 0.3728  F1_score: 0.7129  MAE: 0.4270  Corr: 0.6243  Loss: 0.4501 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [1/25/2] >> loss: 0.3812  Mult_acc_2: 0.8136  Mult_acc_3: 0.6908  Mult_acc_5: 0.4225  F1_score: 0.8159  MAE: 0.3607  Corr: 0.7276  clm loss: 3.1465 total loss: 4.9009 bn loss: 0.4298 av loss: 0.4476 text loss: 0.4958\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0033], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0040], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0014], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.06it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.6250  Mult_acc_5: 0.3947  F1_score: 0.7250  MAE: 0.4244  Corr: 0.6172  Loss: 0.4734 \n",
            "100% 43/43 [00:22<00:00,  1.91it/s]\n",
            "MMSA - TRAIN-(msalm) [2/26/2] >> loss: 0.3586  Mult_acc_2: 0.8246  Mult_acc_3: 0.7193  Mult_acc_5: 0.4591  F1_score: 0.8264  MAE: 0.3400  Corr: 0.7532  clm loss: 3.1008 total loss: 4.8018 bn loss: 0.4203 av loss: 0.4463 text loss: 0.4758\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0037], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0012], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0020], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.05it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6250  Mult_acc_5: 0.3860  F1_score: 0.7392  MAE: 0.4154  Corr: 0.6294  Loss: 0.4427 \n",
            "100% 43/43 [00:22<00:00,  1.94it/s]\n",
            "MMSA - TRAIN-(msalm) [1/27/2] >> loss: 0.358  Mult_acc_2: 0.8121  Mult_acc_3: 0.7120  Mult_acc_5: 0.4503  F1_score: 0.8139  MAE: 0.3424  Corr: 0.7500  clm loss: 3.1108 total loss: 4.8052 bn loss: 0.4171 av loss: 0.448 text loss: 0.4714\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0054], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0034], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0009], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.97it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7171  Mult_acc_3: 0.6316  Mult_acc_5: 0.3925  F1_score: 0.7247  MAE: 0.4075  Corr: 0.6295  Loss: 0.4451 \n",
            "100% 43/43 [00:23<00:00,  1.79it/s]\n",
            "MMSA - TRAIN-(msalm) [2/28/2] >> loss: 0.3534  Mult_acc_2: 0.8151  Mult_acc_3: 0.7091  Mult_acc_5: 0.4649  F1_score: 0.8172  MAE: 0.3371  Corr: 0.7544  clm loss: 3.1171 total loss: 4.7891 bn loss: 0.4161 av loss: 0.4432 text loss: 0.4593\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0060], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0049], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0035], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0032], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0021], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0035], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.75it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7061  Mult_acc_3: 0.6096  Mult_acc_5: 0.3596  F1_score: 0.7155  MAE: 0.4180  Corr: 0.6325  Loss: 0.4405 \n",
            "100% 43/43 [00:21<00:00,  1.98it/s]\n",
            "MMSA - TRAIN-(msalm) [1/29/2] >> loss: 0.3504  Mult_acc_2: 0.8194  Mult_acc_3: 0.7178  Mult_acc_5: 0.4393  F1_score: 0.8204  MAE: 0.3360  Corr: 0.7628  clm loss: 3.0888 total loss: 4.7518 bn loss: 0.4093 av loss: 0.4428 text loss: 0.4606\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0060], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.64it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.6425  Mult_acc_5: 0.3882  F1_score: 0.7405  MAE: 0.4032  Corr: 0.6364  Loss: 0.4136 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [1/30/2] >> loss: 0.3411  Mult_acc_2: 0.8333  Mult_acc_3: 0.7325  Mult_acc_5: 0.4744  F1_score: 0.8354  MAE: 0.3273  Corr: 0.7760  clm loss: 3.09 total loss: 4.7172 bn loss: 0.4024 av loss: 0.4409 text loss: 0.4429\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0064], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0047], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0025], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([5.7153e-05], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0033], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0040], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.84it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6206  Mult_acc_5: 0.3882  F1_score: 0.7360  MAE: 0.4090  Corr: 0.6284  Loss: 0.4362 \n",
            "100% 43/43 [00:23<00:00,  1.87it/s]\n",
            "MMSA - TRAIN-(msalm) [2/31/2] >> loss: 0.3444  Mult_acc_2: 0.8268  Mult_acc_3: 0.7317  Mult_acc_5: 0.4708  F1_score: 0.8290  MAE: 0.3256  Corr: 0.7774  clm loss: 3.0662 total loss: 4.6927 bn loss: 0.392 av loss: 0.4405 text loss: 0.4496\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0060], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0021], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0032], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0041], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.87it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7456  Mult_acc_3: 0.6404  Mult_acc_5: 0.4101  F1_score: 0.7514  MAE: 0.4028  Corr: 0.6284  Loss: 0.4190 \n",
            "100% 43/43 [00:21<00:00,  1.96it/s]\n",
            "MMSA - TRAIN-(msalm) [3/32/2] >> loss: 0.3326  Mult_acc_2: 0.8246  Mult_acc_3: 0.7317  Mult_acc_5: 0.4832  F1_score: 0.8267  MAE: 0.3155  Corr: 0.7923  clm loss: 3.0495 total loss: 4.6489 bn loss: 0.3874 av loss: 0.4383 text loss: 0.441\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0077], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0069], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0052], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0064], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0017], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0028], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0032], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0042], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.53it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6360  Mult_acc_5: 0.4035  F1_score: 0.7355  MAE: 0.4062  Corr: 0.6290  Loss: 0.4463 \n",
            "100% 43/43 [00:22<00:00,  1.93it/s]\n",
            "MMSA - TRAIN-(msalm) [4/33/2] >> loss: 0.3379  Mult_acc_2: 0.8216  Mult_acc_3: 0.7200  Mult_acc_5: 0.4846  F1_score: 0.8241  MAE: 0.3187  Corr: 0.7844  clm loss: 3.0382 total loss: 4.631 bn loss: 0.3868 av loss: 0.4421 text loss: 0.4261\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0080], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0071], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0082], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0014], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0011], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0025], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0031], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0043], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.54it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7281  Mult_acc_3: 0.6469  Mult_acc_5: 0.4101  F1_score: 0.7355  MAE: 0.4048  Corr: 0.6336  Loss: 0.4168 \n",
            "100% 43/43 [00:22<00:00,  1.89it/s]\n",
            "MMSA - TRAIN-(msalm) [5/34/2] >> loss: 0.3335  Mult_acc_2: 0.8231  Mult_acc_3: 0.7303  Mult_acc_5: 0.4861  F1_score: 0.8261  MAE: 0.3224  Corr: 0.7863  clm loss: 3.0187 total loss: 4.6222 bn loss: 0.3841 av loss: 0.4463 text loss: 0.4395\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0084], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0074], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0058], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0072], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0010], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0015], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0029], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0065], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.37it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.6491  Mult_acc_5: 0.4189  F1_score: 0.7414  MAE: 0.4057  Corr: 0.6321  Loss: 0.4324 \n",
            "100% 43/43 [00:21<00:00,  1.97it/s]\n",
            "MMSA - TRAIN-(msalm) [6/35/2] >> loss: 0.3206  Mult_acc_2: 0.8406  Mult_acc_3: 0.7325  Mult_acc_5: 0.4934  F1_score: 0.8428  MAE: 0.3075  Corr: 0.8062  clm loss: 3.0263 total loss: 4.5917 bn loss: 0.3749 av loss: 0.435 text loss: 0.435\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0075], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0087], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0061], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0076], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0006], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0019], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0027], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0045], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0068], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:04<00:00,  3.04it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6425  Mult_acc_5: 0.4057  F1_score: 0.7400  MAE: 0.4091  Corr: 0.6299  Loss: 0.4379 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [7/36/2] >> loss: 0.3268  Mult_acc_2: 0.8458  Mult_acc_3: 0.7442  Mult_acc_5: 0.4993  F1_score: 0.8474  MAE: 0.3074  Corr: 0.8019  clm loss: 2.9921 total loss: 4.5829 bn loss: 0.3831 av loss: 0.4406 text loss: 0.4403\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0091], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0079], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0094], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0064], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0079], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0032], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0023], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0027], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0026], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0046], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.97it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7259  Mult_acc_3: 0.6425  Mult_acc_5: 0.4079  F1_score: 0.7349  MAE: 0.4113  Corr: 0.6346  Loss: 0.4448 \n",
            "100% 43/43 [00:22<00:00,  1.92it/s]\n",
            "MMSA - TRAIN-(msalm) [8/37/2] >> loss: 0.3169  Mult_acc_2: 0.8260  Mult_acc_3: 0.7420  Mult_acc_5: 0.4920  F1_score: 0.8293  MAE: 0.3023  Corr: 0.8082  clm loss: 2.978 total loss: 4.5366 bn loss: 0.3693 av loss: 0.4423 text loss: 0.43\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0094], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0098], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0067], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([6.0143e-05], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0026], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0029], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0024], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0048], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.73it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.6491  Mult_acc_5: 0.4079  F1_score: 0.7327  MAE: 0.4098  Corr: 0.6360  Loss: 0.4410 \n",
            "100% 43/43 [00:21<00:00,  1.99it/s]\n",
            "MMSA - TRAIN-(msalm) [9/38/2] >> loss: 0.3169  Mult_acc_2: 0.8385  Mult_acc_3: 0.7390  Mult_acc_5: 0.4868  F1_score: 0.8412  MAE: 0.3024  Corr: 0.8064  clm loss: 2.9754 total loss: 4.5265 bn loss: 0.3737 av loss: 0.4464 text loss: 0.414\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0084], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0098], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0083], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0101], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0070], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0087], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0055], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0004], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0036], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0030], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0022], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0075], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.85it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6579  Mult_acc_5: 0.4145  F1_score: 0.7405  MAE: 0.4055  Corr: 0.6361  Loss: 0.4208 \n",
            "100% 43/43 [00:22<00:00,  1.90it/s]\n",
            "MMSA - TRAIN-(msalm) [10/39/2] >> loss: 0.3085  Mult_acc_2: 0.8443  Mult_acc_3: 0.7478  Mult_acc_5: 0.4993  F1_score: 0.8472  MAE: 0.2942  Corr: 0.8217  clm loss: 2.9995 total loss: 4.5185 bn loss: 0.356 av loss: 0.4381 text loss: 0.4164\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0101], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0086], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0105], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0073], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0090], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0057], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0008], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0038], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0034], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0031], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0020], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0051], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0078], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:05<00:00,  2.79it/s]\n",
            "MMSA - VAL-(msalm) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.6513  Mult_acc_5: 0.4232  F1_score: 0.7557  MAE: 0.4032  Corr: 0.6386  Loss: 0.4223 \n",
            "***************** Loading Model from checkpoints/deepmlf-base-sims/msalm-sims-1991.pth\n",
            "Model alphas are\n",
            "5.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0060], device='cuda:0', requires_grad=True)\n",
            "5.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0062], device='cuda:0', requires_grad=True)\n",
            "6.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0066], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0044], device='cuda:0', requires_grad=True)\n",
            "7.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0053], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0037], device='cuda:0', requires_grad=True)\n",
            "8.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0028], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0024], device='cuda:0', requires_grad=True)\n",
            "9.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0003], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0022], device='cuda:0', requires_grad=True)\n",
            "10.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([-0.0034], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_1 is: Parameter containing:\n",
            "tensor([0.0039], device='cuda:0', requires_grad=True)\n",
            "11.ca_layer.alpha_2 is: Parameter containing:\n",
            "tensor([0.0050], device='cuda:0', requires_grad=True)\n",
            "100% 15/15 [00:03<00:00,  3.91it/s]\n",
            "MMSA - TEST-(msalm) >>  Mult_acc_2: 0.8009  Mult_acc_3: 0.6958  Mult_acc_5: 0.4114  F1_score: 0.8041  MAE: 0.3745  Corr: 0.7009  Loss: 0.3921 \n",
            "Deleting stored model from checkpoints/deepmlf-base-sims/msalm-sims-1991.pth\n",
            "MMSA - Result for seed 1991: {'Mult_acc_2': 0.8009, 'Mult_acc_3': 0.6958, 'Mult_acc_5': 0.4114, 'F1_score': 0.8041, 'MAE': 0.3745, 'Corr': 0.7009, 'Loss': 0.3921, 'seed': 1991}\n",
            "MMSA - Results saved to MMSA/results/deepmlf/deepmlf-base-sims/normal/sims_avg.csv.\n"
          ]
        }
      ],
      "source": [
        "!source miniconda3/bin/activate && \\\n",
        "conda init --all && \\\n",
        "cd /content/deepmlf && \\\n",
        "conda activate deepmlf && \\\n",
        "python experiments/regression/mult_base.py \\\n",
        "-m msalm \\\n",
        "-d sims \\\n",
        "-g 0 \\\n",
        "--exp-name deepmlf-base-sims \\\n",
        "-c MMSA/config/regression/deepmlf/sims/base_best.json \\\n",
        "--res-save-dir MMSA/results/deepmlf \\\n",
        "-n 2 \\\n",
        "-s 1990 \\\n",
        "-s 1991"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9LGVP0P96X3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
